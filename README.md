# Generation of Images using Generative Adversarial Networks for Augmentation of Training Data in Re-identification Models

This project aims to use Generative Adversarial Networks (GANs) to generate synthetic images and thus increase the quantity and diversity of training data in re-identification models.

## State of the art
The study of the state of the art on the use of generative adversarial networks for data augmentation
to improve the performance of re-identification models is presented at the following 
URL: [https://uselessai.github.io/person-reidentification/](https://uselessai.github.io/person-reidentification/)

## Methodology

This chapter is an important section where the approach used to carry out the study is described in detail. The methods and techniques used to collect and analyze data, as well as the tools and platforms used, are described. Technical details about image generation and re-identification model training are explained. The proposed methodology for the development of the work has been divided into two sections. The first section will contain all the information on the training of the adversarial generative network and the generation of artificial images, while the second section will cover the training and functioning of the re-identification model. The methodology can be summarized as follows:

+ Generation of artificial images
  + Training of the StyleGAN3 adversarial generative network
  + Generation of multiple artificial images of people in different poses
  + Using a real person's image from the database as a base, generate artificial images of that same person in different poses
  + Filtering images: automatic elimination of generated images with noise or incorrect generation


+ Re-identification model
  + Design of the architecture and training of the re-identification model.
  + Execution of tests for the re-identification model.
### Generation of artificial images


For the generation of artificial images, the architecture of the Stylegan3 generative adversarial network (GAN) (3) will be used. Stylegan3 (3) is an image generative model developed by the Nvidia research team in 2021. It is an improved version of the Stylegan2 model and is characterized by its ability to generate high-quality and realistic images in a wide variety of content categories.

Stylegan3 uses a deep learning approach based on generators and discriminators. The generator is a neural network that is trained to generate images that are as realistic as possible. To do this, it is shown a set of real images and asked to generate images that resemble them. As it is trained, the generator learns to extract relevant features from real images and use them to generate images that are as realistic as possible.

The discriminator is a neural network that is trained to distinguish between real and generated images. It is shown both real and generated images and asked to determine which are real and which are generated. As it is trained, the discriminator learns to identify the features that differentiate real images from generated ones and is used to guide the training of the generator towards generating more realistic images.

Stylegan is pre-trained with 25 million face images, of which 70 thousand are real high-quality 1024 x 1024 pixel images from the FFHQ database, and the rest were generated by the Discriminator, whose architecture is more detailed in Figure 4.3.

Currently, Stylegan3 works as a high-quality face image generator, as shown in Figure 4.6. Stylegan3 has been trained to generate faces, and we will apply the transfer learning process to retrain it, as shown in Figure 4.4.

Transfer learning is a technique in which an automated learning model that has been trained to perform a specific task is used as a starting point for training another model to perform a different task. Instead of training the new model from scratch, the knowledge and skills acquired by the original model are used to initiate the training of the new model in a more advanced state.

In this way, the time and effort required to train the new model are reduced, and its performance is improved.

For the re-training of Stylegan3, the Market-1501 database was used, consisting of 51,247 images of 1501 different people with images captured from six different cameras.

To measure the performance of Stylegan, the Fréchet inception distance (FID) metric was used, proposed by P. Dimitrakopoulos et al., 2017 (?). FID is a distance metric between two distributions of images that is used to measure their similarity. The FID metric is based on the idea that the distance between two distributions of images is the same as the distance between the features of the images extracted from a deep neural network. Therefore, to calculate the FID distance between two distributions of images, the features of each distribution are first extracted using a deep neural network, and then the distance between those features is calculated using the Fréchet distance.


Two methods were implemented to generate images, the first one is completely random, creating artificial people, and the second is by using a real image of a person and generating variations from it.

+ Generation of artificial person images.
Through a random number, also known as a seed, the Generator assigns a latent vector that corresponds to an image. Once the original image is obtained, variations can be generated by using another random latent vector through another seed or by interpolations. The latent vector is modified in different AdaIN layers of the model, also called style mixing, to obtain different variations of the original image. These variations can range from a total change in the structure of the image to more subtle changes such as changes in tone, lighting, colors, saturation, etc.
Another way to generate variations of the original image is by modifying the original latent vector through interpolations, as shown in Figure 4.8. Figure 4.7 shows how latent vectors are mixed to obtain variations of the original image.

+ Generation of artificial images of real people.
As explained earlier, a model, an encoder, needs to be trained to obtain the latent vector of a real image. In this case, the stylegan3-editing(57) model was used and trained on the PsP encoder developed by Richardson et al (? ). The encoder is part of a neural network that processes input information and converts it into an internal representation that can be used by Stylegan3 to generate the version of the real image within the latent space.
The LPIPS metric, Learned Perceptual Image Patch Similarity, is used as the loss function to compare the real image with the one obtained in the generator.

Once the images have been generated, it is necessary to automate their filtering. It may happen that images with noise or distortion have been generated.

To measure and discard the images generated by the generative adversarial network, metrics based on the quality of the generated data have been used

After generating the images, it is necessary to automate their filtering. It is possible that images with noise or distortion have been generated. To measure and discard the images generated by the adversarial generative network, metrics based on the quality of the generated data have been used.

The first filter applied is a model for pedestrian detection, and the structural similarity index measure (SSIM) has been used to measure the similarity of the generated images.

+ YoloV4 tiny filtering

YOLOv4 tiny is a reduced version of the YOLOv4 model, which is used to detect objects in images and videos. YOLO (You Only Look Once) is an object detection approach characterized by its speed and accuracy. The tiny version of YOLOv4 is especially useful for low-power devices, as it is less demanding in terms of resources and can be efficiently executed on mobile devices and low-performance computers.

In general terms, YOLOv4 tiny uses a convolutional neural network to extract features from an image and then uses a combination of machine learning techniques to perform object detection. The tiny version of YOLOv4 has been optimized to detect pedestrians with comparable precision and speed to larger models but with less resource demand. This makes it an excellent option for real-time applications on devices with limited capabilities.

+ SSIM Filtering

The Structural SIMilarity (SSIM) metric is a measure of structural similarity between two images. SSIM is often used to evaluate the quality of a processed image compared to an original image, and it is calculated by comparing the structural features of both images.

SSIM is based on the fact that human perception of image quality is based on its structural content, and not just the pixel difference between two images. Therefore, SSIM is used to measure the structural similarity between two images and give a score that reflects the perceived quality by a human observer.

### Re-identification model

A re-identification model is an algorithm used in image processing and artificial intelligence that allows for the identification and tracking of objects or people in a sequence of images. These models are based on the comparison of visual features between different images to determine if they correspond to the same object or person.

Mathematically, a re-identification model uses a similarity function to calculate the similarity between two images. This function takes two vectors of visual features (one from the reference image and the other from the image to be compared) and returns a value that indicates the similarity between the two images. If the value returned by the similarity function exceeds a certain threshold, it is determined that the images correspond to the same object or person.

To calculate the vectors of visual features, the model uses a neural network that has been previously trained with a set of labeled image data. The neural network extracts relevant features from the images and groups them into a feature vector. These vectors are then used in the similarity function to determine the similarity between the images.

The architecture proposed for this work can be seen in Figure 4.2. It uses a Resnet50 convolutional neural network where the last layer is modified so that the output depends on the number of people the model will be trained with. Cross entropy loss was used as the loss function during training.

A re-identification model is an algorithm used in image processing and artificial intelligence to identify and track objects or people in a sequence of images. These models are based on comparing visual features between different images to determine if they represent the same object or person.

Mathematically, a re-identification model uses a similarity function to calculate the similarity between two images. This function takes two vectors of visual features (one from the reference image and the other from the comparison image) and returns a value that indicates the similarity between the two images. If the value returned by the similarity function exceeds a certain threshold, it is determined that the images correspond to the same object or person.

To calculate the vectors of visual features, the model uses a neural network that has been previously trained with a set of labeled images. The neural network extracts relevant features from the images and groups them into a feature vector. These vectors are then used in the similarity function to determine the similarity between the images.

The proposed architecture for this work can be seen in Figure 4.2. It uses a convolutional neural network Resnet50 where the last layer is modified so that the output depends on the number of people with whom the model is to be trained. Cross entropy loss was used as the loss function during training.

Once training is completed, the model functions as a feature extractor. The next step is image classification. Each image to be evaluated is introduced into the model to obtain its respective feature vectors. To classify which images belong to the same person, each of the vectors of the images is compared with the vector of the original image using cosine distance.

Cosine distance is a measure of similarity between two vectors in a vector space. This measure is calculated using the cosine of the angle between the two vectors and can be interpreted as the projection of the shorter vector onto the longer vector.

After obtaining the cosine distance of all the images, they are sorted and the ones with the smallest cosine distance will be the ones that are closest to the original image, meaning that they have been detected as images of the same person

## Experimental Results

### Generation of artificial images

The generative adversarial network StyleGAN3 was used to generate artificial images. StyleGAN3 was pre-trained with 25 million face images, 70 thousand of which were real high-resolution 512 × 512 pixel images from the Flickr-Faces-HQ Dataset (FFHQ), and the rest were generated by the discriminator.

The next table shows the training characteristics of StyleGAN3.

| **cfg** | **gpus** | **batch** | **gamma** | **kimg** | **snap** | **metrics** |  
|---------|----------|-----------|-----------|----------|----------|-------------|
| stylegan3-r | 1 | 16 | 2 | 5000 | 20 | fid50k_full |

**Tabla 1**: Hyperparameters used during the training of Stylegan3. The parameter cfg (stylegan3-r) is used to determine the type of training config R or rotation equivalent, which prevents the deterioration of the FID metric measurement if the generated images are rotated or moved. It was trained on a GPU. batch (16) is the number of images that are fed into the network at each training iteration. gamma (2) is the R1 regularization weight, which indicates how fast the weights are updated. kimg (5000) is the total duration of the training. snap (20) indicates how often the model is saved, in this case, every 80,000 images. metrics (fid50k_full) is the metric used to measure the performance of the model during training..

Transfer learning was performed, and it was re-trained with 51,247 images from the Market-1501 database. Table 5.1 shows the hyperparameters used, and training performance was measured using the Fréchet Inception Distance (59) (FID) metric (see Table 5.1). This metric is applied to both the generated and real images, and the more similar the values of both are, the better the image generation. As shown in Table 5.4, the performance of StyleGAN3 is much better than that of other generative adversarial networks trained with the same database. Figure 5.2 shows examples of the capacity and quality of the StyleGAN3 model to generate artificial images compared to real ones.


## Additional material

- [Stylegan3 project](https://github.com/NVlabs/stylegan3)
- [Stylegan3 Model trained with Market 1506] (https://github.com/NVlabs/stylegan3)



After the artificial images were generated, two filters were applied to discard images that may have been generated incorrectly or contain noise.

+ YoloV4 tiny Filtering

The trained YoloV4 tiny model was used to detect pedestrians, and all generated images with a value below the threshold of 0.6 were discarded. This value was determined through analysis of Fig. 5.3, which shows the different percentages of images classified as non-pedestrians using different threshold values on real images from the Market-1501 database. When a threshold of 0.6 is used, the percentage of misclassified images as non-pedestrians is only 6.45%, making it a conservative value to use in filtering artificially generated images.

+ SSIM Filtering

The Structural SIMilarity (SSIM) metric was used to measure the similarity between two images. The methodology for applying this metric is as follows: one image of a person is chosen and compared to the rest of the images of that same person in different postures. If the value is equal to one, it means it is the same image. It was applied to the real images of the Market-1501 database, and the histogram was obtained (see Fig. 5.4). Using this histogram, images with an SSIM value lower than 0.75 were discarded.


| Método | Market-1501 FID | Referencia |
| --- | --- | --- |
| **Real** | **7.22** | Hao Chen *et al.*~\cite{Chen2020} |
| IS-GAN | 281.63 | Hao Chen *et al.*~\cite{Chen2020} |
| FD-GAN | 257.00 | Saleh Hussin *et al.*~\cite{Hussin2021} |
| PG-GAN | 151.16 | Zhedong Zheng *et al.*~\cite{Zheng2019} |
| DCGAN | 136.26 | Saleh Hussin *et al.*~\cite{Hussin2021} |
| LSGAN | 136.26 | Zhedong Zheng *et al.*~\cite{Zheng2019} |
| PN-GAN | 54.23 | Zhedong Zheng *et al.*~\cite{Zheng2019} |
| GCL | 53.07 | Hao Chen *et al.*~\cite{Chen2020} |
| DG-Net | 18.24 | Hao Chen *et al.*~\cite{Chen2020} |
| DG-GAN | 18.24 | Saleh Hussin *et al.*~\cite{Hussin2021} |
| **StyleGAN3** | **9.29** | |



## Usage

1. Clone the repository
git clone https://github.com/<your-username>/gan-for-reidentification.git


2. Install the required dependencies

pip install -r requirements.txt

3. Generate synthetic images using GANs
python generate_images.py --input <path-to-original-images> --output <path-to-synthetic-images>
  
  
  
## Acknowledgments

This project was inspired by the work of [SomeAwesomeResearcher](https://github.com/SomeAwesomeResearcher).

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
