<!DOCTYPE html>
<head>
    <meta charset="utf-8"/>
    <title>Generation of Images using Generative Adversarial Networks for Augmentation of Training Data in Re-identification Models</title>
    <meta content="This web page presents a thesis on the use of Generative Adversarial Networks (GANs), specifically StyleGAN3, for data augmentation to improve the training of re-identification models in the field of computer vision and machine learning."
          name="description"/>
    <meta content="Generation of Images using Generative Adversarial Networks for Augmentation of Training Data in Re-identification Models" property="og:title"/>
    <meta content="This web page presents a thesis on the use of Generative Adversarial Networks (GANs), specifically StyleGAN3, for data augmentation to improve the training of re-identification models in the field of computer vision and machine learning."
          property="og:description"/>
    <meta content="FIXME" property="og:image"/>
    <meta content="Generation of Images using Generative Adversarial Networks for Augmentation of Training Data in Re-identification Models" property="twitter:title"/>
    <meta content="This web page presents a thesis on the use of Generative Adversarial Networks (GANs), specifically StyleGAN3, for data augmentation to improve the training of re-identification models in the field of computer vision and machine learning."
          property="twitter:description"/>
    <meta content="FIXME" property="twitter:image"/>
    <meta property="og:type" content="website"/>
    <meta content="summary_large_image" name="twitter:card"/>
    <meta content="width=device-width, initial-scale=1" name="viewport"/>
    <link href="main.css"
          rel="stylesheet" type="text/css"/>
<!-- CHARTJS SCRIPTS -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.2.0/dist/chart.umd.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/chartjs-plugin-datalabels/2.2.0/chartjs-plugin-datalabels.min.js" integrity="sha512-JPcRR8yFa8mmCsfrw4TNte1ZvF1e3+1SdGMslZvmrzDYxS69J7J49vkFL8u6u8PlPJK+H3voElBtUCzaXj+6ig==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<!-- /CHARTJS SCRIPTS -->          

    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
    <script type="text/javascript">WebFont.load({google: {families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic", "Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic", "Ubuntu:300,300italic,400,400italic,500,500italic,700,700italic", "Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic", "Changa One:400,400italic", "Varela Round:400", "Bungee Shade:regular", "Roboto:300,regular,500"]}});</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"
            type="text/javascript"></script>
    <![endif]-->
    <script type="text/javascript">!function (o, c) {
        var n = c.documentElement, t = " w-mod-";
        n.className += t + "js", ("ontouchstart" in o || o.DocumentTouch && c instanceof DocumentTouch) && (n.className += t + "touch")
    }(window, document);</script>
    <link href="https://y7v4p6k4.ssl.hwcdn.net/51db7fcf29a6f36b2a000001/51e06d302f5394c87600002a_webclip-comet.png"
          rel="apple-touch-icon"/>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
    <style>
        .wf-loading * {
            opacity: 0;
        }
    </style>
</head>
<body>
<div class="github_logo w-embed"><a href="https://github.com/uselessai/person-reidentification" class="github-corner"
                                    aria-label="View source on Github">
    <svg width="80" height="80" viewBox="0 0 250 250"
         style="fill:#333333; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true">
        <defs>
            <mask id="octomask">
                <path fill="white" d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
                <path fill="black"
                      d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
                      style="transform-origin: 130px 106px;" class="octo-arm"></path>
                <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
                      fill="black" class="octo-body"></path>
            </mask>
        </defs>
        <rect class="filler" width="100%" height="100%" mask="url(#octomask)"></rect>
    </svg>
</a>
    <style>.github-corner:hover .octo-arm {
        animation: octocat-wave 560ms ease-in-out
    }

    @keyframes octocat-wave {
        0%, 100% {
            transform: rotate(0)
        }
        20%, 60% {
            transform: rotate(-25deg)
        }
        40%, 80% {
            transform: rotate(10deg)
        }
    }

    @media (max-width: 500px) {
        .github-corner:hover .octo-arm {
            animation: none
        }

        .github-corner .octo-arm {
            animation: octocat-wave 560ms ease-in-out
        }
    }</style>
</div>
<div class="section hero nerf-_v2 wf-section">
    <div class="container-2 nerf_header_v2 w-container"><h1 class="nerf_title_v2">Generation of Images using Generative Adversarial Networks for Augmentation of Training Data in Re-identification Models</h1>
        <h1 class="nerf_title_v2 charm_subtitle">State of the art</h1>
        <br>
        <p>
        <div class="nerf_authors_list_single w-row">
            <div class="w-col w-col-4 w-col-small-4 w-col-tiny-6"><a href="https://github.com/uselessai/" target="_blank" class="nerf_authors_v2">Laura Álvarez<sup>1</sup></a></div>
            <div class="column w-col w-col-4 w-col-small-4 w-col-tiny-6"><a href="--"  target="_blank" class="nerf_authors_v2">Víctor Uc Cetina<sup>1</sup></a></div>
            <div class="w-col w-col-4 w-col-small-4 w-col-tiny-6"><a href="" target="_blank" class="nerf_authors_v2">Anabel Alonso<sup>1</sup></a>
        </div>
        </p>


<!--        <br>-->
<!--        <hr style="height:1px;border-width:0;color:gray;background-color:lightgray">-->

        <div class="nerf_authors_list_single nerf_authors_affiliation w-row">
            <br>
            <div class="w-col w-col-5" style="text-align: right"><h1 class="nerf_affiliation_v2" style="font-size: 14px"><sup>1</sup>UADY - Universidad Autónoma de Yucatán</h1></div>
           
        </div>


   

        <div class="nerf_authors_list_single nerf_authors_affiliation w-row">
            <br>
            <br>
            <div class="w-col"><h1 class="nerf_affiliation_v2"><i style="font-size: 17px">-------------------------------------</i></h1></div>
            <br>
            <br>
        </div>

        <div style="margin-top:1cm; margin-bottom:1cm;" id="content-desktop">
            <a class="arxiv-button gradient-button" href="">Thesis</a>
            <a class="github-button gradient-button" href="https://github.com/uselessai/person-reidentification">Code</a>
        </div>
        <div style="margin-top:1cm; margin-bottom:1cm;" id="content-mobile">
            <a class="arxiv-button gradient-button" href="">Thesis</a>
            <a class="github-button gradient-button" href="https://github.com/uselessai/person-reidentification">Code</a>
        </div>
        
        
    </div>
</div>


<div data-anchor="slide1" class="section nerf_section">
    <div class="w-container"><h2 class="grey-heading_nerf">Abstract</h2>
        <p class="paragraph-3 nerf_text">
            Person re-identification is a technique used in the fields of artificial intelligence and machine learning to recognize a person in different images or videos, even if they have different angles, lighting, or clothing. This technique is used in applications such as security surveillance, digital image person identification, and video behavior analysis.

It is based on the use of machine learning models that learn to recognize the characteristics that identify a person in different images or videos. These models are trained with a dataset that contains images or videos of people, along with information about the characteristics that identify each person.

However, this can be challenging due to the variability of the characteristics that identify a person, such as clothing, hairstyle, and other aspects that can change their appearance. It can also be a challenge if there is a limited amount of training data due to privacy concerns of the people appearing in the images or videos.

To overcome these challenges, pre-processing techniques and deep learning techniques can be used to allow the model to adapt to variations in a person's appearance and recognize relevant features in low-quality images or videos.

Currently, the best training datasets are limited in size, such as Market1501 with 1501 people recorded with 6 different cameras or DukeMTMC-reID with 702 people in 8 different cameras. In this work, we investigate the use of a generative adversarial network, along with data augmentation techniques, to train a person re-identification model.

<!-- Different data augmentation techniques, such as image and feature augmentation, are analyzed, and their effectiveness in training a person re-identification model using a generative adversarial network is evaluated. The results obtained with the augmented dataset are compared with those obtained with a dataset that has not been augmented.-->
            <p>
        <p></p>
</div>


<div data-anchor="slide1" class="section nerf_section">
    <div class="w-container"><h2 class="grey-heading_nerf">State of the art</h2>
        <p class="paragraph-3 nerf_text">
            In the proposed work, we will analyze the state of the art in the use of generative adversarial networks (GANs)
             for data augmentation to improve the performance of re-identification models. 
            <!-- Specifically, we will investigate the effectiveness of GAN-based data augmentation techniques,
              such as image and feature augmentation, and evaluate their impact on the performance of re-identification models.
             The analysis will be conducted by comparing the performance of re-identification models trained with GAN-augmented 
             data to those trained on non-augmented data. The findings of this study could have important implications for 
             improving the accuracy and robustness of re-identification models, which are used in applications such as security surveillance,
              digital image identification, and video behavior analysis.-->
            <p>
                <p class="paragraph-3 nerf_text">
                    The article "Generative Adversarial Networks" by Ian Goodfellow et al. from 2014 introduces a new class of neural networks called generative adversarial networks (GANs). These networks are a type of machine learning model used to generate synthetic images or videos from a given dataset.
                    GANs are composed of two neural networks trained simultaneously, a generator and a discriminator. The generator network is responsible for generating synthetic images or videos, while the discriminator network evaluates the quality of the images or videos generated by the generator network.
                    The objective of GANs is for the generator network to generate images or videos that are as realistic as possible, so that the discriminator network cannot distinguish between real and synthetic images or videos generated by the generator network.
                    The article presents experiments that demonstrate the capacity of GANs to generate high-quality synthetic images or videos, and discusses the potential of these networks in applications such as 3D image generation, image or video quality enhancement, and behavior analysis in videos.
                    Since this first article, new architectures have been developed that improve the quality of generated images, such as the architecture proposed in 2017, CycleGAN, which is capable of improving the quality of generated images by transferring the style or domain of one group of images to another by combining two generative adversarial networks. Due to the improved performance of generative adversarial networks, they are being used to augment training data, which enables performance improvement in machine learning models where limited databases exist. As shown in the chart, a significant increase in the study of the use of generative adversarial networks for data augmentation in re-identification models can be observed from 2018.
                    The articles have been grouped into four categories corresponding to different methods used for generating new artificial images.
                    
                </p>
        


<div class="white_section_nerf wf-section">
    <div class="grey_container w-container"><h2 class="grey-heading_nerf">Domain-to-Domain transferring</h2>
        <p class="paragraph-3 nerf_text nerf_results_text">
            
            Through a real input image, artificial images are generated based on different styles, or domains, with which the model was trained. This allows the transfer of the style from one dataset to another, for example, transferring the style from an art painting to a real photo. The generated images show a modification with respect to the input image such as color, tonality, lighting, etc., but there is no change in the structure of the image.

    </p>
    <div class="video_class w-embed" style="width:85%;height:auto">
        <div class="video_class w-embed" style="width:100%;height:auto">
            <img src="transferenciadeestilos_stylegan3.png" alt="Domain-to-Domain transferring">
    </div>
    <p>
        Domain-to-Domain transferring. Through an input image, new images are generated with different styles.
    </p>
    </div>
</div>
</div>
<div class="white_section_nerf wf-section">
    <div class="w-container"><h2 class="grey-heading_nerf">Pose modification</h2>
        <p class="paragraph-3 nerf_text nerf_results_text">
            
            The aim of these GANs is for the generator to create images with different poses of people that are indistinguishable from real images in the training dataset. A real image of a person and a heat map or joint map corresponding to the skeleton of another pose are used as inputs. This enables an increase in the training dataset for a person re-identification model by adding images with different person poses.

    </p>
    <div class="video_class w-embed" style="width:85%;height:auto">
        <div class="video_class w-embed" style="width:100%;height:auto">
            <img src="extraccionpostura.png" alt="Pose modification">
            <img src="articulacionesmapadecalor.png" alt="Pose modification">
    </div>
    <p>
        Pose modification. Through an input image and a heat map or joints that correspond to the skeleton of a given posture, a new image of the person with the desired posture is generated. The objective of these GANs is to generate images with different postures that are indistinguishable from real images in the training dataset. This allows for an increase in the size of the training dataset for person re-identification models by adding images with different person postures.
    </p>
    </div>

</div>


<div class="white_section_nerf wf-section">
    <div class="grey_container w-container"><h2 class="grey-heading_nerf">Random artificial images</h2>
        <p class="paragraph-3 nerf_text nerf_results_text">
            
            Random artificial images are generated and labeled with techniques to train the re-identification model.

    </p>
    <div class="video_class w-embed" style="width:85%;height:auto">
        <div class="video_class w-embed" style="width:100%;height:auto">
            <img src="imagenesrealisyficticias.png" alt="Random artificial images">
    </div>
    <p>
        On the left, real images from the Market-1501 database. On the right, artificial images generated with Stylegan3.
    </p>
    </div>
</div>
</div>
<div class="white_section_nerf wf-section">
    <div class="w-container"><h2 class="grey-heading_nerf">State-of-the-art reviews</h2>
        <p class="paragraph-3 nerf_text nerf_results_text">
            
            State-of-the-art reviews are used by researchers and professionals to gain an overview of a research field and to 
            identify major trends and challenges in the area. They are also used as a basis for developing new research and projects 
            in the field. In this case, we focus on articles on the state of the art in the specific topic of using generative 
            adversarial networks to augment training data in re-identification models.

    </p>
   

</div>


<div class="white_section_nerf wf-section">
    <div class=" w-container"><h2 class="grey-heading_nerf">Chart</h2>


        <p class="paragraph-3 nerf_text nerf_results_text">
            State-of-the-art chart of the articles that have emerged over time, divided into the four categories previously described. 
            Each circle represents a year and each article is assigned a color corresponding to its category.
            Each article is clickable and redirects to the original one.
            <p><strong><font color="#3a2bff">State-of-the-art reviews</font> - 
                <font color="#008000">Random artificial images</font> - 
                <font color="#FF0000">Domain-to-Domain transferring</font> - 
                <font color="#fc9303">Pose modification</font></strong>
            </p>
            color_reviews = "#3a2bff" 
            color_articulaciones = "#fc9303"
            color_transfer = "#FF0000"
            color_random = "#008000"
            <p style="text-align: left;"><strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp
            <span style="background-color: #898989;"> <font color="white">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2021&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</font></span>
            <span style="background-color: #A4A4A4;"><font color="white">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2020&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</font></span>
            <span style="background-color: #BFBFBF;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2019&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
            <span style="background-color: #DBDBDB;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2018&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
            <span style="background-color: #F7F7F7;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2017&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
        </strong>    
        </p>
             
           
            

          

    </p>  

        <canvas id="myChart"></canvas>
        <br><br>
        <p style="text-align: left;">The chart has been automatically generated by converting a .bib file into a format compatible with ChartJS. You can find the code at the following <a href="https://github.com/uselessai/person-reidentification/blob/main/convert_bib_2_chartjs.py">link</a>.</p>
</div>
</div>

<br>

<div class="white_section_nerf wf-section">
    <div class="citation w-container"><h2 class="grey-heading_nerf">BibTeX</h2>
        <p class="paragraph-3 nerf_text nerf_results_text citation" style="background-color: #f2f2f2">
            <code style="color:#333333">
               <!-- @misc{alaluf2022times, <br> 
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;title={Generation of Images using Generative Adversarial Networks for Augmentation of Training Data in Re-identification Models Generation of artificial images using StyleGAN3}, <br> 
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;author={Yuval Alaluf and Or Patashnik and Zongze Wu and Asif Zamir and Eli Shechtman and Dani Lischinski and Daniel Cohen-Or}, <br> 
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;year={2022}, <br> 
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;eprint={2201.13433}, <br> 
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;archivePrefix={arXiv}, <br> 
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;primaryClass={cs.CV} <br>
                }-->
            </code>
        </p>
    </div>
</div>

<div class="white_section_nerf wf-section">
    <div class="citation w-container"><h2 class="grey-heading_nerf">Acknowledgements</h2>
        <p class="paragraph-3 nerf_text nerf_results_text">
            We would like to express our gratitude to William Peebles for generously allowing us to use his project page template, which can be found at <a href="https://www.wpeebles.com/gangealing">https://www.wpeebles.com/gangealing</a>.
        </p>
        <p class="paragraph-3 nerf_text nerf_results_text">
          The language model <a href="https://chat.openai.com">CHATGPT</a> was utilized to translate the primary text from Spanish to English.

        </p>      

    </div>
</div>
<script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=51e0d73d83d06baa7a00000f"
        type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
        crossorigin="anonymous"></script>
<script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd002feec.js"
        type="text/javascript"></script>



        <script>

            Chart.register(ChartDataLabels);
            const data = {
              datasets: [
[
{
label:'{Imaging A GAN-Based Self-Training Framework for Unsupervised Domain Adaptive Person Re-Identification}',
backgroundColor: ['#898989'],
abstract: ['{Imaging A GAN-Based Self-Training Framework for Unsupervised Domain Adaptive Person Re-Identification}'],
  URL: ['https://www.mdpi.com/2313-433X/7/4/62'],
  year: '2021',
  textColor: '#FF0000',
  data: [8.333333333333334],
},
{
label:'{Cross-Domain Person Re-Identification Based on Feature Fusion}',
backgroundColor: ['#898989'],
abstract: ['Person re-identification (ReID) is one of the commonly used criminal investigation methods in reconnaissance. Although the current ReID has achieved robust results on single domains, the focus of researches has shifted to cross-domain in recent years, which is caused by domain bias between different datasets. Generative Adversarial Networks (GAN) is used to realize the image style transfer of different datasets to alleviate the effect of cross-domain. However, the existing GAN-based models ignore complete expressions and occlusion of pedestrian characteristics, resulting in low accuracy in feature extraction. To address these issues, we introduce a cross domain model based on feature fusion (FFGAN) to fuse global, local and semantic features to extract more delicate pedestrian features. Before extracting pedestrian features, we preprocess feature maps with a feature erasure block to solve an occlusion issue. Finally, FFGAN enables a more complete visual description of pedestrian characteristics, thereby improving the accuracy of FFGAN in identifying pedestrians. Experimental results show that the effect of FFGAN is significantly improved compared with some advanced cross-domain ReID algorithms.'],
  URL: ['https://www.google.com/search?q=10.1109/ACCESS.2021.3091647'],
  year: '2021',
  textColor: '#FF0000',
  data: [8.333333333333334],
},
{
label:'{Cross-domain person re-identification by hybrid supervised and unsupervised learning}',
backgroundColor: ['#898989'],
abstract: ['Although the single-domain person re-identification (Re-ID) method has achieved great accuracy, the dependence on the label in the same image domain severely limits the scalability of this method. Therefore, cross-domain Re-ID has received more and more attention. In this paper, a novel cross-domain Re-ID method combining supervised and unsupervised learning is proposed, which includes two models: a triple-condition generative adversarial network (TC-GAN) and a dual-task feature extraction network (DFE-Net). We first use TC-GAN to generate labeled images with the target style, and then we combine supervised and unsupervised learning to optimize DFE-Net. Specifically, we use labeled generated data for supervised learning. In addition, we mine effective information in the target data from two perspectives for unsupervised learning. To effectively combine the two types of learning, we design a dynamic weighting function to dynamically adjust the weights of these two approaches. To verify the validity of TC-GAN, DFE-Net, and the dynamic weight function, we conduct multiple experiments on Market-1501 and DukeMTMC-reID. The experimental results show that the dynamic weight function can improve the performance of the models, and our method is better than many state-of-the-art methods.'],
  URL: ['https://www.google.com/search?q=10.1007/s10489-021-02551-8'],
  year: '2021',
  textColor: '#FF0000',
  data: [8.333333333333334],
},
{
label:'{Real-World Person Re-Identification via Super-Resolution and Semi-Supervised Methods}',
backgroundColor: ['#898989'],
abstract: ['Person re-identification has made great progress over the years. However, due to the problem of super-resolution and few labeled samples, it is difficult to apply in practice. In this paper, we propose a semi-supervised super-resolution person re-identification method based on soft multi-labels. Firstly, a Mixed-Space Super-Resolution model (MSSR) is constructed based on Generative Adversarial Networks (GAN), which aims to convert low-resolution person images into high-resolution images. Secondly, a Part-based Graph Convolutional Network (PGCN) is proposed to extract discriminative feature by exploring the relationship of local features within person. Finally, to solve the problem of label limitation, we use the PGCN trained with a small amount of labeled samples to predict the soft multi-labels of unlabeled samples, and further train PGCN with unlabeled samples based on a novel multi-label similarity loss. Experiments have been conducted on the Market1501, CUHK03, and MSMT17 datasets to evaluate this method, which show that it outperforms other semi-supervised methods.'],
  URL: ['https://www.google.com/search?q=10.1109/ACCESS.2021.3063000'],
  year: '2021',
  textColor: '#008000',
  data: [12.5],
},
{
label:'{StyleGAN-LSRO Method for Person Re-identification}',
backgroundColor: ['#898989'],
abstract: ['In this study, the StyleGAN-LSRO method has been developed for person re-identification (re-ID) tasks. This method applies the style-based generative adversarial network (StyleGAN) to generate new synthetic images from existing person re-ID datasets and the label smoothing regularization for outliers (LSRO) algorithm to process those newly produced unlabeled images by assigning them a uniform label distribution along with the definition of a loss function for the training process. A baseline model based on a convolutional neural network (CNN) was developed to learn the discriminative features to recognize a person{\&}{\#}x2019;s identity. The developed method has been tested on three datasets. These datasets are Market-1501, DukeMTMC-reID, and MSMT17. The experimental results show that the StyleGAN model achieved a Fr{\&}{\#}x00E9;chet inception distance score of 12.67 and structural similarity score of 0.387, outperforming all the previous generative methods and demonstrating that the images generated by StyleGAN are of superior quality. Adding these StyleGAN-generated data significantly improves the person re-ID accuracy. The StyleGAN-LSRO person re-ID method achieved 98.5{\%} rank-1 accuracy and 91.8{\%} mean average precision (mAP) on Market-1501, 87.0{\%} rank-1 accuracy and 83.8{\%} mAP on DukeMTMC-reID, and 81.5{\%} rank-1 accuracy and 60.9{\%} mAP on MSMT17, respectively. These results show that the StyleGAN-LSRO method significantly outperforms most of the state-of-the-art person re-ID methods. The success rate for person re-ID increases when the images used are of high resolution and square matrix form. In other cases, the success rate decreases.'],
  URL: ['https://www.google.com/search?q=10.1109/ACCESS.2021.3051723'],
  year: '2021',
  textColor: '#008000',
  data: [12.5],
},
{
label:'{Affine transform for skew correction based on generative adversarial network method for multi-camera person re-identification}',
backgroundColor: ['#898989'],
abstract: ['In intelligent video surveillance system, person re-identification is a key technology. In order to address the problem, the decrease in performance of person Re-Id lead by the skew pedestrian images, this paper proposes the affine transform for skew correction based on generative adversarial network (GAN) method for multi-camera person re-identification (Re-Id). Firstly, an effective GAN is proposed to guide the spatial transformer network (STN) to learn affine transform parameters for skew correction in an adversarial way, and STN is adopted as the preprocessing model for Re-Id to reduce influence of variations in person posture. Then, features are extracted by a deep convolutional neural network from input images which are corrected by STN, and finally results can be obtained by measuring similarity between features. Besides, in the proposed GAN, a classification model and related loss functions are introduced to reduce the damage to the key features of pedestrian during skew correction. The effectiveness of the proposed method is verified by experiments conducted on the skew pedestrian dataset.'],
  URL: ['https://www.google.com/search?q=10.1145/3449365.3449380'],
  year: '2021',
  textColor: '#fc9303',
  data: [25.0],
},
{
label:'{Review of GAN-Based Person Re-Identification}',
backgroundColor: ['#898989'],
abstract: ['{Review of GAN-Based Person Re-Identification}'],
  URL: ['https://www.google.com/search?q=10.32604/jnm.2021.018027'],
  year: '2021',
  textColor: '#3a2bff',
  data: [12.5],
},
{
label:'{Exploring the Quality of GAN Generated Images for Person Re-Identification}',
backgroundColor: ['#898989'],
abstract: ['Recently, GAN based method has demonstrated strong effectiveness in generating augmentation data for person re-identification (ReID), on account of its ability to bridge the gap between domains and enrich the data variety in feature space. However, most of the ReID works pick all the GAN generated data as additional training samples or evaluate the quality of GAN generation at the entire data set level, ignoring the image-level essential feature of data in ReID task. In this paper, we analyze the in-depth characteristics of ReID sample and solve the problem of "What makes a GAN-generated image good for ReID". Specifically, we propose to examine each data sample with id-consistency and diversity constraints by mapping image onto different spaces. With a metric-based sampling method, we demonstrate that not every GAN-generated data is beneficial for augmentation. Models trained with data filtered by our quality evaluation outperform those trained with the full augmentation set by a large margin. Extensive experiments show the effectiveness of our method on both supervised ReID task and unsupervised domain adaptation ReID task.'],
  URL: ['https://www.google.com/search?q=10.1145/3474085.3475547'],
  year: '2021',
  textColor: '#3a2bff',
  data: [12.5],
},
],
[
{
label:'{CGAN-TM: A Novel Domain-to-Domain Transferring Method for Person Re-Identification}',
backgroundColor: ['#A4A4A4'],
abstract: ['Person re-identification (re-ID) is a technique aiming to recognize person cross different cameras. Although some supervised methods have achieved favorable performance, they are far from practical application owing to the lack of labeled data. Thus, unsupervised person re-ID methods are in urgent need. Generally, the commonly used approach in existing unsupervised methods is to first utilize the source image dataset for generating a model in supervised manner, and then transfer the source image domain to the target image domain. However, images may lose their identity information after translation, and the distributions between different domains are far away. To solve these problems, we propose an image domain-to-domain translation method by keeping pedestrians identity information and pulling closer the domains distributions for unsupervised person re-ID tasks. Our work exploits the CycleGAN to transfer the existing labeled image domain to the unlabeled image domain. Specially, a Self-labeled Triplet Net is proposed to maintain the pedestrian identity information, and maximum mean discrepancy is introduced to pull the domain distribution closer. Extensive experiments have been conducted and the results demonstrate that the proposed method performs superiorly than the state-of-the-art unsupervised methods on DukeMTMC-reID and Market-1501.'],
  URL: ['https://www.google.com/search?q=10.1109/TIP.2020.2985545'],
  year: '2020',
  textColor: '#FF0000',
  data: [5.0],
},
{
label:'{Semi-supervised person re-identification by similarity-embedded cycle GANs}',
backgroundColor: ['#A4A4A4'],
abstract: ['Recently, person re-identification (PR-ID) has attracted numerous of research interest because of its broad applications. However, most of the existing PR-ID models always follow the supervised framework, which requires substantial labeled data. In fact, it is often very hard to get enough labeled training samples in many practical application scenarios. To overcome this limitation, some semi-supervised PR-ID methods have been presented more recently. Although some of these semi-supervised models achieve satisfied results, there is still much space to improve. In this paper, we propose a novel semi-supervised PR-ID by similarity-embedded cycle GANs (SECGAN). Our SECGAN model can learn cross-view features with limited labeled data by using cycle GANs. Simultaneously, to further enhance the ability of cycle GANs so that it can extract more discriminative and robust features, similarity metric subnet and specific features extracting subnet are embedded into cycle GANs. Extensive experiments have been conducted on three public PR-ID benchmark datasets, and the experimental results show that our proposed SECGAN approach outperforms several typical supervised methods and the existing state-of-the-art semi-supervised methods including traditional and deep learning semi-supervised methods.'],
  URL: ['https://doi.org/10.1007/s00521-020-04809-7'],
  year: '2020',
  textColor: '#FF0000',
  data: [5.0],
},
{
label:'{Unity style transfer for person re-identification}',
backgroundColor: ['#A4A4A4'],
abstract: ['Style variation has been a major challenge for person re-identification, which aims to match the same pedestrians across different cameras. Existing works attempted to address this problem with camera-invariant descriptor subspace learning. However, there will be more image artifacts when the difference between the images taken by different cameras is larger. To solve this problem, we propose a UnityStyle adaption method, which can smooth the style disparities within the same camera and across different cameras. Specifically, we firstly create UnityGAN to learn the style changes between cameras, producing shape-stable style-unity images for each camera, which is called UnityStyle images. Meanwhile, we use UnityStyle images to eliminate style differences between different images, which makes a better match between query and gallery. Then, we apply the proposed method to Re-ID models, expecting to obtain more style-robust depth features for querying. We conduct extensive experiments on widely used benchmark datasets to evaluate the performance of the proposed framework, the results of which confirm the superiority of the proposed model.'],
  URL: ['https://www.google.com/search?q=10.1109/CVPR42600.2020.00692'],
  year: '2020',
  textColor: '#FF0000',
  data: [5.0],
},
{
label:'{A Novel Method for Person Re-Identification: Conditional Translated Network Based on GANs}',
backgroundColor: ['#A4A4A4'],
abstract: ['The main challenge of person re-identification (re-id) lies in the strikingly discrepancy between different camera views, including illumination, background and human pose. Existing person re-id methods rely mostly on implicit solutions, such as seeking robust features or designing discriminative distance metrics. Compared to these methods, human solutions are more straightforward. That is, imagine the appearance of the target person under different camera views before matching target person. The key idea is that human can intuitively implement viewpoint transfer, noting the association of the target person under different camera views but the machine failed. In this paper, we attempt to imitate such human behavior that transfer person image to certain camera views before matching. In practice, we propose a conditional transfer network (cTransNet) that conditionally implement viewpoint transfer, which transfers image to the viewpoint with the biggest domain gap through a variant of Generative Adversarial Networks (GANs). After that, we obtain hybrid person representation by fusing the feature of original image with the transferred image then perform similarity ranking according to cosine distance. Compared with former methods, we propose a human-like approach and obtains consistent improvement of the rank-1 precision over the baseline in Market-1501, DukeMTMC-ReID and MSMT17 dataset by 3{\%},4{\%},4{\%}, respectively.'],
  URL: ['https://www.google.com/search?q=10.1109/ACCESS.2019.2962301'],
  year: '2020',
  textColor: '#FF0000',
  data: [5.0],
},
{
label:'{Unsupervised Disentanglement GAN for Domain Adaptive Person Re-Identification}',
backgroundColor: ['#A4A4A4'],
abstract: ['While recent person re-identification (ReID) methods achieve high accuracy in a supervised setting, their generalization to an unlabelled domain is still an open problem. In this paper, we introduce a novel unsupervised disentanglement generative adversarial network (UD-GAN) to address the domain adaptation issue of supervised person ReID. Our framework jointly trains a ReID network for discriminative features extraction in a source labelled domain using identity annotation, and adapts the ReID model to an unlabelled target domain by learning disentangled latent representations on the domain. Identity-unrelated features in the target domain are distilled from the latent features. As a result, the ReID features better encompass the identity of a person in the unsupervised domain. We conducted experiments on the Market1501, DukeMTMC and MSMT17 datasets. Results show that the unsupervised domain adaptation problem in ReID is very challenging. Nevertheless, our method shows improvement in half of the domain transfers and achieve state-of-the-art performance for one of them.'],
  URL: ['http://arxiv.org/abs/2007.15560'],
  year: '2020',
  textColor: '#FF0000',
  data: [5.0],
},
{
label:'{Joint Generative and Contrastive Learning for Unsupervised Person Re-identification}',
backgroundColor: ['#A4A4A4'],
abstract: ['Recent self-supervised contrastive learning provides an effective approach for unsupervised person re-identification (ReID) by learning invariance from different views (transformed versions) of an input. In this paper, we incorporate a Generative Adversarial Network (GAN) and a contrastive learning module into one joint training framework. While the GAN provides online data augmentation for contrastive learning, the contrastive module learns view-invariant features for generation. In this context, we propose a mesh-based view generator. Specifically, mesh projections serve as references towards generating novel views of a person. In addition, we propose a view-invariant loss to facilitate contrastive learning between original and generated views. Deviating from previous GAN-based unsupervised ReID methods involving domain adaptation, we do not rely on a labeled source dataset, which makes our method more flexible. Extensive experimental results show that our method significantly outperforms state-of-the-art methods under both, fully unsupervised and unsupervised domain adaptive settings on several large scale ReID datsets.'],
  URL: ['http://arxiv.org/abs/2012.09071'],
  year: '2020',
  textColor: '#008000',
  data: [25.0],
},
{
label:'{PAC-GAN: An effective pose augmentation scheme for unsupervised cross-view person re-identification}',
backgroundColor: ['#A4A4A4'],
abstract: ['Person re-identification (person Re-Id) aims to retrieve the pedestrian images of the same person that captured by disjoint and non-overlapping cameras. Lots of researchers recently focused on this hot issue and proposed deep learning based methods to enhance the recognition rate in a supervised or unsupervised manner. However,there are two limitations that cannot be ignored: firstly, compared with other image retrieval benchmarks, the size of existing person Re-Id datasets is far from meeting the requirement, which cannot provide sufficient pedestrian samples for the training of deep model; secondly, the samples in existing datasets do not have sufficient human motions or postures coverage to provide more priori knowledges for learning. In this paper, we introduce a novel unsupervised pose augmentation cross-view person Re-Id scheme called PAC-GAN to overcome these limitations. We firstly present the formal definition of cross-view pose augmentation and then propose the framework of PAC-GAN that is a novel conditional generative adversarial network (CGAN) based approach to improve the performance of unsupervised corss-view person Re-Id. Specifically, the pose generation model in PAC-GAN called CPG-Net is to generate enough quantity of pose-rich samples from original image and skeleton samples. The pose augmentation dataset is produced by combining the synthesized pose-rich samples with the original samples, which is fed into the corss-view person Re-Id model named Cross-GAN. Besides, we use weight-sharing strategy in the CPG-Net to improve the quality of new generated samples. To the best of our knowledge, we are the first to enhance the unsupervised cross-view person Re-Id by pose augmentation, and the results of extensive experiments show that the proposed scheme can combat the state-of-the-arts with recognition rate.'],
  URL: ['https://doi.org/10.1016/j.neucom.2019.12.094'],
  year: '2020',
  textColor: '#fc9303',
  data: [12.5],
},
{
label:'{PGAN: Part-based nondirect coupling embedded gan for person reidentification}',
backgroundColor: ['#A4A4A4'],
abstract: ['The block-based representation learning method has been proven to be a very effective method for person reidentification (Re-ID), but the features extracted by the existing block-based approach tend to have a high correlation among different blocks. Also, these methods perform less well for persons with large posture changes. Thus, part-based nondirect coupling representation learning method is proposed by introducing a similarity measure loss to constrain features of different blocks. Moreover, part-based nondirect coupling embedded GAN method is proposed, which aims to extract more common features of different postures of a same person. In this way, the extracted features of the network are robust for posture changes of a person, and there are no auxiliary pose information and additional computational cost required in the test stage. Experimental results on public datasets show that our proposed method achieves good performances, especially, it outperforms the state-of-the-art GAN-based methods for person Re-ID.'],
  URL: ['https://www.google.com/search?q=10.1109/MMUL.2020.2999445'],
  year: '2020',
  textColor: '#fc9303',
  data: [12.5],
},
{
label:'',
backgroundColor: ['#A4A4A4'],
abstract: [''],
  URL: [''],
  textColor: '',
  data: [25],
  year: '',
},
],
[
{
label:'{CamStyle: A Novel Data Augmentation Method for Person Re-Identification}',
backgroundColor: ['#BFBFBF'],
abstract: ['Person re-identification (re-ID) is a cross-camera retrieval task that suffers from image style variations caused by different cameras. The art implicitly addresses this problem by learning a camera-invariant descriptor subspace. In this paper, we explicitly consider this challenge by introducing camera style (CamStyle). CamStyle can serve as a data augmentation approach that reduces the risk of deep network overfitting and that smooths the CamStyle disparities. Specifically, with a style transfer model, labeled training images can be style transferred to each camera, and along with the original training samples, form the augmented training set. This method, while increasing data diversity against overfitting, also incurs a considerable level of noise. In the effort to alleviate the impact of noise, the label smooth regularization (LSR) is adopted. The vanilla version of our method (without LSR) performs reasonably well on few camera systems in which overfitting often occurs. With LSR, we demonstrate consistent improvement in all systems regardless of the extent of overfitting. We also report competitive accuracy compared with the state of the art on Market-1501 and DukeMTMC-re-ID. Importantly, CamStyle can be employed to the challenging problems of one view learning and unsupervised domain adaptation (UDA) in person re-identification (re-ID), both of which have critical research and application significance. The former only has labeled data in one camera view and the latter only has labeled data in the source domain. Experimental results show that CamStyle significantly improves the performance of the baseline in the two problems. Specially, for UDA, CamStyle achieves state-of-the-art accuracy based on a baseline deep re-ID model on Market-1501 and DukeMTMC-reID. Our code is available at: https://github.com/zhunzhong07/CamStyle.'],
  URL: ['https://www.google.com/search?q=10.1109/TIP.2018.2874313'],
  year: '2019',
  textColor: '#FF0000',
  data: [6.25],
},
{
label:'{Joint discriminative and generative learning for person re-identification}',
backgroundColor: ['#BFBFBF'],
abstract: ['Person re-identification (re-id) remains challenging due to significant intra-class variations across different cameras. Recently, there has been a growing interest in using generative models to augment training data and enhance the invariance to input changes. The generative pipelines in existing methods, however, stay relatively separate from the discriminative re-id learning stages. Accordingly, re-id models are often trained in a straightforward manner on the generated data. In this paper, we seek to improve learned re-id embeddings by better leveraging the generated data. To this end, we propose a joint learning framework that couples re-id learning and data generation end-to-end. Our model involves a generative module that separately encodes each person into an appearance code and a structure code, and a discriminative module that shares the appearance encoder with the generative module. By switching the appearance or structure codes, the generative module is able to generate high-quality cross-id composed images, which are online fed back to the appearance encoder and used to improve the discriminative module. The proposed joint learning framework renders significant improvement over the baseline without using generated data, leading to the state-of-the-art performance on several benchmark datasets.'],
  URL: ['https://www.google.com/search?q=10.1109/CVPR.2019.00224'],
  year: '2019',
  textColor: '#FF0000',
  data: [6.25],
},
{
label:'{Multi-camera transfer GAN for person re-identification}',
backgroundColor: ['#BFBFBF'],
abstract: ['Person re-identification is a cross-camera retrieval task. Person re-identification performance in a single dataset has been significantly improved, but person re-identification model trained in one dataset usually cant work well in another dataset. To solve this problem, this paper proposes a method of image-to-image translation, CTGAN (Multi-Camera Transfer GAN), which can be performed on multiple camera domains of pedestrian dataset by using one single model. The marked training images are transferred to each camera of the target dataset. At the same time, for the feature learning model, this paper adopts the MSCDA (Mixed Selective Convolution Descriptor Aggregation) method, which can locate the main pedestrian objects in the image, filter out the background noise, and keep the useful depth descriptor. In the paper, experiments show that the method is effective.'],
  URL: ['https://doi.org/10.1016/j.jvcir.2019.01.029'],
  year: '2019',
  textColor: '#FF0000',
  data: [6.25],
},
{
label:'{Color-sensitive person re-identification}',
backgroundColor: ['#BFBFBF'],
abstract: ['Recent deep Re-ID models mainly focus on learning high-level semantic features, while failing to explicitly explore color information which is one of the most important cues for person Re-ID model. In this paper, we propose a novel Color-Sensitive Re-ID to take full advantage of color information. On one hand, we train our model with real and fake images. By using the extra fake images, more color information can be exploited and it can avoid over-fitting during training. On the other hand, we also train our model with images of the same person with different colors. By doing so, features can be forced to focus on the color difference in regions. To generate fake images with specified colors, we propose a novel Color Translation GAN (CTGAN) to learn mappings between different clothing colors and preserve identity consistency among the same person with the clothing color. Extensive evaluations on two benchmark datasets show that our approach significantly outperforms state-of-the-art Re-ID models.'],
  URL: ['https://www.google.com/search?q=10.24963/ijcai.2019/131'],
  year: '2019',
  textColor: '#FF0000',
  data: [6.25],
},
{
label:'{Multi-pseudo regularized label for generated data in person re-identification}',
backgroundColor: ['#BFBFBF'],
abstract: ['Sufficient training data normally is required to train deeply learned models. However, due to the expensive manual process for a labeling large number of images (i.e., annotation), the amount of available training data (i.e., real data) is always limited. To produce more data for training a deep network, generative adversarial network can be used to generate artificial sample data (i.e., generated data). However, the generated data usually does not have annotation labels. To solve this problem, in this paper, we propose a virtual label called Multi-pseudo Regularized Label (MpRL) and assign it to the generated data. With MpRL, the generated data will be used as the supplementary of real training data to train a deep neural network in a semi-supervised learning fashion. To build the corresponding relationship between the real data and generated data, MpRL assigns each generated data a proper virtual label which reflects the likelihood of the affiliation of the generated data to pre-defined training classes in the real data domain. Unlike the traditional label which usually is a single integral number, the virtual label proposed in this paper is a set of weight-based values each individual of which is a number in (0,1] called multi-pseudo label and reflects the degree of relation between each generated data to every pre-defined class of real data. A comprehensive evaluation is carried out by adopting two state-of-the-art convolutional neural networks (CNNs) in our experiments to verify the effectiveness of MpRL. Experiments demonstrate that by assigning MpRL to generated data, we can further improve the person re-ID performance on five re-ID datasets, i.e., Market-1501, DukeMTMC-reID, CUHK03, VIPeR, and CUHK01. The proposed method obtains +6.29{\%}, +6.30{\%}, +5.58{\%}, +5.84{\%}, and +3.48{\%} improvements in rank-1 accuracy over a strong CNN baseline on the five datasets, respectively, and outperforms state-of-the-art methods.'],
  URL: ['https://www.google.com/search?q=10.1109/TIP.2018.2874715'],
  year: '2019',
  textColor: '#008000',
  data: [8.333333333333334],
},
{
label:'{Sparse Label Smoothing Regularization for Person Re-Identification}',
backgroundColor: ['#BFBFBF'],
abstract: ['Person re-identification (re-id) is a cross-camera retrieval task which establishes a correspondence between images of a person from multiple cameras. Deep learning methods have been successfully applied to this problem and have achieved impressive results. However, these methods require a large amount of labeled training data. Currently, the labeled datasets in person re-id are limited in their scale and manual acquisition of such large-scale datasets from surveillance cameras is a tedious and labor-intensive task. In this paper, we propose a framework that performs intelligent data augmentation and assigns the partial smoothing label to generated data. Our approach first exploits the clustering property of existing person re-id datasets to create groups of similar objects that model cross-view variations. Each group is then used to generate realistic images through adversarial training. Our aim is to emphasize the feature similarity between generated samples and the original samples. Finally, we assign a non-uniform label distribution to the generated samples and define a regularized loss function for training. The proposed approach tackles two problems 1) how to efficiently use the generated data and 2) how to address the over-smoothness problem found in current regularization methods. The extensive experiments on four large-scale datasets show that our regularization method significantly improves the re-id accuracy compared to existing methods.'],
  URL: ['https://www.google.com/search?q=10.1109/ACCESS.2019.2901599'],
  year: '2019',
  textColor: '#008000',
  data: [8.333333333333334],
},
{
label:'{Learning disentangled representation for robust person re-identification}',
backgroundColor: ['#BFBFBF'],
abstract: ['We address the problem of person re-identification (reID), that is, retrieving person images from a large dataset, given a query image of the person of interest. A key challenge is to learn person representations robust to intra-class variations, as different persons can have the same attribute and the same persons appearance looks different with viewpoint changes. Recent reID methods focus on learning discriminative features but robust to only a particular factor of variations (e.g., human pose), which requires corresponding supervisory signals (e.g., pose annotations). To tackle this problem, we propose to disentangle identity-related and -unrelated features from person images. Identity-related features contain information useful for specifying a particular person (e.g., clothing), while identity-unrelated ones hold other factors (e.g., human pose, scale changes). To this end, we introduce a new generative adversarial network, dubbed identity shuffle GAN (IS-GAN), that factorizes these features using identification labels without any auxiliary information. We also propose an identity-shuffling technique to regularize the disentangled features. Experimental results demonstrate the effectiveness of IS-GAN, significantly outperforming the state of the art on standard reID benchmarks including the Market-1501, CUHK03 and DukeMTMC-reID. Our code and models are available online: https://cvlab-yonsei.github.io/projects/ISGAN/.'],
  URL: ['https://www.google.com/search?q={Learning disentangled representation for robust person re-identification}'],
  year: '2019',
  textColor: '#008000',
  data: [8.333333333333334],
},
{
label:'{Instance-guided context rendering for cross-domain person re-identification}',
backgroundColor: ['#BFBFBF'],
abstract: ['Existing person re-identification (re-id) methods mostly assume the availability of large-scale identity labels for model learning in any target domain deployment. This greatly limits their scalability in practice. To tackle this limitation, we propose a novel Instance-Guided Context Rendering scheme, which transfers the source person identities into diverse target domain contexts to enable supervised re-id model learning in the unlabelled target domain. Unlike previous image synthesis methods that transform the source person images into limited fixed target styles, our approach produces more visually plausible, and diverse synthetic training data. Specifically, we formulate a dual conditional generative adversarial network that augments each source person image with rich contextual variations. To explicitly achieve diverse rendering effects, we leverage abundant unlabelled target instances as contextual guidance for image generation. Extensive experiments on Market-1501, DukeMTMC-reID and CUHK03 benchmarks show that the re-id performance can be significantly improved when using our synthetic data in cross-domain re-id model learning.'],
  URL: ['https://www.google.com/search?q=10.1109/ICCV.2019.00032'],
  year: '2019',
  textColor: '#fc9303',
  data: [12.5],
},
{
label:'{GAN-based Pose-aware Regulation for Video-based Person Re- identification GAN-based Pose-aware Regulation for Video-based Person Re-identification}',
backgroundColor: ['#BFBFBF'],
abstract: ['{GAN-based Pose-aware Regulation for Video-based Person Re- identification GAN-based Pose-aware Regulation for Video-based Person Re-identification}'],
  URL: ['https://www.google.com/search?q={GAN-based Pose-aware Regulation for Video-based Person Re- identification GAN-based Pose-aware Regulation for Video-based Person Re-identification}'],
  year: '2019',
  textColor: '#fc9303',
  data: [12.5],
},
{
label:'{An introduction to person re-identification with generative adversarial networks}',
backgroundColor: ['#BFBFBF'],
abstract: ['Person re-identification is a basic subject in the field of computer vision. The traditional methods have several limitations in solving the problems of person illumination like occlusion, pose variation and feature variation under complex background. Fortunately, deep learning paradigm opens new ways of the person re-identification research and becomes a hot spot in this field. Generative Adversarial Nets (GANs) in the past few years attracted lots of attention in solving these problems. This paper reviews the GAN based methods for person re-identification focuses on the related papers about different GAN based frameworks and discusses their advantages and disadvantages. Finally, it proposes the direction of future research, especially the prospect of person reidentification methods based on GANs.'],
  URL: ['https://www.google.com/search?q={An introduction to person re-identification with generative adversarial networks}'],
  year: '2019',
  textColor: '#3a2bff',
  data: [25.0],
},
],
[
{
label:'{Camera Style Adaptation for Person Re-identification}',
backgroundColor: ['#DBDBDB'],
abstract: ['Being a cross-camera retrieval task, person re-identification suffers from image style variations caused by different cameras. The art implicitly addresses this problem by learning a camera-invariant descriptor subspace. In this paper, we explicitly consider this challenge by introducing camera style (CamStyle) adaptation. CamStyle can serve as a data augmentation approach that smooths the camera style disparities. Specifically, with CycleGAN, labeled training images can be style-transferred to each camera, and, along with the original training samples, form the augmented training set. This method, while increasing data diversity against over-fitting, also incurs a considerable level of noise. In the effort to alleviate the impact of noise, the label smooth regularization (LSR) is adopted. The vanilla version of our method (without LSR) performs reasonably well on few-camera systems in which over-fitting often occurs. With LSR, we demonstrate consistent improvement in all systems regardless of the extent of over-fitting. We also report competitive accuracy compared with the state of the art. Code is available at: Https://github.com/zhunzhong07/CamStyle.'],
  URL: ['https://www.google.com/search?q=10.1109/CVPR.2018.00541'],
  year: '2018',
  textColor: '#FF0000',
  data: [6.25],
},
{
label:'{M2M-GAN: Many-to-Many Generative Adversarial Transfer Learning for Person Re-Identification}',
backgroundColor: ['#DBDBDB'],
abstract: ['Cross-domain transfer learning (CDTL) is an extremely challenging task for the person re-identification (ReID). Given a source domain with annotations and a target domain without annotations, CDTL seeks an effective method to transfer the knowledge from the source domain to the target domain. However, such a simple two-domain transfer learning method is unavailable for the person ReID in that the source/target domain consists of several sub-domains, e.g., camera-based sub-domains. To address this intractable problem, we propose a novel Many-to-Many Generative Adversarial Transfer Learning method (M2M-GAN) that takes multiple source sub-domains and multiple target sub-domains into consideration and performs each sub-domain transferring mapping from the source domain to the target domain in a unified optimization process. The proposed method first translates the image styles of source sub-domains into that of target sub-domains, and then performs the supervised learning by using the transferred images and the corresponding annotations in source domain. As the gap is reduced, M2M-GAN achieves a promising result for the cross-domain person ReID. Experimental results on three benchmark datasets Market-1501, DukeMTMC-reID and MSMT17 show the effectiveness of our M2M-GAN.'],
  URL: ['http://arxiv.org/abs/1811.03768'],
  year: '2018',
  textColor: '#FF0000',
  data: [6.25],
},
{
label:'{Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification}',
backgroundColor: ['#DBDBDB'],
abstract: ['Person re-identification (re-ID) models trained on one domain often fail to generalize well to another. In our attempt, we present a learning via translation framework. In the baseline, we translate the labeled images from source to target domain in an unsupervised manner. We then train re-ID models with the translated images by supervised methods. Yet, being an essential part of this framework, unsupervised image-image translation suffers from the information loss of source-domain labels during translation. Our motivation is two-fold. First, for each image, the discriminative cues contained in its ID label should be maintained after translation. Second, given the fact that two domains have entirely different persons, a translated image should be dissimilar to any of the target IDs. To this end, we propose to preserve two types of unsupervised similarities, 1) self-similarity of an image before and after translation, and 2) domain-dissimilarity of a translated source image and a target image. Both constraints are implemented in the similarity preserving generative adversarial network (SPGAN) which consists of an Siamese network and a CycleGAN. Through domain adaptation experiment, we show that images generated by SPGAN are more suitable for domain adaptation and yield consistent and competitive re-ID accuracy on two large-scale datasets.'],
  URL: ['https://www.google.com/search?q=10.1109/CVPR.2018.00110'],
  year: '2018',
  textColor: '#FF0000',
  data: [6.25],
},
{
label:'{Cross-modality person re-identification with generative adversarial training}',
backgroundColor: ['#DBDBDB'],
abstract: ['Person re-identification (Re-ID) is an important task in video surveillance which automatically searches and identifies people across different cameras. Despite the extensive Re-ID progress in RGB cameras, few works have studied the Re-ID between infrared and RGB images, which is essentially a cross-modality problem and widely encountered in real-world scenarios. The key challenge lies in two folds, i.e., the lack of discriminative information to re-identify the same person between RGB and infrared modalities, and the difficulty to learn a robust metric for such a large-scale cross-modality retrieval. In this paper, we tackle the above two challenges by proposing a novel cross-modality generative adversarial network (termed cmGAN). To handle the lack of insufficient discriminative information, we design a cutting-edge generative adversarial training based discriminator to learn discriminative feature representation from different modalities. To handle the issue of large-scale cross-modality metric learning, we integrate both identification loss and cross-modality triplet loss, which minimize inter-class ambiguity while maximizing cross-modality similarity among instances. The entire cmGAN can be trained in an end-to-end manner by using standard deep neural network framework. We have quantized the performance of our work in the newly-released SYSU RGB-IR Re-ID benchmark, and have reported superior performance, i.e., Cumulative Match Characteristic curve (CMC) and Mean Average Precision (MAP), over the state-of-the-art works [Wu et al., 2017], at least 12.17{\%} and 11.85{\%} respectively.'],
  URL: ['https://www.google.com/search?q=10.24963/ijcai.2018/94'],
  year: '2018',
  textColor: '#FF0000',
  data: [6.25],
},
{
label:'',
backgroundColor: ['#DBDBDB'],
abstract: [''],
  URL: [''],
  textColor: '',
  data: [25],
  year: '',
},
{
label:'{Pose-Normalized Image Generation for}',
backgroundColor: ['#DBDBDB'],
abstract: ['{Pose-Normalized Image Generation for}'],
  URL: ['http://openaccess.thecvf.com/content{\_}ECCV{\_}2018/html/Xuelin{\_}Qian{\_}Pose-Normalized{\_}Image{\_}Generation{\_}ECCV{\_}2018{\_}paper.html'],
  year: '2018',
  textColor: '#fc9303',
  data: [6.25],
},
{
label:'{FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification}',
backgroundColor: ['#DBDBDB'],
abstract: ['Person re-identification (reID) is an important task that requires to retrieve a persons images from an image dataset, given one image of the person of interest. For learning robust person features, the pose variation of person images is one of the key challenges. Existing works targeting the problem either perform human alignment, or learn human-region-based representations. Extra pose information and computational cost is generally required for inference. To solve this issue, a Feature Distilling Generative Adversarial Network (FD-GAN) is proposed for learning identity-related and pose-unrelated representations. It is a novel framework based on a Siamese structure with multiple novel discriminators on human poses and identities. In addition to the discriminators, a novel same-pose loss is also integrated, which requires appearance of a same persons generated images to be similar. After learning pose-unrelated person features with pose guidance, no auxiliary pose information and additional computational cost is required during testing. Our proposed FD-GAN achieves state-of-the-art performance on three person reID datasets, which demonstrates that the effectiveness and robust feature distilling capability of the proposed FD-GAN.'],
  URL: ['https://www.google.com/search?q={FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification}'],
  year: '2018',
  textColor: '#fc9303',
  data: [6.25],
},
{
label:'{A unified generative adversarial framework for image generation and person re-identification}',
backgroundColor: ['#DBDBDB'],
abstract: ['Person re-identification (re-id) aims to match a certain person across multiple non-overlapping cameras. It is a challenging task because the same persons appearance can be very different across camera views due to the presence of large pose variations. To overcome this issue, in this paper, we propose a novel unified person re-id framework by exploiting person poses and identities jointly for simultaneous person image synthesis under arbitrary poses and pose-invariant person re-identification. The framework is composed of a GAN based network and two Feature Extraction Networks (FEN), and enjoys following merits. First, it is a unified generative adversarial model for person image generation and person re-identification. Second, a pose estimator is utilized into the generator as a supervisor in the training process, which can effectively help pose transfer and guide the image generation with any desired pose. As a result, the proposed model can automatically generate a person image under an arbitrary pose. Third, the identity-sensitive representation is explicitly disentangled from pose variations through the person identity and pose embedding. Fourth, the learned re-id model can have better generalizability on a new person re-id dataset by using the synthesized images as auxiliary samples. Extensive experimental results on four standard benchmarks including Market-1501 [69], DukeMTMC-reID [40], CUHK03 [23], and CUHK01 [22] demonstrate that the proposed model can perform favorably against state-of-the-art methods.'],
  URL: ['https://www.google.com/search?q=10.1145/3240508.3240573'],
  year: '2018',
  textColor: '#fc9303',
  data: [6.25],
},
{
label:'{Deformable GANs for Pose-Based Human Image Generation}',
backgroundColor: ['#DBDBDB'],
abstract: ['In this paper we address the problem of generating person images conditioned on a given pose. Specifically, given an image of a person and a target pose, we synthesize a new image of that person in the novel pose. In order to deal with pixel-to-pixel misalignments caused by the pose differences, we introduce deformable skip connections in the generator of our Generative Adversarial Network. Moreover, a nearest-neighbour loss is proposed instead of the common L1 and L2 losses in order to match the details of the generated image with the target image. We test our approach using photos of persons in different poses and we compare our method with previous work in this area showing state-of-the-art results in two benchmarks. Our method can be applied to the wider field of deformable object generation, provided that the pose of the articulated object can be extracted using a keypoint detector.'],
  URL: ['https://www.google.com/search?q=10.1109/CVPR.2018.00359'],
  year: '2018',
  textColor: '#fc9303',
  data: [6.25],
},
{
label:'',
backgroundColor: ['#DBDBDB'],
abstract: [''],
  URL: [''],
  textColor: '',
  data: [25],
  year: '',
},
],
[
{
label:'',
backgroundColor: ['#F7F7F7'],
abstract: [''],
  URL: [''],
  textColor: '',
  data: [25],
  year: '',
},
{
label:'{Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in Vitro}',
backgroundColor: ['#F7F7F7'],
abstract: ['The main contribution of this paper is a simple semisupervised pipeline that only uses the original training set without collecting extra data. It is challenging in 1) how to obtain more training data only from the training set and 2) how to use the newly generated data. In this work, the generative adversarial network (GAN) is used to generate unlabeled samples. We propose the label smoothing regularization for outliers (LSRO). This method assigns a uniform label distribution to the unlabeled images, which regularizes the supervised model and improves the baseline. We verify the proposed method on a practical problem: person re-identification (re-ID). This task aims to retrieve a query person from other cameras. We adopt the deep convolutional generative adversarial network (DCGAN) for sample generation, and a baseline convolutional neural network (CNN) for representation learning. Experiments show that adding the GAN-generated data effectively improves the discriminative ability of learned CNN embeddings. On three large-scale datasets, Market- 1501, CUHK03 and DukeMTMC-reID, we obtain +4.37{\%}, +1.6{\%} and +2.46{\%} improvement in rank-1 precision over the baseline CNN, respectively. We additionally apply the proposed method to fine-grained bird recognition and achieve a +0.6{\%} improvement over a strong baseline. The code is available at https://github.com/layumi/ Person-reID-GAN.'],
  URL: ['https://www.google.com/search?q=10.1109/ICCV.2017.405'],
  year: '2017',
  textColor: '#008000',
  data: [25.0],
},
{
label:'',
backgroundColor: ['#F7F7F7'],
abstract: [''],
  URL: [''],
  textColor: '',
  data: [25],
  year: '',
},
{
label:'',
backgroundColor: ['#F7F7F7'],
abstract: [''],
  URL: [''],
  textColor: '',
  data: [25],
  year: '',
},
],
]


          };
          
          
           ctx = document.getElementById('myChart').getContext('2d');
          
          
          
          
           myChart = new Chart(ctx, {
            type: 'pie', //doughnut or pie
            data: {
              datasets: data.datasets.map(function(dataset) {
                return {
                  backgroundColor: dataset.map(function(d) { return d.backgroundColor; }),
                  data: dataset.map(function(d) { return d.data; }),
                  year: dataset.map(function(d) { return d.year; }),
                  urls: dataset.map(function(d) { return d.URL; }),
                  textColor:  dataset.map(function(d) { return d.textColor; }),
               
                };
              })
            },
            
            options: {
              
              aspectRatio: 1,
              plugins: {
                

                







                datalabels: {

                  labels: {
                    title: {
                      font: {
                        weight: 'bold',
                        size: 14,
                      }
                    }},

                color: function(context) {
                   return data.datasets[context.datasetIndex][context.dataIndex].textColor;
                },
                align: 'center',
                formatter: function(value, context) {
                  if (context.datasetIndex ==3){
                    return  data.datasets[context.datasetIndex][context.dataIndex].label.slice(1, 5) + "\n" + data.datasets[context.datasetIndex][context.dataIndex].label.slice(5, 10) +  "...";
                  }

                  return  data.datasets[context.datasetIndex][context.dataIndex].label.slice(1, 10) + "\n" + data.datasets[context.datasetIndex][context.dataIndex].label.slice(10, 20) + "\n" + data.datasets[context.datasetIndex][context.dataIndex].label.slice(20, 30) + "...";
                  
                },font: {
                    weight: 'bold',
                  },
                  
              },
                tooltip: {
                  callbacks: {
                    label: function(context) {
                     
                      console.log (context)
                      console.log (context.datasetIndex)
                      console.log (context.dataIndex)
                      label = data.datasets[context.datasetIndex][context.dataIndex].label;
                      year = data.datasets[context.datasetIndex][context.dataIndex].year;
                      console.log (label)
                      return year + " : " + label  ;
                    }
                  }
                }
              },
              showTooltips: true,
              responsive: true,
              onClick: function(e, item) {
                  if (item.length) {
                      console.log (item)
                      console.log (item[0].datasetIndex) // indica nivel del pastel
                      console.log (item[0].index) // indica dentro del nivel la seccion
                      console.log (data.datasets[item[0].datasetIndex][item[0].index].URL)
                      window.open(data.datasets[item[0].datasetIndex][item[0].index].URL, "_blank");
                  }
              },
            
            
            
            },
            
          });
          
              
          
              
          
              </script>












<!--[if lte IE 9]>
<script src="//cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]--></body>

















</html>
