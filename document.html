<!DOCTYPE html><!-- Last Published: Sat Sep 10 2022 23:01:05 GMT+0000 (Coordinated Universal Time) --><html data-wf-domain="www.matthewtancik.com" data-wf-page="5e6fb768456f961381500a5f" data-wf-site="51e0d73d83d06baa7a00000f" class="wf-loading w-mod-js wf-lato-n1-loading wf-lato-i1-loading wf-lato-n3-loading wf-lato-i3-loading wf-lato-n4-loading wf-lato-i4-loading wf-lato-n7-loading wf-lato-i7-loading wf-lato-n9-loading wf-lato-i9-loading wf-montserrat-n1-loading wf-montserrat-i1-loading wf-montserrat-n2-loading wf-montserrat-i2-loading wf-montserrat-n3-loading wf-montserrat-i3-loading wf-montserrat-n4-loading wf-montserrat-i4-loading wf-montserrat-n5-loading wf-montserrat-i5-loading wf-montserrat-n6-loading wf-montserrat-i6-loading wf-montserrat-n7-loading wf-montserrat-i7-loading wf-montserrat-n8-loading wf-montserrat-i8-loading wf-montserrat-n9-loading wf-montserrat-i9-loading wf-ubuntu-n3-loading wf-ubuntu-i3-loading wf-ubuntu-n4-loading wf-ubuntu-i4-loading wf-ubuntu-n5-loading wf-ubuntu-i5-loading wf-ubuntu-n7-loading wf-ubuntu-i7-loading wf-opensans-n3-loading wf-opensans-i3-loading wf-opensans-n4-loading wf-opensans-i4-loading wf-opensans-n6-loading wf-opensans-i6-loading wf-opensans-n7-loading wf-opensans-i7-loading wf-opensans-n8-loading wf-opensans-i8-loading wf-changaone-n4-loading wf-changaone-i4-loading wf-varelaround-n4-loading wf-bungeeshade-n4-loading wf-roboto-n3-loading wf-roboto-n4-loading wf-roboto-n5-loading wf-bungeeoutline-n4-loading"><head><meta charset="utf-8"><title>NeRF: Neural Radiance Fields</title><meta content="A method for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views." name="description"><meta content="NeRF: Neural Radiance Fields" property="og:title"><meta content="A method for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views." property="og:description"><meta content="http://people.eecs.berkeley.edu/~tancik/nerf/website_renders/images/nerf_graph.jpg" property="og:image"><meta content="NeRF: Neural Radiance Fields" property="twitter:title"><meta content="A method for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views." property="twitter:description"><meta content="http://people.eecs.berkeley.edu/~tancik/nerf/website_renders/images/nerf_graph.jpg" property="twitter:image"><meta property="og:type" content="website"><meta content="summary_large_image" name="twitter:card"><meta content="width=device-width, initial-scale=1" name="viewport"><link href="document_files/matthewtancik.webflow.f153671fa.min.css" rel="stylesheet" type="text/css"><script src="document_files/webfont.js" type="text/javascript"></script><link rel="stylesheet" href="document_files/css.css?family=Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic%7CMontserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic%7CUbuntu:300,300italic,400,400italic,500,500italic,700,700italic%7COpen+Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic%7CChanga+One:400,400italic%7CVarela+Round:400%7CBungee+Shade:regular%7CRoboto:300,regular,500%7CBungee+Outline:regular" media="all"><script type="text/javascript">WebFont.load({  google: {    families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic","Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic","Ubuntu:300,300italic,400,400italic,500,500italic,700,700italic","Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic","Changa One:400,400italic","Varela Round:400","Bungee Shade:regular","Roboto:300,regular,500","Bungee Outline:regular"]  }});</script><!--[if lt IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" type="text/javascript"></script><![endif]--><script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script><link href="document_files/5aaddbdee8d43ceeae2f2e4c_favicon-32x32.png" rel="shortcut icon" type="image/x-icon"><link href="https://y7v4p6k4.ssl.hwcdn.net/51db7fcf29a6f36b2a000001/51e06d302f5394c87600002a_webclip-comet.png" rel="apple-touch-icon"><script type="text/javascript">var _gaq = _gaq || [];_gaq.push(['_setAccount', 'UA-46779055-1'], ['_trackPageview']);(function() {  var ga = document.createElement('script');  ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';  var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);})();</script><style>.wf-loading * { opacity: 0; }</style></head><body><div class="github_logo w-embed"><a href="https://github.com/bmild/nerf" class="github-corner" aria-label="View source on Github">
<svg width="80" height="80" viewBox="0 0 250 250" style="fill: rgb(51, 51, 51); color: rgb(255, 255, 255); position: absolute; top: 0; border-top-color: initial; border-top-style: initial; border-top-width: 0; border-right-color: initial; border-right-style: initial; border-right-width: 0; border-bottom-color: initial; border-bottom-style: initial; border-bottom-width: 0; border-left-color: initial; border-left-style: initial; border-left-width: 0; right: 0;" aria-hidden="true">
<defs>
  <mask id="octomask">
    <path fill="white" d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
    <path fill="black" d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" style="transform-origin: 130px 106px;" class="octo-arm"></path>
    <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="black" class="octo-body"></path>
  </mask>
</defs>
<rect class="filler" width="100%" height="100%" mask="url(#octomask)"></rect>
</svg></a>
<style>.github-corner:hover .octo-arm {  }@keyframes octocat-wave{  }@media (max-width: 500px) { .github-corner:hover .octo-arm {  }.github-corner .octo-arm {  } }</style></div><div class="section hero nerf-_v2 wf-section"><div class="container-2 nerf_header_v2 w-container"><h1 class="nerf_title_v2">NeRF</h1><h1 class="nerf_subheader_v2">Representing Scenes as Neural Radiance Fields for View Synthesis</h1><h1 class="eccv_label">ECCV 2020 Oral - Best Paper Honorable Mention</h1><div class="nerf_authors_list_single w-row"><div class="w-col w-col-2 w-col-small-4 w-col-tiny-6"><a href="https://bmild.github.io/" target="_blank" class="nerf_authors_v2">Ben Mildenhall<span class="text-span_nerf_star">*</span><span class="superscript text-span_nerf">1*</span></a></div><div class="column w-col w-col-2 w-col-small-4 w-col-tiny-6"><a href="https://pratulsrinivasan.github.io/" target="_blank" class="nerf_authors_v2">Pratul P. Srinivasan<span class="text-span_nerf_star">*</span><span class="superscript text-span_nerf">1*</span></a></div><div class="w-col w-col-2 w-col-small-4 w-col-tiny-6"><a href="https://www.matthewtancik.com/" target="_blank" class="nerf_authors_v2">Matthew Tancik<span class="text-span_nerf_star">*</span><span class="superscript text-span_nerf">1*</span></a></div><div class="w-col w-col-2 w-col-small-4 w-col-tiny-6"><a href="https://jonbarron.info/" target="_blank" class="nerf_authors_v2">Jonathan T. Barron<span class="text-span_nerf">2</span><span class="superscript"></span></a></div><div class="w-col w-col-2 w-col-small-4 w-col-tiny-6"><a href="http://cseweb.ucsd.edu/~ravir/" target="_blank" class="nerf_authors_v2">Ravi Ramamoorthi<span class="text-span_nerf">3</span></a></div><div class="w-col w-col-2 w-col-small-4 w-col-tiny-6"><a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html" target="_blank" class="nerf_authors_v2">Ren Ng<span class="text-span_nerf">1</span></a></div></div><div class="columns-6 w-row"><div class="nerf_mobile_col_inst w-col w-col-4 w-col-small-4 w-col-tiny-4"><div class="nerf_mobile_inst"><span class="text-span_nerf">1 </span>UC Berkeley</div></div><div class="nerf_mobile_col_inst w-col w-col-4 w-col-small-4 w-col-tiny-4"><div class="nerf_mobile_inst"><span class="text-span_nerf">2</span> Google Research</div></div><div class="nerf_mobile_col_inst w-col w-col-4 w-col-small-4 w-col-tiny-4"><div class="nerf_mobile_inst"><span class="text-span_nerf">3 </span>UC San Diego</div></div></div><div class="nerf_authors_list_single nerf_authors_affiliation w-row"><div class="w-col w-col-2"><h1 class="nerf_affiliation_v2">UC Berkeley</h1></div><div class="column w-col w-col-2"><h1 class="nerf_affiliation_v2">UC Berkeley</h1></div><div class="w-col w-col-2"><h1 class="nerf_affiliation_v2">UC Berkeley</h1></div><div class="w-col w-col-2"><h1 class="nerf_affiliation_v2">Google Research</h1></div><div class="w-col w-col-2"><h1 class="nerf_affiliation_v2">UC San Diego</h1></div><div class="w-col w-col-2"><h1 class="nerf_affiliation_v2">UC Berkeley</h1></div></div><div class="div-block-10"><div class="nerf_equal_v2"><span class="text-span_nerf">*</span><span class="text-span_nerf_star">*</span>Denotes Equal Contribution</div></div><div class="link_column_nerf_v2 w-row"><div class="w-col w-col-4 w-col-small-4 w-col-tiny-4"><a href="https://arxiv.org/abs/2003.08934" class="link-block w-inline-block"><img src="document_files/5cab99df4998decfbf9e218e_paper-01.png" alt="paper" sizes="(max-width: 479px) 12vw, (max-width: 767px) 7vw, (max-width: 991px) 41.8515625px, 56.6953125px" srcset="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01-p-500.png 500w, https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01.png 672w" class="paper_img image-8_nerf"></a></div><div class="w-col w-col-4 w-col-small-4 w-col-tiny-4"><a href="https://github.com/bmild/nerf" target="_blank" class="link-block w-inline-block"><img src="document_files/5cae3b53b42ebb3dd4175a82_68747470733a2f2f7777772e69636f6e66696e6465722e636f6d2f646174612f69636f6e732f6f637469636f6e732f313032342f6d61726b2d6769746875622d3235362e706e67.png" alt="paper" class="paper_img image-8 github_icon_nerf_v2"></a></div><div class="column-2 w-col w-col-4 w-col-small-4 w-col-tiny-4"><a href="https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1?usp=sharing" target="_blank" class="link-block w-inline-block"><img src="document_files/5e7136849ee3b0a0c6a95151_database.svg" alt="paper" class="paper_img image-8_nerf nerf_db_icon"></a></div></div><div class="paper_code_nerf w-row"><div class="w-col w-col-4 w-col-small-4 w-col-tiny-4"><div class="text-block-2"><strong class="bold-text-nerf_v2">Paper</strong></div></div><div class="w-col w-col-4 w-col-small-4 w-col-tiny-4"><div class="text-block-2"><strong class="bold-text-nerf_v2">&lt;/Code&gt;</strong></div></div><div class="w-col w-col-4 w-col-small-4 w-col-tiny-4"><div class="text-block-2"><strong class="bold-text-nerf_v2">Data</strong></div></div></div><div data-delay="10000" data-animation="slide" class="nerf_slider_v2 w-slider" data-autoplay="true" data-easing="ease" data-hide-arrows="false" data-disable-swipe="false" data-autoplay-limit="0" data-nav-spacing="3" data-duration="800" data-infinite="true"><div class="mask w-slider-mask"><div class="slide w-slide"><div class="div-block-9 first_video"><div class="video_class w-embed"><video width="100%" height="100%" autoplay="" muted="" controls="" loop="" preload="metadata" poster="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/website_renders/images/colorspout_200k_rgb.jpg">
  <source src="document_files/colorspout_200k_rgb.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div><div class="video_class mobile w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="https://storage.googleapis.com/nerf_data/website_renders/images/colorspout_200k_rgb.jpg">
  <source src="https://storage.googleapis.com/nerf_data/website_renders/colorspout_200k_rgb.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div></div><div class="div-block-9"><div class="video_class w-embed"><video width="100%" height="100%" autoplay="" muted="" controls="" loop="" preload="metadata" poster="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/website_renders/images/orchid.jpg">
  <source src="document_files/orchid.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div><div class="video_class mobile w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="https://storage.googleapis.com/nerf_data/website_renders/images/orchid.jpg">
  <source src="https://storage.googleapis.com/nerf_data/website_renders/orchid.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div></div></div><div class="w-slide"><div class="div-block-9"><div class="video_class w-embed"><video width="100%" height="100%" autoplay="" muted="" controls="" loop="" preload="metadata" poster="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/website_renders/images/benchflower_100k_rgb.jpg">
  <source src="document_files/benchflower_100k_rgb.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div><div class="video_class mobile w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="https://storage.googleapis.com/nerf_data/website_renders/images/benchflower_100k_rgb.jpg">
  <source src="https://storage.googleapis.com/nerf_data/website_renders/benchflower_100k_rgb.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div></div></div><div class="w-slide"><div class="div-block-9"><div class="video_class w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/website_renders/images/lumpyroots_200k_rgb.jpg">
  <source src="document_files/lumpyroots_200k_rgb.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div><div class="video_class mobile w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="https://storage.googleapis.com/nerf_data/website_renders/images/lumpyroots_200k_rgb.jpg">
  <source src="https://storage.googleapis.com/nerf_data/website_renders/lumpyroots_200k_rgb.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div></div></div><div class="w-slide"><div class="div-block-9"><div class="video_class w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/website_renders/images/fern_200k_rgb.jpg">
  <source src="document_files/fern_200k_rgb.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div><div class="video_class mobile w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="https://storage.googleapis.com/nerf_data/website_renders/images/fern_200k_rgb.jpg">
  <source src="https://storage.googleapis.com/nerf_data/website_renders/fern_200k_rgb.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div></div></div><div class="w-slide"><div class="div-block-9"><div class="video_class w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/website_renders/images/fullmagnolia_200k_rgb.jpg">
  <source src="document_files/fullmagnolia_200k_rgb.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div><div class="video_class mobile w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="https://storage.googleapis.com/nerf_data/website_renders/images/fullmagnolia_200k_rgb.jpg">
  <source src="https://storage.googleapis.com/nerf_data/website_renders/fullmagnolia_200k_rgb.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div></div></div><div class="w-slide"><div class="div-block-9"><div class="video_class w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/website_renders/images/trike_200k_rgb.jpg">
  <source src="document_files/trike_200k_rgb.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div><div class="video_class mobile w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="https://storage.googleapis.com/nerf_data/website_renders/images/trike_200k_rgb.jpg">
  <source src="https://storage.googleapis.com/nerf_data/website_renders/trike_200k_rgb.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div></div></div><div class="w-slide"><div class="div-block-9"><div class="video_class w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/website_renders/images/pecanpie_200k_rgb.jpg">
  <source src="document_files/pecanpie_200k_rgb.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div><div class="video_class mobile w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="https://storage.googleapis.com/nerf_data/website_renders/images/pecanpie_200k_rgb.jpg">
  <source src="https://storage.googleapis.com/nerf_data/website_renders/pecanpie_200k_rgb.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div></div></div><div class="w-slide"><div class="div-block-9"><div class="video_class w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/website_renders/images/trex.jpg">
  <source src="document_files/trex.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div><div class="video_class mobile w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="https://storage.googleapis.com/nerf_data/website_renders/images/trex.jpg">
  <source src="https://storage.googleapis.com/nerf_data/website_renders/trex.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div></div></div><div class="w-slide"><div class="div-block-9"><div class="video_class w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/website_renders/images/redtoyota.jpg">
  <source src="document_files/redtoyota.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div><div class="video_class mobile w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="https://storage.googleapis.com/nerf_data/website_renders/images/redtoyota.jpg">
  <source src="https://storage.googleapis.com/nerf_data/website_renders/redtoyota.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div></div></div><div class="w-slide"><div class="div-block-9"><div class="video_class w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/website_renders/images/piano_200k_rgb.jpg">
  <source src="document_files/piano_200k_rgb.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div><div class="video_class mobile w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="https://storage.googleapis.com/nerf_data/website_renders/images/piano_200k_rgb.jpg">
  <source src="https://storage.googleapis.com/nerf_data/website_renders/piano_200k_rgb.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div></div></div><div class="w-slide"><div class="div-block-9"><div class="video_class w-embed"><video width="100%" height="100%" autoplay="" muted="" controls="" loop="" preload="metadata" poster="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/website_renders/images/colorspout_200k_rgb.jpg">
  <source src="document_files/colorspout_200k_rgb.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div><div class="video_class mobile w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="https://storage.googleapis.com/nerf_data/website_renders/images/colorspout_200k_rgb.jpg">
  <source src="https://storage.googleapis.com/nerf_data/website_renders/colorspout_200k_rgb.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div></div><div class="div-block-9 last_block"><div class="video_class w-embed"><video width="100%" height="100%" autoplay="" muted="" controls="" loop="" preload="metadata" poster="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/website_renders/images/orchid.jpg">
  <source src="document_files/orchid.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div><div class="video_class mobile w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="https://storage.googleapis.com/nerf_data/website_renders/images/orchid.jpg">
  <source src="https://storage.googleapis.com/nerf_data/website_renders/orchid.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div></div></div></div><div class="w-slider-arrow-left"><div class="w-icon-slider-left"></div></div><div class="w-slider-arrow-right"><div class="w-icon-slider-right"></div></div><div class="nerf_slide_nav w-slider-nav w-slider-nav-invert w-round"></div></div></div></div><div data-anchor="slide1" class="section nerf_section"><div class="w-container"><h2 class="grey-heading_nerf">Overview Video</h2><div style="padding-top: 56.17021%;" class="w-embed-youtubevideo stega_movie youtube"><iframe src="https://www.youtube.com/embed/JuH79E8rdKc?rel=1&amp;controls=1&amp;autoplay=0&amp;mute=0&amp;start=0" frameborder="0" style="position: absolute; left: 0; top: 0; width: 100%; height: 100%; pointer-events: auto;" allow="autoplay; encrypted-media" allowfullscreen="" title="NeRF: Neural Radiance Fields"></iframe></div><a href="https://www.youtube.com/watch?v=LRAqeM8EjOo&amp;feature=youtu.be" target="_blank" class="nerf_full_talk_vid">ECCV Technical Talk Video<span class="superscript"></span></a></div></div><div data-anchor="slide1" class="section nerf_section"><div class="grey_container w-container"><h2 class="grey-heading_nerf">Abstract &amp; Method</h2><p class="paragraph-3 nerf_text">We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views.</p><img src="document_files/5e700a025ff238947d682a1f_pipeline_website-03.svg" alt="" class="nerf_network"><p class="paragraph-3 nerf_text">Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x, y, z) and viewing direction (θ, φ)) and whose output is the volume density and view-dependent emitted radiance at that spatial location.</p><div class="columns-5 w-row"><div class="nerf_pipeline_cols w-col w-col-8 w-col-tiny-tiny-stack"><img src="document_files/5e700ef6067b43821ed52768_pipeline_website-01.png" sizes="(max-width: 479px) 92vw, (max-width: 767px) 94vw, (max-width: 991px) 465.328125px, 606.6640625px" srcset="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5e700ef6067b43821ed52768_pipeline_website-01-p-500.png 500w, https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5e700ef6067b43821ed52768_pipeline_website-01-p-800.png 800w, https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5e700ef6067b43821ed52768_pipeline_website-01.png 1500w" alt="" class="nerf_volume_pipeline"></div><div class="w-col w-col-4 w-col-tiny-tiny-stack"><img src="document_files/5e700a02ee168a2a63febc3b_pipeline_website-02.svg" alt="" class="nerf_volume_rendering"></div></div><p class="paragraph-3 nerf_text">We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis.</p></div></div><div class="white_section_nerf wf-section"><div class="w-container"><h2 class="grey-heading_nerf">Synthetic Results</h2><p class="paragraph-3 nerf_text nerf_results_text">Here are results on our synthetic dataset of pathtraced objects with realistic non-Lambertian materials. The dataset will be released soon.</p><div><div class="video_class w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/website_renders/images/synth_grid_website.jpg">
  <source src="document_files/synth_grid_3.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div><div class="video_class mobile w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="https://storage.googleapis.com/nerf_data/website_renders/images/synth_grid_website.jpg">
  <source src="https://storage.googleapis.com/nerf_data/website_renders/synth_grid_3.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div></div></div></div><div data-anchor="slide1" class="section nerf_section"></div><div class="white_section_nerf wf-section"><div class="grey_container w-container"><h2 class="grey-heading_nerf">View-Dependent Appearance</h2><p class="paragraph-3 nerf_text nerf_results_text">Here we visualize the view-dependent appearance encoded in our NeRF representation by fixing the camera viewpoint but changing the queried viewing direction.</p><div><div class="video_class w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/website_renders/images/viewdirs_website_bww.jpg">
  <source src="document_files/viewdirs_website_bww.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div><div class="video_class mobile w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="https://storage.googleapis.com/nerf_data/website_renders/images/viewdirs_website_bww.jpg">
  <source src="https://storage.googleapis.com/nerf_data/website_renders/viewdirs_website_bww.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div><div class="video_class w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/website_renders/images/viewdirs_website.jpg">
  <source src="document_files/viewdirs_website_stove.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div><div class="video_class mobile w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="https://storage.googleapis.com/nerf_data/website_renders/images/viewdirs_website.jpg">
  <source src="https://storage.googleapis.com/nerf_data/website_renders/viewdirs_website_stove.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div></div></div></div><div data-anchor="slide1" class="white_section_nerf"><div class="w-container"><h2 class="grey-heading_nerf">Geometry Visualization</h2><p class="paragraph-3 stega_text"></p><p class="paragraph-3 nerf_text nerf_results_text">NeRFs are able to represent detailed scene geometry with complex occlusions. Here we visualize depth maps for rendered novel views computed as the expected termination of each camera ray in the encoded volume.</p><div class="video_class mobile w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="https://storage.googleapis.com/nerf_data/website_renders/images/depth_reflower.jpg">
  <source src="https://storage.googleapis.com/nerf_data/website_renders/depth_reflower.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div><div class="video_class w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/website_renders/images/depth_reflower.jpg">
  <source src="document_files/depth_reflower.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div><div class="video_class w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/website_renders/images/depth_ornament.jpg">
  <source src="document_files/depth_ornament.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div><div class="video_class mobile w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="https://storage.googleapis.com/nerf_data/website_renders/images/depth_ornament.jpg">
  <source src="https://storage.googleapis.com/nerf_data/website_renders/depth_ornament.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div><p class="paragraph-3 nerf_text nerf_results_text">Our estimated scene geometry is detailed enough to support mixed-reality applications such as inserting virtual objects into real world scenes with compelling occlusion effects.</p><div class="video_class w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/website_renders/images/redball_highres.jpg">
  <source src="document_files/redball_highres_1.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div><div class="video_class mobile w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="https://storage.googleapis.com/nerf_data/website_renders/images/redball_highres.jpg">
  <source src="https://storage.googleapis.com/nerf_data/website_renders/redball_highres_1.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div><p class="paragraph-3 nerf_text nerf_results_text">We can also convert the NeRF to a mesh using marching cubes.</p><div class="video_class w-embed w-script"><model-viewer alt="Lego Mesh" src="document_files/5e7e623b0c4ef40966f34511_legomesh.glb.txt" style="width: 100%; height: 600px; background-color: rgb(64, 64, 64);" poster="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5e7e6db2d75a9b467eee4111_legomesh_cover.png" exposure=".8" auto-rotate="" camera-controls=""></model-viewer>

<!-- Loads <model-viewer> for modern browsers: -->
<script type="module" src="document_files/model-viewer.js">
</script>

<!-- Loads <model-viewer> for old browsers like IE11: -->
<script nomodule="" src="https://unpkg.com/@google/model-viewer/dist/model-viewer-legacy.js">
</script></div><div class="video_class mobile w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="https://storage.googleapis.com/nerf_data/website_renders/images/lego_mesh.jpg">
  <source src="https://storage.googleapis.com/nerf_data/website_renders/lego_mesh.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div></div></div><div class="white_section_nerf wf-section"><div class="grey_container w-container"><h2 class="grey-heading_nerf">360° Scene Capture with Real Data</h2><p class="paragraph-3 nerf_text nerf_results_text">NeRFs can even represent real objects captured by a set of inward-facing views, without any background isolation or masking.</p><div><div class="video_class w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/website_renders/images/website_360.jpg">
  <source src="document_files/website_360.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div><div class="video_class mobile w-embed"><video width="100%" height="100%" muted="" controls="" loop="" preload="metadata" poster="https://storage.googleapis.com/nerf_data/website_renders/images/website_360.jpg">
  <source src="https://storage.googleapis.com/nerf_data/website_renders/website_360.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div></div></div></div><div class="white_section_nerf wf-section"></div><div data-anchor="slide1" class="white_section_nerf"><div class="w-container"><h2 class="nerf_followup">Followup Works</h2><h2 class="grey-heading_nerf">Positional Encoding</h2><p class="paragraph-3 stega_text"></p><p class="paragraph-3 nerf_text nerf_results_text">Fully-connected deep networks are biased to learn low frequencies faster. Surprisingly, applying a simple mapping to the network input is able to mitigate this issue. We explore these input mappings in a followup work.<a href="https://people.eecs.berkeley.edu/~bmild/fourfeat/index.html" target="_blank"><br>Project Page</a>.</p><div><div class="video_class w-embed"><video width="100%" height="100%" muted="" autoplay="" loop="" preload="metadata">
  <source src="document_files/lion_none_gauss_v1.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div><div class="video_class mobile w-embed"><video width="100%" height="100%" muted="" autoplay="" loop="" preload="metadata">
  <source src="document_files/lion_none_gauss_v1.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div></div></div></div><div data-anchor="slide1" class="white_section_nerf"><div class="w-container"><h2 class="grey-heading_nerf">Multiscale Representation</h2><p class="paragraph-3 stega_text"></p><p class="paragraph-3 nerf_text nerf_results_text">By efficiently rendering anti-aliased conical frustums instead of rays, our followup, mip-NeRF, reduces objectionable aliasing artifacts and significantly improves NeRF's ability to represent fine details, while also being 7% faster than NeRF and half the size. <a href="https://jonbarron.info/mipnerf/" target="_blank">Project Page</a></p><img src="document_files/6086e97ec8800aae5cc3c7c0_rays.jpg" loading="lazy" sizes="(max-width: 767px) 100vw, (max-width: 991px) 728px, 940px" srcset="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/6086e97ec8800aae5cc3c7c0_rays-p-500.jpeg 500w, https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/6086e97ec8800aae5cc3c7c0_rays-p-800.jpeg 800w, https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/6086e97ec8800aae5cc3c7c0_rays-p-1080.jpeg 1080w, https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/6086e97ec8800aae5cc3c7c0_rays.jpg 1600w" alt=""><div><div class="video_class mobile w-embed"><video width="100%" height="100%" muted="" autoplay="" loop="" preload="metadata">
  <source src="https://storage.googleapis.com/nerf_data/website_renders/lion_none_gauss_v1.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div></div></div><div class="w-container"><h2 class="grey-heading_nerf">Learned Initializations</h2><p class="paragraph-3 stega_text"></p><p class="paragraph-3 nerf_text nerf_results_text">In a followup work we explore how meta-learning can be applied to speed up convergence and embed dataset specific priors.<br>‍<a href="https://www.matthewtancik.com/learnit">Project Page</a></p><div class="w-row"><div class="w-col w-col-6"><div class="video_class w-embed"><video width="100%" height="100%" muted="" autoplay="" loop="" playsinline="" webkit-playsinline="">
  <source src="document_files/trevi.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div></div><div class="w-col w-col-6"><div class="video_class w-embed"><video width="100%" height="100%" muted="" autoplay="" loop="" playsinline="" webkit-playsinline="">
  <source src="document_files/sacre.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div></div></div><div><div class="video_class mobile w-embed"><video width="100%" height="100%" muted="" autoplay="" loop="" preload="metadata">
  <source src="https://storage.googleapis.com/nerf_data/website_renders/lion_none_gauss_v1.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div></div></div><div class="w-container"><h2 class="grey-heading_nerf">Relighting</h2><p class="paragraph-3 stega_text"></p><p class="paragraph-3 nerf_text nerf_results_text">We extend NeRF to enable the rendering of scenes from novel viewpoints under arbitrary lighting conditions.<br><a href="https://people.eecs.berkeley.edu/~pratul/nerv/" target="_blank">Project Page</a></p><div class="video_class w-embed"><video width="100%" height="100%" muted="" autoplay="" loop="" playsinline="" webkit-playsinline="">
  <source src="document_files/nerv_results.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div><div><div class="video_class mobile w-embed"><video width="100%" height="100%" muted="" autoplay="" loop="" preload="metadata">
  <source src="https://storage.googleapis.com/nerf_data/website_renders/lion_none_gauss_v1.mp4" type="video/mp4">
Your browser does not support the video tag.
</video></div></div></div></div><div class="white_section_nerf wf-section"><div class="grey_container citation w-container"><h2 class="grey-heading_nerf">Citation</h2><p class="paragraph-3 nerf_text nerf_results_text citation">@inproceedings{mildenhall2020nerf,<br> &nbsp;title={NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},<br> &nbsp;author={Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},<br> &nbsp;year={2020},<br> &nbsp;booktitle={ECCV},<br>}</p></div></div><script src="document_files/jquery-3.5.1.min.dc5e7f18c8.js?site=51e0d73d83d06baa7a00000f" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script><script src="document_files/webflow.6f0746308.js" type="text/javascript"></script><!--[if lte IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]--></body></html>