@misc{,
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2020 - Faster Person Re-Identiﬁcation.pdf:pdf},
title = {{2020 - Faster Person Re-Identiﬁcation.pdf}}
}
@article{Sun2020,
abstract = {The main challenge of person re-identification (re-id) lies in the strikingly discrepancy between different camera views, including illumination, background and human pose. Existing person re-id methods rely mostly on implicit solutions, such as seeking robust features or designing discriminative distance metrics. Compared to these methods, human solutions are more straightforward. That is, imagine the appearance of the target person under different camera views before matching target person. The key idea is that human can intuitively implement viewpoint transfer, noting the association of the target person under different camera views but the machine failed. In this paper, we attempt to imitate such human behavior that transfer person image to certain camera views before matching. In practice, we propose a conditional transfer network (cTransNet) that conditionally implement viewpoint transfer, which transfers image to the viewpoint with the biggest domain gap through a variant of Generative Adversarial Networks (GANs). After that, we obtain hybrid person representation by fusing the feature of original image with the transferred image then perform similarity ranking according to cosine distance. Compared with former methods, we propose a human-like approach and obtains consistent improvement of the rank-1 precision over the baseline in Market-1501, DukeMTMC-ReID and MSMT17 dataset by 3{\%},4{\%},4{\%}, respectively.},
author = {Sun, Rui and Lu, Weiming and Zhao, Ye and Zhang, Jun and Kai, Caihong},
doi = {10.1109/ACCESS.2019.2962301},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/2020 - A Novel Method for Person Re-Identification Conditional Translated Network based on GANs.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Person re-identification,conditional viewpoint translation,deep learning,generative adversarial network,hybrid person representation},
pages = {3677--3686},
publisher = {IEEE},
title = {{A Novel Method for Person Re-Identification: Conditional Translated Network Based on GANs}},
volume = {8},
year = {2020}
}
@article{Jiang2020a,
author = {Jiang, Tong},
doi = {10.32604/jnm.2020.09823},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2020 - A Review of Person Re-Identification.pdf:pdf},
issn = {2579-0110},
journal = {Journal of New Media},
keywords = {person re-identification,video surveillance system},
number = {2},
pages = {45--60},
title = {{A Review of Person Re-Identification}},
volume = {2},
year = {2020}
}
@article{Li2018,
abstract = {Person re-identification (re-id) aims to match a certain person across multiple non-overlapping cameras. It is a challenging task because the same person's appearance can be very different across camera views due to the presence of large pose variations. To overcome this issue, in this paper, we propose a novel unified person re-id framework by exploiting person poses and identities jointly for simultaneous person image synthesis under arbitrary poses and pose-invariant person re-identification. The framework is composed of a GAN based network and two Feature Extraction Networks (FEN), and enjoys following merits. First, it is a unified generative adversarial model for person image generation and person re-identification. Second, a pose estimator is utilized into the generator as a supervisor in the training process, which can effectively help pose transfer and guide the image generation with any desired pose. As a result, the proposed model can automatically generate a person image under an arbitrary pose. Third, the identity-sensitive representation is explicitly disentangled from pose variations through the person identity and pose embedding. Fourth, the learned re-id model can have better generalizability on a new person re-id dataset by using the synthesized images as auxiliary samples. Extensive experimental results on four standard benchmarks including Market-1501 [69], DukeMTMC-reID [40], CUHK03 [23], and CUHK01 [22] demonstrate that the proposed model can perform favorably against state-of-the-art methods.},
author = {Li, Yaoyu and Zhang, Tianzhu and Duan, Lingyu and Xu, Changsheng},
doi = {10.1145/3240508.3240573},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/201917{\_}A Unified Generative Adversarial Framework for Image.pdf:pdf},
isbn = {9781450356657},
journal = {MM 2018 - Proceedings of the 2018 ACM Multimedia Conference},
keywords = {GAN,Multimedia System,Person Re-identification},
pages = {163--172},
title = {{A unified generative adversarial framework for image generation and person re-identification}},
year = {2018}
}
@article{Ding2020,
abstract = {Due to domain bias, directly deploying a deep person re-identification (re-ID) model trained on one dataset often achieves considerably poor accuracy on another dataset. In this article, we propose an Adaptive Exploration (AE) method to address the domain-shift problem for re-ID in an unsupervised manner. Specifically, in the target domain, the re-ID model is inducted to (1) maximize distances between all person images and (2) minimize distances between similar person images. In the first case, by treating each person image as an individual class, a non-parametric classifier with a feature memory is exploited to encourage person images to move far away from each other. In the second case, according to a similarity threshold, our method adaptively selects neighborhoods for each person image in the feature space. By treating these similar person images as the same class, the non-parametric classifier forces them to stay closer. However, a problem of the adaptive selection is that, when an image has too many neighborhoods, it is more likely to attract other images as its neighborhoods. As a result, a minority of images may select a large number of neighborhoods while a majority of images has only a few neighborhoods. To address this issue, we additionally integrate a balance strategy into the adaptive selection. We evaluate our methods with two protocols. The first one is called "target-only re-ID", in which only the unlabeled target data is used for training. The second one is called "domain adaptive re-ID", in which both the source data and the target data are used during training. Experimental results on large-scale re-ID datasets demonstrate the effectiveness of our method. Our code has been released at https://github.com/dyh127/Adaptive-Exploration-for-Unsupervised-Person-Re-Identification.},
archivePrefix = {arXiv},
arxivId = {1907.04194},
author = {Ding, Yuhang and Fan, Hehe and Xu, Mingliang and Yang, Yi},
doi = {10.1145/3369393},
eprint = {1907.04194},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/2020 - Adaptive Exploration for Unsupervised Person Re-Identification.pdf:pdf},
issn = {15516865},
journal = {ACM Transactions on Multimedia Computing, Communications and Applications},
keywords = {Person re-identification,deep learning,domain adaptation,unsupervised learning},
number = {1},
title = {{Adaptive Exploration for Unsupervised Person Re-identification}},
volume = {16},
year = {2020}
}
@article{Ni2021,
abstract = {In intelligent video surveillance system, person re-identification is a key technology. In order to address the problem, the decrease in performance of person Re-Id lead by the skew pedestrian images, this paper proposes the affine transform for skew correction based on generative adversarial network (GAN) method for multi-camera person re-identification (Re-Id). Firstly, an effective GAN is proposed to guide the spatial transformer network (STN) to learn affine transform parameters for skew correction in an adversarial way, and STN is adopted as the preprocessing model for Re-Id to reduce influence of variations in person posture. Then, features are extracted by a deep convolutional neural network from input images which are corrected by STN, and finally results can be obtained by measuring similarity between features. Besides, in the proposed GAN, a classification model and related loss functions are introduced to reduce the damage to the key features of pedestrian during skew correction. The effectiveness of the proposed method is verified by experiments conducted on the skew pedestrian dataset.},
author = {Ni, Ziyang and Pei, Jihong and Zhao, Yang},
doi = {10.1145/3449365.3449380},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2021 - Affine Transform for Skew Correction Based on Generative.pdf:pdf},
isbn = {9781450388108},
journal = {ACM International Conference Proceeding Series},
keywords = {Affine transform,Generative adversarial network,Person re-identification,Skew correction},
pages = {89--95},
title = {{Affine transform for skew correction based on generative adversarial network method for multi-camera person re-identification}},
volume = {PartF16898},
year = {2021}
}
@article{Karras2021,
abstract = {We observe that despite their hierarchical convolutional nature, the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner. This manifests itself as, e.g., detail appearing to be glued to image coordinates instead of the surfaces of depicted objects. We trace the root cause to careless signal processing that causes aliasing in the generator network. Interpreting all signals in the network as continuous, we derive generally applicable, small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process. The resulting networks match the FID of StyleGAN2 but differ dramatically in their internal representations, and they are fully equivariant to translation and rotation even at subpixel scales. Our results pave the way for generative models better suited for video and animation.},
archivePrefix = {arXiv},
arxivId = {2106.12423},
author = {Karras, Tero and Aittala, Miika and Laine, Samuli and H{\"{a}}rk{\"{o}}nen, Erik and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
eprint = {2106.12423},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/Arquitecturas GAN/2021 - Stylegan3.pdf:pdf},
number = {NeurIPS},
title = {{Alias-Free Generative Adversarial Networks}},
url = {http://arxiv.org/abs/2106.12423},
year = {2021}
}
@article{Gong,
author = {Gong, Yunpeng and Zeng, Zhiyong},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2021 - PREPRINT - An Effective Data Augmentation for Person Re-identification.pdf:pdf},
keywords = {data augmentation,deep learning,person re-identification,random grayscale,random grayscale patch replacement,transformation},
pages = {1--13},
title = {{An Effective Data Augmentation for Person Re- identification}}
}
@article{Alqahtani2019,
abstract = {Person re-identification is a basic subject in the field of computer vision. The traditional methods have several limitations in solving the problems of person illumination like occlusion, pose variation and feature variation under complex background. Fortunately, deep learning paradigm opens new ways of the person re-identification research and becomes a hot spot in this field. Generative Adversarial Nets (GANs) in the past few years attracted lots of attention in solving these problems. This paper reviews the GAN based methods for person re-identification focuses on the related papers about different GAN based frameworks and discusses their advantages and disadvantages. Finally, it proposes the direction of future research, especially the prospect of person reidentification methods based on GANs.},
author = {Alqahtani, Hamed and Kavakli-Thorne, Manolya and Liu, Charles Z.},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/2019 - Introduction to Person Re-Identification with Generative Advercial Networks.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Deep Learning,Generative Adversarial Nets,Person Reidentification},
pages = {1--15},
title = {{An introduction to person re-identification with generative adversarial networks}},
year = {2019}
}
@article{Karras2020,
abstract = {The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.},
archivePrefix = {arXiv},
arxivId = {1912.04958},
author = {Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
doi = {10.1109/CVPR42600.2020.00813},
eprint = {1912.04958},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2020 - StyleGan.pdf:pdf},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {8107--8116},
title = {{Analyzing and improving the image quality of stylegan}},
year = {2020}
}
@article{siamese-neuralnet,
archivePrefix = {arXiv},
arxivId = {arXiv:1607.08378v2},
author = {Re-identification, Architecture Human},
eprint = {arXiv:1607.08378v2},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2016 - red neuronal siamesa.pdf:pdf},
keywords = {deep convolutional neural net-,gating function,human re-identification,matching gate,siamese convolutional neural net-,work},
pages = {1--18},
title = {{arXiv : 1607 . 08378v2 [ cs . CV ] 26 Sep 2016}}
}
@article{Zhong2018,
abstract = {Being a cross-camera retrieval task, person re-identification suffers from image style variations caused by different cameras. The art implicitly addresses this problem by learning a camera-invariant descriptor subspace. In this paper, we explicitly consider this challenge by introducing camera style (CamStyle) adaptation. CamStyle can serve as a data augmentation approach that smooths the camera style disparities. Specifically, with CycleGAN, labeled training images can be style-transferred to each camera, and, along with the original training samples, form the augmented training set. This method, while increasing data diversity against over-fitting, also incurs a considerable level of noise. In the effort to alleviate the impact of noise, the label smooth regularization (LSR) is adopted. The vanilla version of our method (without LSR) performs reasonably well on few-camera systems in which over-fitting often occurs. With LSR, we demonstrate consistent improvement in all systems regardless of the extent of over-fitting. We also report competitive accuracy compared with the state of the art. Code is available at: Https://github.com/zhunzhong07/CamStyle.},
archivePrefix = {arXiv},
arxivId = {1711.10295},
author = {Zhong, Zhun and Zheng, Liang and Zheng, Zhedong and Li, Shaozi and Yang, Yi},
doi = {10.1109/CVPR.2018.00541},
eprint = {1711.10295},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/2018 - Camera Style Adaptation for Person Re-identification.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {5157--5166},
title = {{Camera Style Adaptation for Person Re-identification}},
year = {2018}
}
@article{Zhong2019,
abstract = {Person re-identification (re-ID) is a cross-camera retrieval task that suffers from image style variations caused by different cameras. The art implicitly addresses this problem by learning a camera-invariant descriptor subspace. In this paper, we explicitly consider this challenge by introducing camera style (CamStyle). CamStyle can serve as a data augmentation approach that reduces the risk of deep network overfitting and that smooths the CamStyle disparities. Specifically, with a style transfer model, labeled training images can be style transferred to each camera, and along with the original training samples, form the augmented training set. This method, while increasing data diversity against overfitting, also incurs a considerable level of noise. In the effort to alleviate the impact of noise, the label smooth regularization (LSR) is adopted. The vanilla version of our method (without LSR) performs reasonably well on few camera systems in which overfitting often occurs. With LSR, we demonstrate consistent improvement in all systems regardless of the extent of overfitting. We also report competitive accuracy compared with the state of the art on Market-1501 and DukeMTMC-re-ID. Importantly, CamStyle can be employed to the challenging problems of one view learning and unsupervised domain adaptation (UDA) in person re-identification (re-ID), both of which have critical research and application significance. The former only has labeled data in one camera view and the latter only has labeled data in the source domain. Experimental results show that CamStyle significantly improves the performance of the baseline in the two problems. Specially, for UDA, CamStyle achieves state-of-the-art accuracy based on a baseline deep re-ID model on Market-1501 and DukeMTMC-reID. Our code is available at: https://github.com/zhunzhong07/CamStyle.},
author = {Zhong, Zhun and Zheng, Liang and Zheng, Zhedong and Li, Shaozi and Yang, Yi},
doi = {10.1109/TIP.2018.2874313},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/2019 - CamStyle A Novel Data augmentation Method for person re-identification.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {CamStyle,Person re-identification,one-view learning,unsupervised domain adaptation},
number = {3},
pages = {1176--1190},
pmid = {30296233},
title = {{CamStyle: A Novel Data Augmentation Method for Person Re-Identification}},
volume = {28},
year = {2019}
}
@article{Tang2020,
abstract = {Person re-identification (re-ID) is a technique aiming to recognize person cross different cameras. Although some supervised methods have achieved favorable performance, they are far from practical application owing to the lack of labeled data. Thus, unsupervised person re-ID methods are in urgent need. Generally, the commonly used approach in existing unsupervised methods is to first utilize the source image dataset for generating a model in supervised manner, and then transfer the source image domain to the target image domain. However, images may lose their identity information after translation, and the distributions between different domains are far away. To solve these problems, we propose an image domain-to-domain translation method by keeping pedestrian's identity information and pulling closer the domains' distributions for unsupervised person re-ID tasks. Our work exploits the CycleGAN to transfer the existing labeled image domain to the unlabeled image domain. Specially, a Self-labeled Triplet Net is proposed to maintain the pedestrian identity information, and maximum mean discrepancy is introduced to pull the domain distribution closer. Extensive experiments have been conducted and the results demonstrate that the proposed method performs superiorly than the state-of-the-art unsupervised methods on DukeMTMC-reID and Market-1501.},
author = {Tang, Yingzhi and Xi, Yang and Wang, Nannan and Song, Bin and Gao, Xinbo},
doi = {10.1109/TIP.2020.2985545},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/2020 - CGAN-TM A Novel Domain-to-Domain.pdf:pdf},
issn = {19410042},
journal = {IEEE Transactions on Image Processing},
keywords = {CycleGAN,Person re-identification,maximum mean discrepancy,triplet net},
pages = {5641--5651},
publisher = {IEEE},
title = {{CGAN-TM: A Novel Domain-to-Domain Transferring Method for Person Re-Identification}},
volume = {29},
year = {2020}
}
@article{Yu2020,
abstract = {Recent years have witnessed great progress in person re-identification (re-id). Several academic benchmarks such as Market1501, CUHK03 and DukeMTMC play important roles to promote the re-id research. To our best knowledge, all the existing benchmarks assume the same person will have the same clothes. While in real-world scenarios, it is very often for a person to change clothes. To address the clothes changing person re-id problem, we construct a novel large-scale re-id benchmark named ClOthes ChAnging Person Set (COCAS), which provides multiple images of the same identity with different clothes. COCAS totally contains 62,382 body images from 5,266 persons. Based on COCAS, we introduce a new person re-id setting for clothes changing problem, where the query includes both a clothes template and a person image taking another clothes. Moreover, we propose a two-branch network named Biometric-Clothes Network (BC-Net) which can effectively integrate biometric and clothes feature for re-id under our setting. Experiments show that it is feasible for clothes changing re-id with clothes templates.},
author = {Yu, Shijie and Li, Shihua and Chen, Dapeng and Zhao, Rui and Yan, Junjie and Qiao, Yu},
file = {:C$\backslash$:/Users/Lau/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yu et al. - 2020 - COCAS A large-scale clothes changing person dataset for re-identification.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {3400--3409},
title = {{COCAS: A large-scale clothes changing person dataset for re-identification}},
year = {2020}
}
@article{Wang2019a,
abstract = {Recent deep Re-ID models mainly focus on learning high-level semantic features, while failing to explicitly explore color information which is one of the most important cues for person Re-ID model. In this paper, we propose a novel Color-Sensitive Re-ID to take full advantage of color information. On one hand, we train our model with real and fake images. By using the extra fake images, more color information can be exploited and it can avoid over-fitting during training. On the other hand, we also train our model with images of the same person with different colors. By doing so, features can be forced to focus on the color difference in regions. To generate fake images with specified colors, we propose a novel Color Translation GAN (CTGAN) to learn mappings between different clothing colors and preserve identity consistency among the same person with the clothing color. Extensive evaluations on two benchmark datasets show that our approach significantly outperforms state-of-the-art Re-ID models.},
author = {Wang, Guan'an and Yang, Yang Y. and Cheng, Jian and Wang, Jinqiao and Hou, Zengguang},
doi = {10.24963/ijcai.2019/131},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2019 - Color-Sensitive Person Re-Identification.pdf:pdf},
isbn = {9780999241141},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Computer Vision: Biometrics {\&} Face and Gesture Rec,Computer Vision: Computer Vision,Computer Vision: Recognition: Detection {\&} Categori},
number = {May 2020},
pages = {933--939},
title = {{Color-sensitive person re-identification}},
volume = {2019-Augus},
year = {2019}
}
@article{Jia2020,
abstract = {Person Reid is a challenging task for two factors, first one is background interference, such as changes in light, weather, posture, and camera position. Second is domain adaptive capacity, such as model train by market1501 achieve the same performance on Duke dataset. To solve the above problems, we come up with adopt human semantic to remove clutter from unwanted background information, is naturally a better alternative compare with bounding box, we adopt Local Regions Representation to extra the image features, which can preeminently improve the representation of local feature and global feature. Our proposed CSReID integrates human semantic and Local Regions Representation in person re-identification and not need to train on the evaluation dataset can achieve state of the art cross-modal performance.},
author = {Jia, Ruoran and Liu, Shuguang},
doi = {10.1088/1742-6596/1684/1/012071},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2020 - Cross- Scenario Person Re-identification.pdf:pdf},
issn = {17426596},
journal = {Journal of Physics: Conference Series},
number = {1},
title = {{Cross- scenario person re-identification}},
volume = {1684},
year = {2020}
}
@article{Luo2021a,
abstract = {Person re-identification (ReID) is one of the commonly used criminal investigation methods in reconnaissance. Although the current ReID has achieved robust results on single domains, the focus of researches has shifted to cross-domain in recent years, which is caused by domain bias between different datasets. Generative Adversarial Networks (GAN) is used to realize the image style transfer of different datasets to alleviate the effect of cross-domain. However, the existing GAN-based models ignore complete expressions and occlusion of pedestrian characteristics, resulting in low accuracy in feature extraction. To address these issues, we introduce a cross domain model based on feature fusion (FFGAN) to fuse global, local and semantic features to extract more delicate pedestrian features. Before extracting pedestrian features, we preprocess feature maps with a feature erasure block to solve an occlusion issue. Finally, FFGAN enables a more complete visual description of pedestrian characteristics, thereby improving the accuracy of FFGAN in identifying pedestrians. Experimental results show that the effect of FFGAN is significantly improved compared with some advanced cross-domain ReID algorithms.},
author = {Luo, Xianjun and Ouyang, Zhi and Du, Nisuo and Song, Jingkuan and Wei, Qin},
doi = {10.1109/ACCESS.2021.3091647},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2021 - Cross-Domain Person Re-Identification.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Person re-identification,cross domain,feature erasure,global feature,local features,semantic features},
pages = {98327--98336},
publisher = {IEEE},
title = {{Cross-Domain Person Re-Identification Based on Feature Fusion}},
volume = {9},
year = {2021}
}
@article{Pang2021,
abstract = {Although the single-domain person re-identification (Re-ID) method has achieved great accuracy, the dependence on the label in the same image domain severely limits the scalability of this method. Therefore, cross-domain Re-ID has received more and more attention. In this paper, a novel cross-domain Re-ID method combining supervised and unsupervised learning is proposed, which includes two models: a triple-condition generative adversarial network (TC-GAN) and a dual-task feature extraction network (DFE-Net). We first use TC-GAN to generate labeled images with the target style, and then we combine supervised and unsupervised learning to optimize DFE-Net. Specifically, we use labeled generated data for supervised learning. In addition, we mine effective information in the target data from two perspectives for unsupervised learning. To effectively combine the two types of learning, we design a dynamic weighting function to dynamically adjust the weights of these two approaches. To verify the validity of TC-GAN, DFE-Net, and the dynamic weight function, we conduct multiple experiments on Market-1501 and DukeMTMC-reID. The experimental results show that the dynamic weight function can improve the performance of the models, and our method is better than many state-of-the-art methods.},
author = {Pang, Zhiqi and Guo, Jifeng and Sun, Wenbo and Xiao, Yanbang and Yu, Ming},
doi = {10.1007/s10489-021-02551-8},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2021 - Cross-domain person re-identification by hybrid supervised.pdf:pdf},
isbn = {1048902102551},
issn = {15737497},
journal = {Applied Intelligence},
keywords = {Computer vision,Deep learning,Generative adversarial networks,Person re-identification},
publisher = {Applied Intelligence},
title = {{Cross-domain person re-identification by hybrid supervised and unsupervised learning}},
year = {2021}
}
@article{Zhang2019,
abstract = {Person re-identification (re-id) refers to matching pedestrians across disjoint yet non-overlapping camera views. The most effective way to match these pedestrians undertaking significant visual variations is to seek reliably invariant features that can describe the person of interest faithfully. Most of existing methods are presented in a supervised manner to produce discriminative features by relying on labeled paired images in correspondence. However, annotating pair-wise images is prohibitively expensive in labours, and thus not practical in large-scale networked cameras. Moreover, seeking comparable representations across camera views demands a flexible model to address the complex distributions of images. In this work, we study the co-occurrence statistic patterns between pairs of images, and propose to crossing Generative Adversarial Network (Cross-GAN) for learning a joint distribution for cross-image representations in an unsupervised manner. Given a pair of person images, the proposed model consists of the variational auto-encoder to encode the pair into respective latent variables, a proposed cross-view alignment to reduce the view disparity, and an adversarial layer to seek the joint distribution of latent representations. The learned latent representations are well-aligned to reflect the co-occurrence patterns of paired images. We empirically evaluate the proposed model against challenging datasets, and our results show the importance of joint invariant features in improving matching rates of person re-id with comparison to semi/unsupervised state-of-the-arts.},
archivePrefix = {arXiv},
arxivId = {1801.01760},
author = {Zhang, Chengyuan and Wu, Lin and Wang, Yang},
doi = {10.1016/j.neucom.2019.01.093},
eprint = {1801.01760},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/Arquitecturas GAN/2018 - CrossGAN.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Generative adversarial networks,Person re-identification,Variational auto-encoder},
pages = {259--269},
title = {{Crossing generative adversarial networks for cross-view person re-identification}},
volume = {340},
year = {2019}
}
@article{Dai2018a,
abstract = {Person re-identification (Re-ID) is an important task in video surveillance which automatically searches and identifies people across different cameras. Despite the extensive Re-ID progress in RGB cameras, few works have studied the Re-ID between infrared and RGB images, which is essentially a cross-modality problem and widely encountered in real-world scenarios. The key challenge lies in two folds, i.e., the lack of discriminative information to re-identify the same person between RGB and infrared modalities, and the difficulty to learn a robust metric for such a large-scale cross-modality retrieval. In this paper, we tackle the above two challenges by proposing a novel cross-modality generative adversarial network (termed cmGAN). To handle the lack of insufficient discriminative information, we design a cutting-edge generative adversarial training based discriminator to learn discriminative feature representation from different modalities. To handle the issue of large-scale cross-modality metric learning, we integrate both identification loss and cross-modality triplet loss, which minimize inter-class ambiguity while maximizing cross-modality similarity among instances. The entire cmGAN can be trained in an end-to-end manner by using standard deep neural network framework. We have quantized the performance of our work in the newly-released SYSU RGB-IR Re-ID benchmark, and have reported superior performance, i.e., Cumulative Match Characteristic curve (CMC) and Mean Average Precision (MAP), over the state-of-the-art works [Wu et al., 2017], at least 12.17{\%} and 11.85{\%} respectively.},
author = {Dai, Pingyang and Ji, Rongrong and Wang, Haibin and Wu, Qiong and Huang, Yuyu},
doi = {10.24963/ijcai.2018/94},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/2018 - Cross-Modality Person Re-Identification with Generative Adversarial Training.pdf:pdf},
isbn = {9780999241127},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Computer Vision: Computer Vision,Computer Vision: Recognition: Detection {\&} Categori},
pages = {677--683},
title = {{Cross-modality person re-identification with generative adversarial training}},
volume = {2018-July},
year = {2018}
}
@article{He,
archivePrefix = {arXiv},
arxivId = {arXiv:1512.03385v1},
author = {He, Kaiming},
eprint = {arXiv:1512.03385v1},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2016 - Resnet Arquitectura.pdf:pdf},
title = {{Deep Residual Learning for Image Recognition}}
}
@article{Li2014a,
abstract = {Person re-identification is to match pedestrian images from disjoint camera views detected by pedestrian detectors. Challenges are presented in the form of complex variations of lightings, poses, viewpoints, blurring effects, image resolutions, camera settings, occlusions and background clutter across camera views. In addition, misalignment introduced by the pedestrian detector will affect most existing person re-identification methods that use manually cropped pedestrian images and assume perfect detection. In this paper, we propose a novel filter pairing neural network (FPNN) to jointly handle misalignment, photometric and geometric transforms, occlusions and background clutter. All the key components are jointly optimized to maximize the strength of each component when cooperating with others. In contrast to existing works that use handcrafted features, our method automatically learns features optimal for the re-identification task from data. The learned filter pairs encode photometric transforms. Its deep architecture makes it possible to model a mixture of complex photometric and geometric transforms. We build the largest benchmark re-id dataset with 13, 164 images of 1, 360 pedestrians. Unlike existing datasets, which only provide manually cropped pedestrian images, our dataset provides automatically detected bounding boxes for evaluation close to practical applications. Our neural network significantly outperforms state-of-the-art methods on this dataset.},
author = {Li, Wei and Zhao, Rui and Xiao, Tong and Wang, Xiaogang},
doi = {10.1109/CVPR.2014.27},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/DataSets/2014 - CUHK03.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Person Re-Identification},
pages = {152--159},
title = {{DeepReID: Deep filter pairing neural network for person re-identification}},
year = {2014}
}
@article{Siarohin2018,
abstract = {In this paper we address the problem of generating person images conditioned on a given pose. Specifically, given an image of a person and a target pose, we synthesize a new image of that person in the novel pose. In order to deal with pixel-to-pixel misalignments caused by the pose differences, we introduce deformable skip connections in the generator of our Generative Adversarial Network. Moreover, a nearest-neighbour loss is proposed instead of the common L1 and L2 losses in order to match the details of the generated image with the target image. We test our approach using photos of persons in different poses and we compare our method with previous work in this area showing state-of-the-art results in two benchmarks. Our method can be applied to the wider field of deformable object generation, provided that the pose of the articulated object can be extracted using a keypoint detector.},
archivePrefix = {arXiv},
arxivId = {1801.00055},
author = {Siarohin, Aliaksandr and Sangineto, Enver and Lathuiliere, Stephane and Sebe, Nicu},
doi = {10.1109/CVPR.2018.00359},
eprint = {1801.00055},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/2018 - Deformable GANs for Pose-based Human Image Generation.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {3408--3416},
title = {{Deformable GANs for Pose-Based Human Image Generation}},
year = {2018}
}
@article{Liu2019,
abstract = {Person re-identification aims at identifying a certain pedestrian across non-overlapping multi-camera networks in different time and places. Existing person re-identification approaches mainly focus on matching pedestrians on images; however, little attention has been paid to re-identify pedestrians in videos. Compared to images, video clips contain motion patterns of pedestrians, which is crucial to person re-identification. Moreover, consecutive video frames present pedestrian appearance with different body poses and from different viewpoints, providing valuable information toward addressing the challenge of pose variation, occlusion, and viewpoint change, and so on. In this article, we propose a Dense 3D-Convolutional Network (D3DNet) to jointly learn spatio-temporal and appearance representation for person re-identification in videos. The D3DNet consists of multiple three-dimensional (3D) dense blocks and transition layers. The 3D dense blocks enlarge the receptive fields of visual neurons in both spatial and temporal dimensions, leading to discriminative appearance representation as well as short-term and long-term motion patterns of pedestrians without the requirement of an additional motion estimation module. Moreover, we formulate a loss function consisting of an identification loss and a center loss to minimize intra-class variance and maximize inter-class variance simultaneously, toward addressing the challenge of large intra-class variance and small inter-class variance. Extensive experiments on two real-world video datasets of person identification, i.e., MARS and iLIDS-VID, have shown the effectiveness of the proposed approach.},
author = {Liu, Jiawei and Zha, Zheng Jun and Chen, Xuejin and Wang, Zilei and Zhang, Yongdong},
doi = {10.1145/3231741},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2019 - Dense 3D-Convolutional Neural Network for Person Re-Identification in Videos.pdf:pdf},
isbn = {6162010600},
issn = {15516865},
journal = {ACM Transactions on Multimedia Computing, Communications and Applications},
keywords = {Deep learning,Network structure,Person re-identification},
number = {1s},
title = {{Dense 3D-convolutional neural network for person re-identification in videos}},
volume = {15},
year = {2019}
}
@article{Zhou2020,
abstract = {Diabetic retinopathy (DR) is a complication of diabetes that severely affects eyes. It can be graded into five levels of severity according to international protocol. However, optimizing a grading model to have strong generalizability requires a large amount of balanced training data, which is difficult to collect, particularly for the high severity levels. Typical data augmentation methods, including random flipping and rotation, cannot generate data with high diversity. In this paper, we propose a diabetic retinopathy generative adversarial network (DR-GAN) to synthesize high-resolution fundus images which can be manipulated with arbitrary grading and lesion information. Thus, large-scale generated data can be used for more meaningful augmentation to train a DR grading and lesion segmentation model. The proposed retina generator is conditioned on the structural and lesion masks, as well as adaptive grading vectors sampled from the latent grading space, which can be adopted to control the synthesized grading severity. Moreover, a multi-scale spatial and channel attention module is devised to improve the generation ability to synthesize small details. Multi-scale discriminators are designed to operate from large to small receptive fields, and joint adversarial losses are adopted to optimize the whole network in an end-to-end manner. With extensive experiments evaluated on the EyePACS dataset connected to Kaggle, as well as the FGADR dataset, we validate the effectiveness of our method, which can both synthesize highly realistic (1280 {\&}{\#}x00D7; 1280) controllable fundus images and contribute to the DR grading task.},
archivePrefix = {arXiv},
arxivId = {1912.04670},
author = {Zhou, Yi and Wang, Boyang and He, Xiaodong and Cui, Shanshan and Shao, Ling},
doi = {10.1109/JBHI.2020.3045475},
eprint = {1912.04670},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2020 - DR-GAN.pdf:pdf},
issn = {21682208},
journal = {IEEE Journal of Biomedical and Health Informatics},
keywords = {Gallium nitride,Generative adversarial networks,Generators,Image segmentation,Image synthesis,Lesions,Retina},
number = {Xx},
pages = {1--11},
pmid = {33332280},
title = {{DR-GAN: Conditional Generative Adversarial Network for Fine-Grained Lesion Synthesis on Diabetic Retinopathy Images}},
volume = {XX},
year = {2020}
}
@article{Kettunen2019,
abstract = {It has been recently shown that the hidden variables of convolutional neural networks make for an efficient perceptual similarity metric that accurately predicts human judgment on relative image similarity assessment. First, we show that such learned perceptual similarity metrics (LPIPS) are susceptible to adversarial attacks that dramatically contradict human visual similarity judgment. While this is not surprising in light of neural networks' well-known weakness to adversarial perturbations, we proceed to show that self-ensembling with an infinite family of random transformations of the input --- a technique known not to render classification networks robust --- is enough to turn the metric robust against attack, while retaining predictive power on human judgments. Finally, we study the geometry imposed by our our novel self-ensembled metric (E-LPIPS) on the space of natural images. We find evidence of "perceptual convexity" by showing that convex combinations of similar-looking images retain appearance, and that discrete geodesics yield meaningful frame interpolation and texture morphing, all without explicit correspondences.},
archivePrefix = {arXiv},
arxivId = {1906.03973},
author = {Kettunen, Markus and H{\"{a}}rk{\"{o}}nen, Erik and Lehtinen, Jaakko},
eprint = {1906.03973},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/Metricas/LPIPS.pdf:pdf},
title = {{E-LPIPS: Robust Perceptual Image Similarity via Random Transformation Ensembles}},
url = {http://arxiv.org/abs/1906.03973},
year = {2019}
}
@article{Gray2007,
abstract = {Traditionally, appearance models for recognition, reacquisition and tracking problems have been evaluated independently using metrics applied to a complete system. It is shown that appearance models for these three problems can be evaluated using a cumulative matching curve on a standardized dataset, and that this one curve can be converted to a synthetic disambiguation rate for single camera tracking or a synthetic reacquisition rate for cross camera tracking. A challenging new dataset for viewpoint invariant pedestrian recognition (VIPeR) is provided as an example. This dataset contains 632 pedestrian image pairs from arbitrary viewpoints. Several baseline methods are tested on this dataset and the results are presented as a benchmark for future appearance models and matching methods.},
author = {Gray, Doug and Brennan, Shane and Tao, Hai},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/DataSets/2017 - VIPeR.pdf:pdf},
journal = {10th International Workshop on Performance Evaluation for Tracking and Surveillance (PETS),},
number = {March},
pages = {41--47},
title = {{Evaluating appearance models for recognition, reacquisition, and tracking}},
volume = {3},
year = {2007}
}
@book{Jiang2021,
abstract = {Recently, GAN based method has demonstrated strong effectiveness in generating augmentation data for person re-identification (ReID), on account of its ability to bridge the gap between domains and enrich the data variety in feature space. However, most of the ReID works pick all the GAN generated data as additional training samples or evaluate the quality of GAN generation at the entire data set level, ignoring the image-level essential feature of data in ReID task. In this paper, we analyze the in-depth characteristics of ReID sample and solve the problem of "What makes a GAN-generated image good for ReID". Specifically, we propose to examine each data sample with id-consistency and diversity constraints by mapping image onto different spaces. With a metric-based sampling method, we demonstrate that not every GAN-generated data is beneficial for augmentation. Models trained with data filtered by our quality evaluation outperform those trained with the full augmentation set by a large margin. Extensive experiments show the effectiveness of our method on both supervised ReID task and unsupervised domain adaptation ReID task.},
archivePrefix = {arXiv},
arxivId = {2108.09977},
author = {Jiang, Yiqi and Chen, Weihua and Sun, Xiuyu and Shi, Xiaoyu and Wang, Fan and Li, Hao},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia (MM '21), October 20{\^{a}}•ﬁ24, 2021, Virtual Event, China},
doi = {10.1145/3474085.3475547},
eprint = {2108.09977},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2021 - Exploring{\_}the{\_}Quality{\_}of{\_}GAN{\_}Generated{\_}Images{\_}for{\_}.pdf:pdf},
isbn = {9781450386517},
keywords = {GAN, Person Re-Identification, Dataset, Augmentati,acm reference format,and hao,augmentation,dataset,fan wang,gan,person re-identification,sampling,weihua chen,xiaoyu shi,xiuyu sun,yiqi jiang},
number = {1},
pages = {4146--4155},
publisher = {Association for Computing Machinery},
title = {{Exploring the Quality of GAN Generated Images for Person Re-Identification}},
volume = {1},
year = {2021}
}
@article{Ge2018,
abstract = {Person re-identification (reID) is an important task that requires to retrieve a person's images from an image dataset, given one image of the person of interest. For learning robust person features, the pose variation of person images is one of the key challenges. Existing works targeting the problem either perform human alignment, or learn human-region-based representations. Extra pose information and computational cost is generally required for inference. To solve this issue, a Feature Distilling Generative Adversarial Network (FD-GAN) is proposed for learning identity-related and pose-unrelated representations. It is a novel framework based on a Siamese structure with multiple novel discriminators on human poses and identities. In addition to the discriminators, a novel same-pose loss is also integrated, which requires appearance of a same person's generated images to be similar. After learning pose-unrelated person features with pose guidance, no auxiliary pose information and additional computational cost is required during testing. Our proposed FD-GAN achieves state-of-the-art performance on three person reID datasets, which demonstrates that the effectiveness and robust feature distilling capability of the proposed FD-GAN.},
archivePrefix = {arXiv},
arxivId = {1810.02936},
author = {Ge, Yixiao and Li, Zhuowan and Zhao, Haiyu and Yin, Guojun and Yi, Shuai and Wang, Xiaogang and Li, Hongsheng},
eprint = {1810.02936},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/2018 - FD-GAN Pose-guided Feature Distilling GAN for.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {NeurIPS},
pages = {1222--1233},
title = {{FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification}},
volume = {2018-Decem},
year = {2018}
}
@article{Yin2020,
abstract = {Person re-identification (re-id) plays a critical role in tracking people via surveillance systems by matching people across non-overlapping camera views at different locations. Although most re-id methods largely depend on the appearance features of a person, such methods always assume that the appearance information (particularly color) is distinguishable. However, distinguishing people who dress in very similar clothes (especially the same type of clothes, e.g. uniform) is ineffective if relying only on appearance cues. We call this problem the fine-grained person re-identification (FG re-id) problem. To solve this problem, rather than relying on clothing color, we propose to exploit two types of local dynamic pose features: motion-attentive local dynamic pose feature and joint-specific local dynamic pose feature. They are complementary to each other and describe identity-specific pose characteristics, which are found to be more unique and discriminative against similar appearance between people. A deep neural network is formed to learn these local dynamic pose features and to jointly quantify motion and global visual cues. Due to the lack of a suitable benchmark dataset for evaluating the FG re-id problem, we also contribute a fine-grained person re-identification (FGPR) dataset, which contains 358 identities. Extensive evaluations on the FGPR dataset show that our proposed model achieves the best performance compared with related person re-id and fine-grained recognition methods for FG re-id. In addition, we verify that our method is still effective for conventional video-based person re-id.},
author = {Yin, Jiahang and Wu, Ancong and Zheng, Wei Shi},
doi = {10.1007/s11263-019-01259-0},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2020 - Fine-Grained Person Re-identification.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Fine-grained cross-view matching,Person re-identification,Visual surveillance},
number = {6},
pages = {1654--1672},
publisher = {Springer US},
title = {{Fine-Grained Person Re-identification}},
url = {https://doi.org/10.1007/s11263-019-01259-0},
volume = {128},
year = {2020}
}
@article{Yu2021,
author = {Yu, Yu and {Zhang Weibin}, Deng and Yun},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/Metricas/FID.pdf:pdf},
number = {September},
pages = {0--7},
title = {{Frechet Inception Distance ( FID ) for Evaluating GANs}},
year = {2021}
}
@article{Nambiar2019,
author = {Nambiar, Athira and Bernardino, Alexandre and Nascimento, Jacinto C},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2019 - Gait-based Person Re-identification A Survey.pdf:pdf},
number = {2},
title = {{Gait-based Person Re-identification : A Survey}},
volume = {52},
year = {2019}
}
@article{Video-based2019,
author = {Video-based, N M Gan-based Pose-aware Regulation and Borgia, Alessandro and Hua, Yang and Kodirov, Elyor and Robertson, Neil M},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/2019 - GAN-Based Pose-Aware Regulation for Video-Based Person Re-Identification.pdf:pdf},
title = {{GAN-based Pose-aware Regulation for Video-based Person Re- identification GAN-based Pose-aware Regulation for Video-based Person Re-identification}},
year = {2019}
}
@article{Wang2019,
abstract = {Generative adversarial networks (GANs) have been extensively studied in the past few years. Arguably the revolutionary techniques are in the area of computer vision such as plausible image generation, image to image translation, facial attribute manipulation and similar domains. Despite the significant success achieved in the computer vision field, applying GANs to real-world problems still poses significant challenges, three of which we focus on here: (1) High quality image generation; (2) Diverse image generation; and (3) Stable training. Through an in-depth review of GAN-related research in the literature, we provide an account of the architecture-variants and loss-variants, which have been proposed to handle these three challenges from two perspectives. We propose loss-variants and architecture-variants for classifying the most popular GANs, and discuss the potential improvements with focusing on these two aspects. While several reviews for GANs have been presented to date, none have focused on the review of GAN-variants based on their handling the challenges mentioned above. In this paper, we review and critically discuss 7 architecture-variant GANs and 9 loss-variant GANs for remedying those three challenges. The objective of this review is to provide an insight on the footprint that current GANs research focuses on the performance improvement. Code related to GAN-variants studied in this work is summarized on https:// github.com/ sheqi/GAN Review.},
archivePrefix = {arXiv},
arxivId = {arXiv:1906.01529v4},
author = {Wang, Zhengwei and She, Qi and Ward, Tom{\'{a}}s E.},
eprint = {arXiv:1906.01529v4},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/2020 - Generative Adversarial Networks in Computer Vision - A survey and Taxonomy.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Architecture-variants,Computer vision,Generative adversarial networks,Loss-variants,Stable training},
number = {1},
pages = {1--41},
title = {{Generative adversarial networks: A survey and taxonomy}},
year = {2019}
}
@article{Tang2021,
abstract = {Recently, person re-identification (re-ID) with weakly labeled or unlabeled data has drawn much attention in open-set and cross-domain re-ID systems especially for the attribute auxiliary weakly supervised person re-ID. Although state-of-the-art clustering-based methods have achieved good performance, the pseudo labels generated through clustering are often low-quality and noisy. To address this problem, we propose a graph neural network based Attribute Auxiliary structured Grouping (A2G) to improve the confidence of the pseudo labels. Different from the existing clustering-based approaches that only utilize the similarity in feature space, we learn the feature representation from the similarities in both attribute space and feature space by graph learning on the pedestrian attribute graph. Specifically, we first utilize the pair-wise attribute similarity to represent the relation between two pedestrians to construct a pedestrian attribute graph. Furthermore, we aggregate the features from their neighborhood on a pedestrian attribute graph by the graph neural network, which would make the attribute similar pairs closer and simultaneously take the dissimilar pairs further in the feature space. Finally, to avoid the over-confidence of the hard pseudo labels, we regularize the learning of the embedding model with the smoothed pseudo label (SPL) in the optimization of the feature embedding network. We conduct extensive experiments on several person re-ID datasets to validate the efficacy of our proposed method. The results demonstrate that our technique is effective and promising for person re-ID tasks.},
author = {Tang, Geyu and Gao, Xingyu and Chen, Zhenyu and Zhong, Huicai},
doi = {10.1109/ACCESS.2021.3069915},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2021 - Graph{\_}Neural{\_}Network{\_}Based{\_}Attribute{\_}Auxiliary{\_}Str.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Aggregates,Annotations,Cameras,Graph neural networks,Optimization,Smoothing methods,Task analysis,Unsupervised person re-identification,attribute-auxiliary structured grouping,graph neural network},
pages = {1--13},
title = {{Graph Neural Network Based Attribute Auxiliary Structured Grouping for Person Re-Identification}},
volume = {4},
year = {2021}
}
@article{Luberg2018,
author = {Luberg, Karl-kristjan},
file = {:C$\backslash$:/Users/Lau/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Luberg - 2018 - Human body poses recognition using neural networks with class based data augmentation.pdf:pdf},
title = {{Human body poses recognition using neural networks with class based data augmentation}},
year = {2018}
}
@article{Zeng2020,
abstract = {Most person re-identification (ReID) approaches assume that person images are captured under relatively similar illumination conditions. In reality, long-term person retrieval is common, and person images are often captured under different illumination conditions at different times across a day. In this situation, the performances of existing ReID models often degrade dramatically. This paper addresses the ReID problem with illumination variations and names it as Illumination-Adaptive Person Re-identification (IA-ReID). We propose an Illumination-Identity Disentanglement (IID) network to dispel different scales of illuminations away while preserving individuals' identity information. To demonstrate the illumination issue and to evaluate our model, we construct two large-scale simulated datasets with a wide range of illumination variations. Experimental results on the simulated datasets and real-world images demonstrate the effectiveness of the proposed framework.},
archivePrefix = {arXiv},
arxivId = {1905.04525},
author = {Zeng, Zelong and Wang, Zhixiang and Wang, Zheng and Zheng, Yinqiang and Chuang, Yung Yu and Satoh, Shin'Ichi},
doi = {10.1109/TMM.2020.2969782},
eprint = {1905.04525},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2020 - Illumination-Adaptive Person Re-identification.pdf:pdf},
issn = {19410077},
journal = {IEEE Transactions on Multimedia},
keywords = {Person re-identification,feature disentanglement,illumination-adaptive},
number = {12},
pages = {3064--3074},
title = {{Illumination-Adaptive Person Re-Identification}},
volume = {22},
year = {2020}
}
@article{Wang2004,
abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a Structural Similarity Index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000.},
author = {Wang, Zhou and Bovik, Alan Conrad and Sheikh, Hamid Rahim and Simoncelli, Eero P.},
doi = {10.1109/TIP.2003.819861},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/Metricas/SSIM.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Error sensitivity,Human visual system (HVS),Image coding,Image quality assessment,JPEG,JPEG2000,Perceptual quality,Structural information,Structural similarity (SSIM)},
number = {4},
pages = {600--612},
pmid = {15376593},
title = {{Image quality assessment: From error visibility to structural similarity}},
volume = {13},
year = {2004}
}
@article{Deng2018,
abstract = {Person re-identification (re-ID) models trained on one domain often fail to generalize well to another. In our attempt, we present a 'learning via translation' framework. In the baseline, we translate the labeled images from source to target domain in an unsupervised manner. We then train re-ID models with the translated images by supervised methods. Yet, being an essential part of this framework, unsupervised image-image translation suffers from the information loss of source-domain labels during translation. Our motivation is two-fold. First, for each image, the discriminative cues contained in its ID label should be maintained after translation. Second, given the fact that two domains have entirely different persons, a translated image should be dissimilar to any of the target IDs. To this end, we propose to preserve two types of unsupervised similarities, 1) self-similarity of an image before and after translation, and 2) domain-dissimilarity of a translated source image and a target image. Both constraints are implemented in the similarity preserving generative adversarial network (SPGAN) which consists of an Siamese network and a CycleGAN. Through domain adaptation experiment, we show that images generated by SPGAN are more suitable for domain adaptation and yield consistent and competitive re-ID accuracy on two large-scale datasets.},
archivePrefix = {arXiv},
arxivId = {1711.07027},
author = {Deng, Weijian and Zheng, Liang and Ye, Qixiang and Kang, Guoliang and Yang, Yi and Jiao, Jianbin},
doi = {10.1109/CVPR.2018.00110},
eprint = {1711.07027},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/2018 - Image-Image Domain Adaptation with Preserved Self-Similarity and.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {994--1003},
publisher = {IEEE},
title = {{Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification}},
year = {2018}
}
@article{Russakovsky,
archivePrefix = {arXiv},
arxivId = {arXiv:1409.0575v3},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Jan, C V and Krause, J and Ma, S},
eprint = {arXiv:1409.0575v3},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2015 - ImageNet.pdf:pdf},
keywords = {benchmark,dataset,large-scale,object detection,object recognition},
title = {{ImageNet Large Scale Visual Recognition Challenge}}
}
@article{Isola2017,
abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
archivePrefix = {arXiv},
arxivId = {1611.07004},
author = {Isola, Phillip and Zhu, Jun Yan and Zhou, Tinghui and Efros, Alexei A.},
doi = {10.1109/CVPR.2017.632},
eprint = {1611.07004},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/Otros/2017 - PatchGAN.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {5967--5976},
title = {{Image-to-image translation with conditional adversarial networks}},
volume = {2017-Janua},
year = {2017}
}
@article{Li2021,
author = {Li, Yuanyuan and Chen, Sixin and Qi, Guanqiu and Zhu, Zhiqin and Haner, Matthew and Cai, Ruihua},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/2021 - A GAN-Based Self-Training Framework for Unsupervised.pdf:pdf},
pages = {1--16},
title = {{Imaging A GAN-Based Self-Training Framework for Unsupervised Domain Adaptive Person Re-Identification}},
year = {2021}
}
@article{Chen2019,
abstract = {Existing person re-identification (re-id) methods mostly assume the availability of large-scale identity labels for model learning in any target domain deployment. This greatly limits their scalability in practice. To tackle this limitation, we propose a novel Instance-Guided Context Rendering scheme, which transfers the source person identities into diverse target domain contexts to enable supervised re-id model learning in the unlabelled target domain. Unlike previous image synthesis methods that transform the source person images into limited fixed target styles, our approach produces more visually plausible, and diverse synthetic training data. Specifically, we formulate a dual conditional generative adversarial network that augments each source person image with rich contextual variations. To explicitly achieve diverse rendering effects, we leverage abundant unlabelled target instances as contextual guidance for image generation. Extensive experiments on Market-1501, DukeMTMC-reID and CUHK03 benchmarks show that the re-id performance can be significantly improved when using our synthetic data in cross-domain re-id model learning.},
author = {Chen, Yanbei and Zhu, Xiatian and Gong, Shaogang},
doi = {10.1109/ICCV.2019.00032},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2019 - Instance-Guided Context Rendering for Cross-Domain Person Re-Identification.pdf:pdf},
isbn = {9781728148038},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {232--242},
title = {{Instance-guided context rendering for cross-domain person re-identification}},
volume = {2019-Octob},
year = {2019}
}
@article{Zhu2020,
abstract = {Existing person re-identification (re-id) methods mostly exploit a large set of cross-camera identity labelled training data. This requires a tedious data collection and annotation process, leading to poor scalability in practical re-id applications. On the other hand unsupervised re-id methods do not need identity label information, but they usually suffer from much inferior and insufficient model performance. To overcome these fundamental limitations, we propose a novel person reidentification paradigm based on an idea of independent per-camera identity annotation. This eliminates the most time-consuming and tedious inter-camera identity labelling process, significantly reducing the amount of human annotation efforts. Consequently, it gives rise to a more scalable and more feasible setting, which we call Intra-Camera Supervised (ICS) person re-id, for which we formulate a Multi-tAsk mulTi-labEl (MATE) deep learning method. Specifically, MATE is designed for self-discovering the cross-camera identity correspondence in a per-camera multi-task inference framework. Extensive experiments demonstrate the cost-effectiveness},
archivePrefix = {arXiv},
arxivId = {2002.05046},
author = {Zhu, Xiangping and Zhu, Xiatian and Li, Minxian and Morerio, Pietro and Murino, Vittorio and Gong, Shaogang},
doi = {10.1007/s11263-021-01440-4},
eprint = {2002.05046},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2021 - Intra-Camera Supervised Person Re-Identification.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Person re-identification,Intra-camera labelling,Cr,cross-camera labelling,intra-camera labelling,multi-label,multi-task learning,person re-identification},
publisher = {Springer US},
title = {{Intra-camera supervised person re-identification}},
url = {https://doi.org/10.1007/s11263-021-01440-4},
year = {2020}
}
@article{Zheng2019,
abstract = {Person re-identification (re-id) remains challenging due to significant intra-class variations across different cameras. Recently, there has been a growing interest in using generative models to augment training data and enhance the invariance to input changes. The generative pipelines in existing methods, however, stay relatively separate from the discriminative re-id learning stages. Accordingly, re-id models are often trained in a straightforward manner on the generated data. In this paper, we seek to improve learned re-id embeddings by better leveraging the generated data. To this end, we propose a joint learning framework that couples re-id learning and data generation end-to-end. Our model involves a generative module that separately encodes each person into an appearance code and a structure code, and a discriminative module that shares the appearance encoder with the generative module. By switching the appearance or structure codes, the generative module is able to generate high-quality cross-id composed images, which are online fed back to the appearance encoder and used to improve the discriminative module. The proposed joint learning framework renders significant improvement over the baseline without using generated data, leading to the state-of-the-art performance on several benchmark datasets.},
archivePrefix = {arXiv},
arxivId = {1904.07223},
author = {Zheng, Zhedong and Yang, Xiaodong and Yu, Zhiding and Zheng, Liang and Yang, Yi and Kautz, Jan},
doi = {10.1109/CVPR.2019.00224},
eprint = {1904.07223},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/2019 - Joint Discriminative and Generative Learning for Person Re-identification.pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {And Body Pose,Categorization,Face,Gesture,Recognition: Detection,Representation Learning,Retrieval},
pages = {2133--2142},
title = {{Joint discriminative and generative learning for person re-identification}},
volume = {2019-June},
year = {2019}
}
@article{Chen2020,
abstract = {Recent self-supervised contrastive learning provides an effective approach for unsupervised person re-identification (ReID) by learning invariance from different views (transformed versions) of an input. In this paper, we incorporate a Generative Adversarial Network (GAN) and a contrastive learning module into one joint training framework. While the GAN provides online data augmentation for contrastive learning, the contrastive module learns view-invariant features for generation. In this context, we propose a mesh-based view generator. Specifically, mesh projections serve as references towards generating novel views of a person. In addition, we propose a view-invariant loss to facilitate contrastive learning between original and generated views. Deviating from previous GAN-based unsupervised ReID methods involving domain adaptation, we do not rely on a labeled source dataset, which makes our method more flexible. Extensive experimental results show that our method significantly outperforms state-of-the-art methods under both, fully unsupervised and unsupervised domain adaptive settings on several large scale ReID datsets.},
archivePrefix = {arXiv},
arxivId = {2012.09071},
author = {Chen, Hao and Wang, Yaohui and Lagadec, Benoit and Dantcheva, Antitza and Bremond, Francois},
doi = {10.1109/cvpr46437.2021.00204},
eprint = {2012.09071},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2021 - Joint Generative and Contrastive Learning for Unsupervised Person.pdf:pdf},
pages = {2004--2013},
title = {{Joint Generative and Contrastive Learning for Unsupervised Person Re-identification}},
url = {http://arxiv.org/abs/2012.09071},
year = {2020}
}
@article{Wang2018,
abstract = {The combination of global and partial features has been an essential solution to improve discriminative performances in person re-identification (Re-ID) tasks. Previous part-based methods mainly focus on locating regions with specific pre-defined semantics to learn local representations, which increases learning difficulty but not efficient or robust to scenarios with large variances. In this paper, we propose an end-to-end feature learning strategy integrating discriminative information with various granularities. We carefully design the Multiple Granularity Network (MGN), a multi-branch deep network architecture consisting of one branch for global feature representations and two branches for local feature representations. Instead of learning on semantic regions, we uniformly partition the images into several stripes, and vary the number of parts in different local branches to obtain local feature representations with multiple granularities. Comprehensive experiments implemented on the mainstream evaluation datasets including Market-1501, DukeMTMC-reid and CUHK03 indicate that our method robustly achieves state-of-the-art performances and outperforms any existing approaches by a large margin. For example, on Market-1501 dataset in single query mode, we obtain a top result of Rank-1/mAP=96.6{\%}/94.2{\%} with this method after re-ranking.},
archivePrefix = {arXiv},
arxivId = {1804.01438},
author = {Wang, Guanshuo and Yuan, Yufeng and Chen, Xiong and Li, Jiwei and Zhou, Xi},
doi = {10.1145/3240508.3240552},
eprint = {1804.01438},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2018 - Learning Discriminative Features with Multiple Granularities.pdf:pdf},
isbn = {9781450356657},
journal = {MM 2018 - Proceedings of the 2018 ACM Multimedia Conference},
keywords = {Feature learning,Multi-branch deep network,Person re-identification},
pages = {274--282},
title = {{Learning discriminative features with multiple granularities for person re-identification}},
year = {2018}
}
@article{Eom2019,
abstract = {We address the problem of person re-identification (reID), that is, retrieving person images from a large dataset, given a query image of the person of interest. A key challenge is to learn person representations robust to intra-class variations, as different persons can have the same attribute and the same person's appearance looks different with viewpoint changes. Recent reID methods focus on learning discriminative features but robust to only a particular factor of variations (e.g., human pose), which requires corresponding supervisory signals (e.g., pose annotations). To tackle this problem, we propose to disentangle identity-related and -unrelated features from person images. Identity-related features contain information useful for specifying a particular person (e.g., clothing), while identity-unrelated ones hold other factors (e.g., human pose, scale changes). To this end, we introduce a new generative adversarial network, dubbed identity shuffle GAN (IS-GAN), that factorizes these features using identification labels without any auxiliary information. We also propose an identity-shuffling technique to regularize the disentangled features. Experimental results demonstrate the effectiveness of IS-GAN, significantly outperforming the state of the art on standard reID benchmarks including the Market-1501, CUHK03 and DukeMTMC-reID. Our code and models are available online: https://cvlab-yonsei.github.io/projects/ISGAN/.},
archivePrefix = {arXiv},
arxivId = {1910.12003},
author = {Eom, Chanho and Ham, Bumsub},
eprint = {1910.12003},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2019 - Learning Disentangled Representation for Robust.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
title = {{Learning disentangled representation for robust person re-identification}},
volume = {32},
year = {2019}
}
@article{Kim,
archivePrefix = {arXiv},
arxivId = {arXiv:1703.05192v2},
author = {Kim, Taeksoo and Cha, Moonsu and Kim, Hyunsoo and Kwon, Jung and Jiwon, Lee},
eprint = {arXiv:1703.05192v2},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2017 - DiscoGAN.pdf:pdf},
title = {{Learning to Discover Cross-Domain Relations with Generative Adversarial Networks}}
}
@article{Barbosa2018,
abstract = {Re-identification is generally carried out by encoding the appearance of a subject in terms of outfit, suggesting scenarios where people do not change their attire. In this paper we overcome this restriction, by proposing a framework based on a deep convolutional neural network, SOMAnet, that additionally models other discriminative aspects, namely, structural attributes of the human figure (e.g. height, obesity, gender). Our method is unique in many respects. First, SOMAnet is based on the Inception architecture, departing from the usual siamese framework. This spares expensive data preparation (pairing images across cameras) and allows the understanding of what the network learned. Second, and most notably, the training data consists of a synthetic 100K instance dataset, SOMAset, created by photorealistic human body generation software. SOMAset will be released with a open source license to enable further developments in re-identification. Synthetic data represents a cost-effective way of acquiring semi-realistic imagery (full realism is usually not required in re-identification since surveillance cameras capture low-resolution silhouettes), while at the same time providing complete control of the samples in terms of ground truth. Thus it is relatively easy to customize the data w.r.t. the surveillance scenario at-hand, e.g. ethnicity. SOMAnet, trained on SOMAset and fine-tuned on recent re-identification benchmarks, matches subjects even with different apparel.},
archivePrefix = {arXiv},
arxivId = {1701.03153},
author = {Barbosa, Igor Barros and Cristani, Marco and Caputo, Barbara and Rognhaugen, Aleksander and Theoharis, Theoharis},
doi = {10.1016/j.cviu.2017.12.002},
eprint = {1701.03153},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/2017 - Synthetic training data for deep CNNs in re-identification.pdf:pdf},
issn = {1090235X},
journal = {Computer Vision and Image Understanding},
keywords = {Automated training dataset generation,Deep learning,Re-identification,Re-identification photorealistic dataset,Training set},
number = {December},
pages = {50--62},
title = {{Looking beyond appearances: Synthetic training data for deep CNNs in re-identification}},
volume = {167},
year = {2018}
}
@article{Zhao2015,
abstract = {Neural networks are becoming central in several areas of computer vision and image processing and different architectures have been proposed to solve specific problems. The impact of the loss layer of neural networks, however, has not received much attention in the context of image processing: the default and virtually only choice is L2. In this paper, we bring attention to alternative choices for image restoration. In particular, we show the importance of perceptually-motivated losses when the resulting image is to be evaluated by a human observer. We compare the performance of several losses, and propose a novel, differentiable error function. We show that the quality of the results improves significantly with better loss functions, even when the network architecture is left unchanged.},
archivePrefix = {arXiv},
arxivId = {1511.08861},
author = {Zhao, Hang and Gallo, Orazio and Frosio, Iuri and Kautz, Jan},
eprint = {1511.08861},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/Metricas/L2.pdf:pdf},
number = {November},
title = {{Loss Functions for Neural Networks for Image Processing}},
url = {http://arxiv.org/abs/1511.08861},
year = {2015}
}
@article{Liang2018,
abstract = {Cross-domain transfer learning (CDTL) is an extremely challenging task for the person re-identification (ReID). Given a source domain with annotations and a target domain without annotations, CDTL seeks an effective method to transfer the knowledge from the source domain to the target domain. However, such a simple two-domain transfer learning method is unavailable for the person ReID in that the source/target domain consists of several sub-domains, e.g., camera-based sub-domains. To address this intractable problem, we propose a novel Many-to-Many Generative Adversarial Transfer Learning method (M2M-GAN) that takes multiple source sub-domains and multiple target sub-domains into consideration and performs each sub-domain transferring mapping from the source domain to the target domain in a unified optimization process. The proposed method first translates the image styles of source sub-domains into that of target sub-domains, and then performs the supervised learning by using the transferred images and the corresponding annotations in source domain. As the gap is reduced, M2M-GAN achieves a promising result for the cross-domain person ReID. Experimental results on three benchmark datasets Market-1501, DukeMTMC-reID and MSMT17 show the effectiveness of our M2M-GAN.},
archivePrefix = {arXiv},
arxivId = {1811.03768},
author = {Liang, Wenqi and Wang, Guangcong and Lai, Jianhuang and Zhu, Junyong},
eprint = {1811.03768},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2018 - M2M-GAN Many-to-Many Generative Adversarial Transfer Learning for Person.pdf:pdf},
title = {{M2M-GAN: Many-to-Many Generative Adversarial Transfer Learning for Person Re-Identification}},
url = {http://arxiv.org/abs/1811.03768},
year = {2018}
}
@article{Ye2019a,
abstract = {Visible thermal person re-identification (VT-ReID) is a cross-modality pedestrian retrieval problem, which automatically searches persons between day-time visible images and night-time thermal images. Despite the extensive progress in single-modality ReID, the cross-modality pedestrian retrieval problem has limited attention due to its challenges in modality discrepancy and large intra-class variations across cameras. Existing cross-modality ReID methods usually solve this problem by learning cross-modality feature representations with modality-sharable classifier. However, this learning strategy may lose discriminative information in different modalities. In this paper, we propose a novel modality-aware collaborative (MAC) learning method on top of a two-stream network for VT-ReID, which handles the modality-discrepancy in both feature level and classifier level. In feature level, it handles the modality discrepancy by a two-stream network with different parameters. In classifier level, it contains two separate modality-specific identity classifiers for two modalities to capture the modality-specific information, and they have the same network architecture but different parameters. In addition, we introduce a collaborative learning scheme, which regularizes the modality-sharable and modality-specific identity classifiers by utilizing the relationship between different classifiers. Extensive experiments on two cross-modality person re-identification datasets demonstrate the superiority of the proposed method, achieving much better performance than the state-of-the-art.},
author = {Ye, Mang and Lan, Xiangyuan and Leng, Qingming},
doi = {10.1145/3343031.3351043},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2019 - Modality-aware Collaborative Learning for Visible Thermal.pdf:pdf},
isbn = {9781450368896},
journal = {MM 2019 - Proceedings of the 27th ACM International Conference on Multimedia},
keywords = {Collaborative Learning,Cross-modality,Pedestrian Retrieval,Person Re-Identification},
pages = {347--355},
title = {{Modality-aware collaborative learning for visible thermal person re-identification}},
year = {2019}
}
@article{He2020a,
abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
archivePrefix = {arXiv},
arxivId = {1911.05722},
author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
doi = {10.1109/CVPR42600.2020.00975},
eprint = {1911.05722},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/Metricas/MOCO.pdf:pdf},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {9726--9735},
title = {{Momentum Contrast for Unsupervised Visual Representation Learning}},
year = {2020}
}
@article{Zhou2019,
abstract = {Person re-identification is a cross-camera retrieval task. Person re-identification performance in a single dataset has been significantly improved, but person re-identification model trained in one dataset usually can't work well in another dataset. To solve this problem, this paper proposes a method of image-to-image translation, CTGAN (Multi-Camera Transfer GAN), which can be performed on multiple camera domains of pedestrian dataset by using one single model. The marked training images are transferred to each camera of the target dataset. At the same time, for the feature learning model, this paper adopts the MSCDA (Mixed Selective Convolution Descriptor Aggregation) method, which can locate the main pedestrian objects in the image, filter out the background noise, and keep the useful depth descriptor. In the paper, experiments show that the method is effective.},
author = {Zhou, Shuren and Ke, Maolin and Luo, Peng},
doi = {10.1016/j.jvcir.2019.01.029},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/2019 - Multi-camera transfer GAN for person re-identification.pdf:pdf},
issn = {10959076},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Computer vision,Deep learning,Generative adversarial networks,Person re-identification},
pages = {393--400},
publisher = {Elsevier Inc.},
title = {{Multi-camera transfer GAN for person re-identification}},
url = {https://doi.org/10.1016/j.jvcir.2019.01.029},
volume = {59},
year = {2019}
}
@article{Huang2019,
abstract = {Sufficient training data normally is required to train deeply learned models. However, due to the expensive manual process for a labeling large number of images (i.e., annotation), the amount of available training data (i.e., real data) is always limited. To produce more data for training a deep network, generative adversarial network can be used to generate artificial sample data (i.e., generated data). However, the generated data usually does not have annotation labels. To solve this problem, in this paper, we propose a virtual label called Multi-pseudo Regularized Label (MpRL) and assign it to the generated data. With MpRL, the generated data will be used as the supplementary of real training data to train a deep neural network in a semi-supervised learning fashion. To build the corresponding relationship between the real data and generated data, MpRL assigns each generated data a proper virtual label which reflects the likelihood of the affiliation of the generated data to pre-defined training classes in the real data domain. Unlike the traditional label which usually is a single integral number, the virtual label proposed in this paper is a set of weight-based values each individual of which is a number in (0,1] called multi-pseudo label and reflects the degree of relation between each generated data to every pre-defined class of real data. A comprehensive evaluation is carried out by adopting two state-of-the-art convolutional neural networks (CNNs) in our experiments to verify the effectiveness of MpRL. Experiments demonstrate that by assigning MpRL to generated data, we can further improve the person re-ID performance on five re-ID datasets, i.e., Market-1501, DukeMTMC-reID, CUHK03, VIPeR, and CUHK01. The proposed method obtains +6.29{\%}, +6.30{\%}, +5.58{\%}, +5.84{\%}, and +3.48{\%} improvements in rank-1 accuracy over a strong CNN baseline on the five datasets, respectively, and outperforms state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {1801.06742},
author = {Huang, Yan and Xu, Jingsong and Wu, Qiang and Zheng, Zhedong and Zhang, Zhaoxiang and Zhang, Jian},
doi = {10.1109/TIP.2018.2874715},
eprint = {1801.06742},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/2018 - Multi-pseudo Regularized Label for Generated Data in Person Re-Identification.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Person re-identification,generated data,semi-supervised learning,virtual label},
number = {3},
pages = {1391--1403},
title = {{Multi-pseudo regularized label for generated data in person re-identification}},
volume = {28},
year = {2019}
}
@article{Zhang2020,
abstract = {Person re-identification (person Re-Id) aims to retrieve the pedestrian images of the same person that captured by disjoint and non-overlapping cameras. Lots of researchers recently focused on this hot issue and proposed deep learning based methods to enhance the recognition rate in a supervised or unsupervised manner. However,there are two limitations that cannot be ignored: firstly, compared with other image retrieval benchmarks, the size of existing person Re-Id datasets is far from meeting the requirement, which cannot provide sufficient pedestrian samples for the training of deep model; secondly, the samples in existing datasets do not have sufficient human motions or postures coverage to provide more priori knowledges for learning. In this paper, we introduce a novel unsupervised pose augmentation cross-view person Re-Id scheme called PAC-GAN to overcome these limitations. We firstly present the formal definition of cross-view pose augmentation and then propose the framework of PAC-GAN that is a novel conditional generative adversarial network (CGAN) based approach to improve the performance of unsupervised corss-view person Re-Id. Specifically, the pose generation model in PAC-GAN called CPG-Net is to generate enough quantity of pose-rich samples from original image and skeleton samples. The pose augmentation dataset is produced by combining the synthesized pose-rich samples with the original samples, which is fed into the corss-view person Re-Id model named Cross-GAN. Besides, we use weight-sharing strategy in the CPG-Net to improve the quality of new generated samples. To the best of our knowledge, we are the first to enhance the unsupervised cross-view person Re-Id by pose augmentation, and the results of extensive experiments show that the proposed scheme can combat the state-of-the-arts with recognition rate.},
archivePrefix = {arXiv},
arxivId = {1906.01792},
author = {Zhang, Chengyuan and Zhu, Lei and Zhang, Shi Chao and Yu, Weiren},
doi = {10.1016/j.neucom.2019.12.094},
eprint = {1906.01792},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/2019 - PAC-GAN An effective pose augmentation scheme for unsupervised.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Cross-view person Re-Id,Generative adversarial networks,Pose augmentation,Unsupervised learning},
pages = {22--39},
publisher = {Elsevier B.V.},
title = {{PAC-GAN: An effective pose augmentation scheme for unsupervised cross-view person re-identification}},
url = {https://doi.org/10.1016/j.neucom.2019.12.094},
volume = {387},
year = {2020}
}
@article{Ristani2016,
abstract = {To help accelerate progress in multi-target, multi-camera tracking systems, we present (i) a new pair of precision-recall measures of performance that treats errors of all types uniformly and emphasizes correct identification over sources of error; (ii) the largest fully-annotated and calibrated data set to date with more than 2 million frames of 1080 p, 60 fps video taken by 8 cameras observing more than 2, 700 identities over 85 min; and (iii) a reference software system as a comparison baseline. We show that (i) our measures properly account for bottom-line identity match performance in the multi-camera setting; (ii) our data set poses realistic challenges to current trackers; and (iii) the performance of our system is comparable to the state of the art.},
archivePrefix = {arXiv},
arxivId = {1609.01775},
author = {Ristani, Ergys and Solera, Francesco and Zou, Roger and Cucchiara, Rita and Tomasi, Carlo},
doi = {10.1007/978-3-319-48881-3_2},
eprint = {1609.01775},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/DataSets/2016 - DukeMTMC.pdf:pdf},
isbn = {9783319488806},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Identity management,Large scale data set,Multi camera data set,Multi camera tracking,Performance evaluation},
number = {c},
pages = {17--35},
title = {{Performance measures and a data set for multi-target, multi-camera tracking}},
volume = {9914 LNCS},
year = {2016}
}
@book{Cristani2018,
abstract = {Person re-identification is defined as the problem of recognizing an individual captured in diverse times and/or locations over several nonoverlapping camera views, considering a large set of candidates. This problem affects primarily the management of distributed, multiview surveillance systems, in which subjects must be tracked across different places, either a posteriori or on-the-fly when they move through different locations. Re-identification is a very difficult problem, as most of the time people can be captured by several low resolution cameras, under occlusion conditions, badly (and different from view to view) illuminated, and in varying poses. In this context, a robust modeling of the entire body appearance of a person is necessary, especially when other classical biometric cues (face, gait) are not available or difficult to catch, due to the sensors' scarce resolution or low frame-rate. This chapter gives an overview of the re-identification problem, illustrating the standard re-identification pipeline and detailing the several approaches and techniques, devoting more attention to those which have shown to be particularly effective and significant.},
author = {Cristani, Marco and Murino, Vittorio},
booktitle = {Academic Press Library in Signal Processing: Image and Video Processing and Analysis and Computer Vision},
doi = {10.1016/B978-0-12-811889-4.00010-5},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2018 - Chapter 10 - Person re-identification.pdf:pdf},
isbn = {9780128119006},
keywords = {Re-identification,Video surveillance},
pages = {365--394},
publisher = {Elsevier Ltd.},
title = {{Person re-identification}},
url = {http://dx.doi.org/10.1016/B978-0-12-811889-4.00010-5},
volume = {6},
year = {2018}
}
@article{Zheng2017a,
abstract = {This paper1 presents a novel large-scale dataset and comprehensive baselines for end-to-end pedestrian detection and person recognition in raw video frames. Our baselines address three issues: the performance of various combinations of detectors and recognizers, mechanisms for pedestrian detection to help improve overall re-identification (re-ID) accuracy and assessing the effectiveness of different detectors for re-ID. We make three distinct contributions. First, a new dataset, PRW, is introduced to evaluate Person Reidentification in the Wild, using videos acquired through six near-synchronized cameras. It contains 932 identities and 11,816 frames in which pedestrians are annotated with their bounding box positions and identities. Extensive benchmarking results are presented on this dataset. Second, we show that pedestrian detection aids re-ID through two simple yet effective improvements: a cascaded fine-tuning strategy that trains a detection model first and then the classification model, and a Confidence Weighted Similarity (CWS) metric that incorporates detection scores into similarity measurement. Third, we derive insights in evaluating detector performance for the particular scenario of accurate person re-ID.},
archivePrefix = {arXiv},
arxivId = {1604.02531},
author = {Zheng, Liang and Zhang, Hengheng and Sun, Shaoyan and Chandraker, Manmohan and Yang, Yi and Tian, Qi},
doi = {10.1109/CVPR.2017.357},
eprint = {1604.02531},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2016 - Person Re-identification in the Wild.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
number = {April 2016},
pages = {3346--3355},
title = {{Person re-identification in theWild}},
volume = {2017-Janua},
year = {2017}
}
@article{Vanhoucke2014,
abstract = {Invited Talk at ICLR 2014},
archivePrefix = {arXiv},
arxivId = {arXiv:1610.02984v1},
author = {Vanhoucke, Vincent},
eprint = {arXiv:1610.02984v1},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2016 - Person Re-identification Past-Present-Future.pdf:pdf},
journal = {ICLR invited talk},
number = {April},
pages = {1--20},
title = {{Person Re-identification Past-Present-Future}},
volume = {14},
year = {2014}
}
@article{Wei2018,
abstract = {Although the performance of person Re-Identification (ReID) has been significantly boosted, many challenging issues in real scenarios have not been fully investigated, e.g., the complex scenes and lighting variations, viewpoint and pose changes, and the large number of identities in a camera network. To facilitate the research towards conquering those issues, this paper contributes a new dataset called MSMT171 with many important features, e.g., 1) the raw videos are taken by an 15-camera network deployed in both indoor and outdoor scenes, 2) the videos cover a long period of time and present complex lighting variations, and 3) it contains currently the largest number of annotated identities, i.e., 4,101 identities and 126,441 bounding boxes. We also observe that, domain gap commonly exists between datasets, which essentially causes severe performance drop when training and testing on different datasets. This results in that available training data cannot be effectively leveraged for new testing domains. To relieve the expensive costs of annotating new training samples, we propose a Person Transfer Generative Adversarial Network (PTGAN) to bridge the domain gap. Comprehensive experiments show that the domain gap could be substantially narrowed-down by the PTGAN.},
archivePrefix = {arXiv},
arxivId = {1711.08565},
author = {Wei, Longhui and Zhang, Shiliang and Gao, Wen and Tian, Qi},
doi = {10.1109/CVPR.2018.00016},
eprint = {1711.08565},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/2018 - Person Transfer GAN to Bridge Domain Gap for Person Re-Identification.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {79--88},
title = {{Person Transfer GAN to Bridge Domain Gap for Person Re-identification}},
year = {2018}
}
@article{Zhang2020b,
abstract = {The block-based representation learning method has been proven to be a very effective method for person reidentification (Re-ID), but the features extracted by the existing block-based approach tend to have a high correlation among different blocks. Also, these methods perform less well for persons with large posture changes. Thus, part-based nondirect coupling representation learning method is proposed by introducing a similarity measure loss to constrain features of different blocks. Moreover, part-based nondirect coupling embedded GAN method is proposed, which aims to extract more common features of different postures of a same person. In this way, the extracted features of the network are robust for posture changes of a person, and there are no auxiliary pose information and additional computational cost required in the test stage. Experimental results on public datasets show that our proposed method achieves good performances, especially, it outperforms the state-of-the-art GAN-based methods for person Re-ID.},
author = {Zhang, Yue and Jin, Yi and Chen, Jianqiang and Kan, Shichao and Cen, Yigang and Cao, Qi},
doi = {10.1109/MMUL.2020.2999445},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/2020 -  PGAN part-based nondirect coupling embedded GAN for person re-id.pdf:pdf},
issn = {19410166},
journal = {IEEE Multimedia},
number = {3},
pages = {23--33},
title = {{PGAN: Part-based nondirect coupling embedded gan for person reidentification}},
volume = {27},
year = {2020}
}
@article{Re-identification2018,
author = {Re-identification, Person},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/2018 - Pose-Normalized Image Generation for Person.pdf:pdf},
journal = {The European Conference on Computer Vision (ECCV)},
keywords = {gan,person re-id,pose normalization},
pages = {650--667},
title = {{Pose-Normalized Image Generation for}},
url = {http://openaccess.thecvf.com/content{\_}ECCV{\_}2018/html/Xuelin{\_}Qian{\_}Pose-Normalized{\_}Image{\_}Generation{\_}ECCV{\_}2018{\_}paper.html},
year = {2018}
}
@article{Cao2017,
abstract = {We present an approach to efficiently detect the 2D pose of multiple people in an image. The approach uses a non-parametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and their association via two branches of the same sequential prediction process. Our method placed first in the inaugural COCO 2016 keypoints challenge, and significantly exceeds the previous state-of-the-art result on the MPII MultiPerson benchmark, both in performance and efficiency.},
archivePrefix = {arXiv},
arxivId = {1611.08050},
author = {Cao, Zhe and Simon, Tomas and Wei, Shih En and Sheikh, Yaser},
doi = {10.1109/CVPR.2017.143},
eprint = {1611.08050},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/Otros/2017 - HumanPoseEstimator.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {1302--1310},
title = {{Realtime multi-person 2D pose estimation using part affinity fields}},
volume = {2017-Janua},
year = {2017}
}
@article{Jiang2020,
abstract = {The “You only look once v4” (YOLOv4) is one type of object detection methods in deep learning. YOLOv4-tiny is proposed based on YOLOv4 to simple the network structure and reduce parameters, which makes it be suitable for developing on the mobile and embedded devices. To improve the real-time of object detection, a fast object detection method is proposed based on YOLOv4-tiny. It firstly uses two ResBlock-D modules in ResNet-D network instead of two CSPBlock modules in Yolov4-tiny, which reduces the computation complexity. Secondly, it designs an auxiliary residual network block to extract more feature information of object to reduce detection error. In the design of auxiliary network, two consecutive 3x3 convolutions are used to obtain 5x5 receptive fields to extract global features, and channel attention and spatial attention are also used to extract more effective information. In the end, it merges the auxiliary network and backbone network to construct the whole network structure of improved YOLOv4-tiny. Simulation results show that the proposed method has faster object detection than YOLOv4-tiny and YOLOv3-tiny, and almost the same mean value of average precision as the YOLOv4-tiny. It is more suitable for real-time object detection, especially for developing on embedded devices .},
author = {Jiang, Zicong and Zhao, Liquan and Shuaiyang, L. I. and Yanfei, J. I.A.},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/Arquitecturas GAN/2020 - YOLOV4-Tiny.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Auxiliary residual network,Deep learning,Real-time object detection,YOLO},
number = {October},
pages = {1--11},
title = {{Real-time object detection method for embedded devices}},
volume = {3},
year = {2020}
}
@article{Huang2020,
abstract = {Person re-identification (Re-ID) in real-world scenarios usually suffers from various degradation factors, e.g., low-resolution, weak illumination, blurring and adverse weather. On the one hand, these degradations lead to severe discriminative information loss, which significantly obstructs identity representation learning; on the other hand, the feature mismatch problem caused by low-level visual variations greatly reduces retrieval performance. An intuitive solution to this problem is to utilize low-level image restoration methods to improve the image quality. However, existing restoration methods cannot directly serve to real-world Re-ID due to various limitations, e.g., the requirements of reference samples, domain gap between synthesis and reality, and incompatibility between low-level and high-level methods. In this paper, to solve the above problem, we propose a degradation invariance learning framework for real-world person Re-ID. By introducing a self-supervised disentangled representation learning strategy, our method is able to simultaneously extract identity-related robust features and remove real-world degradations without extra supervision. We use low-resolution images as the main demonstration, and experiments show that our approach is able to achieve state-of-the-art performance on several Re-ID benchmarks. In addition, our framework can be easily extended to other real-world degradation factors, such as weak illumination, with only a few modifications.},
archivePrefix = {arXiv},
arxivId = {2004.04933},
author = {Huang, Yukun and Zha, Zheng Jun and Fu, Xueyang and Hong, Richang and Li, Liang},
doi = {10.1109/CVPR42600.2020.01409},
eprint = {2004.04933},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2020 - Real-world Person Re-Identification via Degradation Invariance Learning SECCION GAN.pdf:pdf},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {14072--14082},
title = {{Real-world person re-Identification via degradation invariance learning}},
year = {2020}
}
@article{Xia2021,
abstract = {Person re-identification has made great progress over the years. However, due to the problem of super-resolution and few labeled samples, it is difficult to apply in practice. In this paper, we propose a semi-supervised super-resolution person re-identification method based on soft multi-labels. Firstly, a Mixed-Space Super-Resolution model (MSSR) is constructed based on Generative Adversarial Networks (GAN), which aims to convert low-resolution person images into high-resolution images. Secondly, a Part-based Graph Convolutional Network (PGCN) is proposed to extract discriminative feature by exploring the relationship of local features within person. Finally, to solve the problem of label limitation, we use the PGCN trained with a small amount of labeled samples to predict the soft multi-labels of unlabeled samples, and further train PGCN with unlabeled samples based on a novel multi-label similarity loss. Experiments have been conducted on the Market1501, CUHK03, and MSMT17 datasets to evaluate this method, which show that it outperforms other semi-supervised methods.},
author = {Xia, Limin and Zhu, Jiahui and Yu, Zhimin},
doi = {10.1109/ACCESS.2021.3063000},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2021 - Real-World Person Re-Identification via SuperResolution and Semi-Supervised Methods.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {GAN,Person re-identification,multi-labels,semi-supervised,super-resolution},
pages = {35834--35845},
title = {{Real-World Person Re-Identification via Super-Resolution and Semi-Supervised Methods}},
volume = {9},
year = {2021}
}
@article{Park2019a,
abstract = {Person re-identification (reID) aims at retrieving an image of the person of interest from a set of images typically captured by multiple cameras. Recent reID methods have shown that exploiting local features describing body parts, together with a global feature of a person image itself, gives robust feature representations, even in the case of missing body parts. However, using the individual part-level features directly, without considering relations between body parts, confuses differentiating identities of different persons having similar attributes in corresponding parts. To address this issue, we propose a new relation network for person reID that considers relations between individual body parts and the rest of them. Our model makes a single part-level feature incorporate partial information of other body parts as well, supporting it to be more discriminative. We also introduce a global contrastive pooling (GCP) method to obtain a global feature of a person image. We propose to use contrastive features for GCP to complement conventional max and averaging pooling techniques. We show that our model outperforms the state of the art on the Market1501, DukeMTMC-reID and CUHK03 datasets, demonstrating the effectiveness of our approach on discriminative person representations.},
author = {Park, Hyunjong and Ham, Bumsub},
doi = {10.1609/aaai.v34i07.6857},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2020 - Relation Network for Person Re-Identification.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Vision},
title = {{Relation Network for Person Re-identification}},
year = {2019}
}
@article{Bhatia2020,
abstract = {We propose a loss function for generative adversarial networks (GANs) using R{\'{e}}nyi information measures with parameter $\alpha$. More specifically, we formulate GAN's generator loss function in terms of R{\'{e}}nyi cross-entropy functionals. We demonstrate that for any $\alpha$, this generalized loss function preserves the equilibrium point satisfied by the original GAN loss based on the Jensen-R{\'{e}}nyi divergence, a natural extension of the Jensen-Shannon divergence. We also prove that the R{\'{e}}nyi-centric loss function reduces to the original GAN loss function as $\alpha$ → 1. We show empirically that the proposed loss function, when implemented on both DCGAN (with L1 normalization) and StyleGAN architectures, confers performance benefits by virtue of the extra degree of freedom provided by the parameter $\alpha$. More specifically, we show improvements with regard to: (a) the quality of the generated images as measured via the Fr{\'{e}}chet Inception Distance (FID) score (e.g., best FID=8.33 for R{\'{e}}nyiStyleGAN vs 9.7 for StyleGAN when evaluated over 64×64 CelebA images) and (b) training stability. While it was applied to GANs in this study, the proposed approach is generic and can be used in other applications of information theory to deep learning, e.g., AI bias or privacy.},
author = {Bhatia, Himesh and Paul, William and Alajaji, Fady and Gharesifard, Bahman and Burlina, Philippe},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/2020 - Generative Adversarial Networks.pdf:pdf},
issn = {23318422},
journal = {arXiv},
number = {2},
pages = {139--144},
title = {{R{\'{e}}nyi Generative Adversarial Networks}},
volume = {27},
year = {2020}
}
@article{Tran2019,
abstract = {The large pose discrepancy between two face images is one of the fundamental challenges in automatic face recognition. Conventional approaches to pose-invariant face recognition either perform face frontalization on, or learn a pose-invariant representation from, a non-frontal face image. We argue that it is more desirable to perform both tasks jointly to allow them to leverage each other. To this end, this paper proposes a Disentangled Representation learning-Generative Adversarial Network (DR-GAN) with three distinct novelties. First, the encoder-decoder structure of the generator enables DR-GAN to learn a representation that is both generative and discriminative, which can be used for face image synthesis and pose-invariant face recognition. Second, this representation is explicitly disentangled from other face variations such as pose, through the pose code provided to the decoder and pose estimation in the discriminator. Third, DR-GAN can take one or multiple images as the input, and generate one unified identity representation along with an arbitrary number of synthetic face images. Extensive quantitative and qualitative evaluation on a number of controlled and in-the-wild databases demonstrate the superiority of DR-GAN over the state of the art in both learning representations and rotating large-pose face images.},
archivePrefix = {arXiv},
arxivId = {1705.11136},
author = {Tran, Luan and Yin, Xi and Liu, Xiaoming},
doi = {10.1109/TPAMI.2018.2868350},
eprint = {1705.11136},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2018 - DRGAN.pdf:pdf},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Representation learning,face rotation and frontalization,generative adversarial network,pose-invariant face recognition},
number = {12},
pages = {3007--3021},
pmid = {30183620},
title = {{Representation Learning by Rotating Your Faces}},
volume = {41},
year = {2019}
}
@article{Szegedy2016,
abstract = {Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2{\%} top-1 and 5:6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5{\%} top-5 error and 17:3{\%} top-1 error on the validation set and 3:6{\%} top-5 error on the official test set.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
doi = {10.1109/CVPR.2016.308},
eprint = {1512.00567},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/Otros/2015 - LSR.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {2818--2826},
title = {{Rethinking the Inception Architecture for Computer Vision}},
volume = {2016-Decem},
year = {2016}
}
@article{Luo2021,
author = {Luo, Zhiyuan},
doi = {10.32604/jnm.2021.018027},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/2021 - Review of GAN-Based Person Re-Identification.pdf:pdf},
keywords = {generative adversarial network,person re-identification},
title = {{Review of GAN-Based Person Re-Identification}},
year = {2021}
}
@article{Wu,
author = {Wu, Ancong and Zheng, Wei-shi and Yu, Hong-xing and Gong, Shaogang and Lai, Jianhuang},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2017 - Dataset personas infrarrojos.pdf:pdf},
pages = {5380--5389},
title = {{RGB-Infrared Cross-Modality Person Re-Identification}}
}
@article{MarketZheng2015,
abstract = {This paper contributes a new high quality dataset for person re-identification, named "Market-1501". Generally , current datasets: 1) are limited in scale; 2) consist of hand-drawn bboxes, which are unavailable under realistic settings; 3) have only one ground truth and one query image for each identity (close environment). To tackle these problems, the proposed Market-1501 dataset is featured in three aspects. First, it contains over 32,000 annotated b-boxes, plus a distractor set of over 500K images, making it the largest person re-id dataset to date. Second, images in Market-1501 dataset are produced using the De-formable Part Model (DPM) as pedestrian detector. Third, our dataset is collected in an open system, where each identity has multiple images under each camera. As a minor contribution, inspired by recent advances in large-scale image search, this paper proposes an un-supervised Bag-of-Words descriptor. We view person re-identification as a special task of image search. In experiment , we show that the proposed descriptor yields competitive accuracy on VIPeR, CUHK03, and Market-1501 datasets, and is scalable on the large-scale 500k dataset.},
author = {Zheng, Liang and Shen, Liyue and Tian, Lu and Wang, Shengjin and Wang, Jingdong and Tian, Qi},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2015-MARKET1501Zheng{\_}Scalable{\_}Person{\_}Re-Identification{\_}ICCV{\_}2015{\_}paper.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {15505499},
journal = {Iccv},
pages = {1116--1124},
title = {{Scalable Person Re-identification : A Benchmark University of Texas at San Antonio}},
url = {http://www.liangzheng.com.cn.},
year = {2015}
}
@article{Zhang2020a,
abstract = {Recently, person re-identification (PR-ID) has attracted numerous of research interest because of its broad applications. However, most of the existing PR-ID models always follow the supervised framework, which requires substantial labeled data. In fact, it is often very hard to get enough labeled training samples in many practical application scenarios. To overcome this limitation, some semi-supervised PR-ID methods have been presented more recently. Although some of these semi-supervised models achieve satisfied results, there is still much space to improve. In this paper, we propose a novel semi-supervised PR-ID by similarity-embedded cycle GANs (SECGAN). Our SECGAN model can learn cross-view features with limited labeled data by using cycle GANs. Simultaneously, to further enhance the ability of cycle GANs so that it can extract more discriminative and robust features, similarity metric subnet and specific features extracting subnet are embedded into cycle GANs. Extensive experiments have been conducted on three public PR-ID benchmark datasets, and the experimental results show that our proposed SECGAN approach outperforms several typical supervised methods and the existing state-of-the-art semi-supervised methods including traditional and deep learning semi-supervised methods.},
author = {Zhang, Xinyu and Jing, Xiao Yuan and Zhu, Xiaoke and Ma, Fei},
doi = {10.1007/s00521-020-04809-7},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/2020 - Semi-supervised person re-identification by similarity-embedded cycle.pdf:pdf},
isbn = {0123456789},
issn = {14333058},
journal = {Neural Computing and Applications},
keywords = {Cycle GANs,Deep metric learning,Person re-identification,Semi-supervised learning,Similarity embedded},
number = {17},
pages = {14143--14152},
publisher = {Springer London},
title = {{Semi-supervised person re-identification by similarity-embedded cycle GANs}},
url = {https://doi.org/10.1007/s00521-020-04809-7},
volume = {32},
year = {2020}
}
@article{Ainam2019,
abstract = {Person re-identification (re-id) is a cross-camera retrieval task which establishes a correspondence between images of a person from multiple cameras. Deep learning methods have been successfully applied to this problem and have achieved impressive results. However, these methods require a large amount of labeled training data. Currently, the labeled datasets in person re-id are limited in their scale and manual acquisition of such large-scale datasets from surveillance cameras is a tedious and labor-intensive task. In this paper, we propose a framework that performs intelligent data augmentation and assigns the partial smoothing label to generated data. Our approach first exploits the clustering property of existing person re-id datasets to create groups of similar objects that model cross-view variations. Each group is then used to generate realistic images through adversarial training. Our aim is to emphasize the feature similarity between generated samples and the original samples. Finally, we assign a non-uniform label distribution to the generated samples and define a regularized loss function for training. The proposed approach tackles two problems 1) how to efficiently use the generated data and 2) how to address the over-smoothness problem found in current regularization methods. The extensive experiments on four large-scale datasets show that our regularization method significantly improves the re-id accuracy compared to existing methods.},
archivePrefix = {arXiv},
arxivId = {1809.04976},
author = {Ainam, Jean Paul and Qin, Ke and Liu, Guisong and Luo, Guangchun},
doi = {10.1109/ACCESS.2019.2901599},
eprint = {1809.04976},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/2019 - Sparse Label Smoothing Regularization.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Computational and artificial intelligence,artificial neural network,feature extraction,image retrieval},
pages = {27899--27910},
publisher = {IEEE},
title = {{Sparse Label Smoothing Regularization for Person Re-Identification}},
volume = {7},
year = {2019}
}
@article{Zhan2020,
abstract = {The recent person re-identification research has achieved great success by learning from a large number of labeled person images. On the other hand, the learned models often experience significant performance drops when applied to images collected in a different environment. Unsupervised domain adaptation (UDA) has been investigated to mitigate this constraint, but most existing systems adapt images at pixel level only and ignore obvious discrepancies at spatial level. This paper presents an innovative UDA-based person re-identification network that is capable of adapting images at both spatial and pixel levels simultaneously. A novel disentangled cycle-consistency loss is designed which guides the learning of spatial-level and pixel-level adaptation in a collaborative manner. In addition, a novel multi-modal mechanism is incorporated which is capable of generating images of different geometry views and augmenting training images effectively. Extensive experiments over a number of public datasets show that the proposed UDA network achieves superior person re-identification performance as compared with the state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1911.11312},
author = {Zhan, Fangneng and Zhang, Changgong},
doi = {10.1109/ICPR48806.2021.9412465},
eprint = {1911.11312},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2021 - Spatial-Aware GAN for unsupervised person Re-Identifiacacion.pdf:pdf},
isbn = {9781728188089},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
pages = {6889--6896},
title = {{Spatial-aware GAN for unsupervised person Re-identification}},
year = {2020}
}
@article{Choi2018,
abstract = {Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.},
archivePrefix = {arXiv},
arxivId = {1711.09020},
author = {Choi, Yunjey and Choi, Minje and Kim, Munyoung and Ha, Jung Woo and Kim, Sunghun and Choo, Jaegul},
doi = {10.1109/CVPR.2018.00916},
eprint = {1711.09020},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2018 - StarGAN.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {8789--8797},
title = {{StarGAN: Unified Generative Adversarial Networks for Multi-domain Image-to-Image Translation}},
year = {2018}
}
@article{Hussin2021,
abstract = {In this study, the StyleGAN-LSRO method has been developed for person re-identification (re-ID) tasks. This method applies the style-based generative adversarial network (StyleGAN) to generate new synthetic images from existing person re-ID datasets and the label smoothing regularization for outliers (LSRO) algorithm to process those newly produced unlabeled images by assigning them a uniform label distribution along with the definition of a loss function for the training process. A baseline model based on a convolutional neural network (CNN) was developed to learn the discriminative features to recognize a person{\&}{\#}x2019;s identity. The developed method has been tested on three datasets. These datasets are Market-1501, DukeMTMC-reID, and MSMT17. The experimental results show that the StyleGAN model achieved a Fr{\&}{\#}x00E9;chet inception distance score of 12.67 and structural similarity score of 0.387, outperforming all the previous generative methods and demonstrating that the images generated by StyleGAN are of superior quality. Adding these StyleGAN-generated data significantly improves the person re-ID accuracy. The StyleGAN-LSRO person re-ID method achieved 98.5{\%} rank-1 accuracy and 91.8{\%} mean average precision (mAP) on Market-1501, 87.0{\%} rank-1 accuracy and 83.8{\%} mAP on DukeMTMC-reID, and 81.5{\%} rank-1 accuracy and 60.9{\%} mAP on MSMT17, respectively. These results show that the StyleGAN-LSRO method significantly outperforms most of the state-of-the-art person re-ID methods. The success rate for person re-ID increases when the images used are of high resolution and square matrix form. In other cases, the success rate decreases.},
author = {Hussin, Saleh Hussin S. and Yildirim, Remzi},
doi = {10.1109/ACCESS.2021.3051723},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2021 - StyleGAN-LSRO Method for Person Re-Identification.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Cameras,Convolutional neural networks,Gallium nitride,Generative adversarial networks,Generators,Image resolution,StyleGAN,Task analysis,Training,deep learning,generative adversarial networks,label smoothing regularization,person re-identification},
pages = {13857--13869},
title = {{StyleGAN-LSRO Method for Person Re-identification}},
year = {2021}
}
@article{Alaluf2022,
abstract = {StyleGAN is arguably one of the most intriguing and well-studied generative models, demonstrating impressive performance in image generation, inversion, and manipulation. In this work, we explore the recent StyleGAN3 architecture, compare it to its predecessor, and investigate its unique advantages, as well as drawbacks. In particular, we demonstrate that while StyleGAN3 can be trained on unaligned data, one can still use aligned data for training, without hindering the ability to generate unaligned imagery. Next, our analysis of the disentanglement of the different latent spaces of StyleGAN3 indicates that the commonly used W/W+ spaces are more entangled than their StyleGAN2 counterparts, underscoring the benefits of using the StyleSpace for fine-grained editing. Considering image inversion, we observe that existing encoder-based techniques struggle when trained on unaligned data. We therefore propose an encoding scheme trained solely on aligned data, yet can still invert unaligned images. Finally, we introduce a novel video inversion and editing workflow that leverages the capabilities of a fine-tuned StyleGAN3 generator to reduce texture sticking and expand the field of view of the edited video.},
archivePrefix = {arXiv},
arxivId = {2201.13433},
author = {Alaluf, Yuval and Patashnik, Or and Wu, Zongze and Zamir, Asif and Shechtman, Eli and Lischinski, Dani and Cohen-Or, Daniel},
eprint = {2201.13433},
file = {:C$\backslash$:/Users/Lau/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alaluf et al. - 2022 - Third Time's the Charm Image and Video Editing with StyleGAN3.pdf:pdf},
title = {{Third Time's the Charm? Image and Video Editing with StyleGAN3}},
url = {http://arxiv.org/abs/2201.13433},
year = {2022}
}
@article{Li2014,
abstract = {Generative Adversarial Nets (GANs) have shown promise in image generation and semi-supervised learning (SSL). However, existing GANs in SSL have two problems: (1) the generator and the discriminator (i.e. the classifier) may not be optimal at the same time; and (2) the generator cannot control the semantics of the generated samples. The problems essentially arise from the two-player formulation, where a single discriminator shares incompatible roles of identifying fake samples and predicting labels and it only estimates the data without considering the labels. To address the problems, we present triple generative adversarial net (Triple-GAN), which consists of three players - a generator, a discriminator and a classifier. The generator and the classifier characterize the conditional distributions between images and labels, and the discriminator solely focuses on identifying fake image-label pairs. We design compatible utilities to ensure that the distributions characterized by the classifier and the generator both converge to the data distribution. Our results on various datasets demonstrate that Triple-GAN as a unified model can simultaneously (1) achieve the state-of-the-art classification results among deep generative models, and (2) disentangle the classes and styles of the input and transfer smoothly in the data space via interpolation in the latent space class-conditionally.},
author = {Li, Chongxuan and Xu, Kun and Zhu, Jun and Zhang, Bo},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2014-generative-adversarial-nets-Paper.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {4089--4099},
title = {{Triple generative adversarial nets}},
volume = {2014-Decem},
year = {2014}
}
@article{Liu2020,
abstract = {Style variation has been a major challenge for person re-identification, which aims to match the same pedestrians across different cameras. Existing works attempted to address this problem with camera-invariant descriptor subspace learning. However, there will be more image artifacts when the difference between the images taken by different cameras is larger. To solve this problem, we propose a UnityStyle adaption method, which can smooth the style disparities within the same camera and across different cameras. Specifically, we firstly create UnityGAN to learn the style changes between cameras, producing shape-stable style-unity images for each camera, which is called UnityStyle images. Meanwhile, we use UnityStyle images to eliminate style differences between different images, which makes a better match between query and gallery. Then, we apply the proposed method to Re-ID models, expecting to obtain more style-robust depth features for querying. We conduct extensive experiments on widely used benchmark datasets to evaluate the performance of the proposed framework, the results of which confirm the superiority of the proposed model.},
archivePrefix = {arXiv},
arxivId = {2003.02068},
author = {Liu, Chong and Chang, Xiaojun and Shen, Yi Dong},
doi = {10.1109/CVPR42600.2020.00692},
eprint = {2003.02068},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2020 - Unity Style Transfer for Person Re-Identification.pdf:pdf},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {6886--6895},
title = {{Unity style transfer for person re-identification}},
year = {2020}
}
@article{Zheng2017,
abstract = {The main contribution of this paper is a simple semisupervised pipeline that only uses the original training set without collecting extra data. It is challenging in 1) how to obtain more training data only from the training set and 2) how to use the newly generated data. In this work, the generative adversarial network (GAN) is used to generate unlabeled samples. We propose the label smoothing regularization for outliers (LSRO). This method assigns a uniform label distribution to the unlabeled images, which regularizes the supervised model and improves the baseline. We verify the proposed method on a practical problem: person re-identification (re-ID). This task aims to retrieve a query person from other cameras. We adopt the deep convolutional generative adversarial network (DCGAN) for sample generation, and a baseline convolutional neural network (CNN) for representation learning. Experiments show that adding the GAN-generated data effectively improves the discriminative ability of learned CNN embeddings. On three large-scale datasets, Market- 1501, CUHK03 and DukeMTMC-reID, we obtain +4.37{\%}, +1.6{\%} and +2.46{\%} improvement in rank-1 precision over the baseline CNN, respectively. We additionally apply the proposed method to fine-grained bird recognition and achieve a +0.6{\%} improvement over a strong baseline. The code is available at https://github.com/layumi/ Person-reID-GAN.},
archivePrefix = {arXiv},
arxivId = {1701.07717},
author = {Zheng, Zhedong and Zheng, Liang and Yang, Yi},
doi = {10.1109/ICCV.2017.405},
eprint = {1701.07717},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/GAN/2017 - Unlabeled Samples Generated by GAN.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {3774--3782},
title = {{Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in Vitro}},
volume = {2017-Octob},
year = {2017}
}
@article{Zhu2017,
abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to push F(G(X)) ≈ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
archivePrefix = {arXiv},
arxivId = {1703.10593},
author = {Zhu, Jun Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
doi = {10.1109/ICCV.2017.244},
eprint = {1703.10593},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2020 - CycleGAN.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2242--2251},
title = {{Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks}},
volume = {2017-Octob},
year = {2017}
}
@article{Khraimeche2020,
abstract = {While recent person re-identification (ReID) methods achieve high accuracy in a supervised setting, their generalization to an unlabelled domain is still an open problem. In this paper, we introduce a novel unsupervised disentanglement generative adversarial network (UD-GAN) to address the domain adaptation issue of supervised person ReID. Our framework jointly trains a ReID network for discriminative features extraction in a source labelled domain using identity annotation, and adapts the ReID model to an unlabelled target domain by learning disentangled latent representations on the domain. Identity-unrelated features in the target domain are distilled from the latent features. As a result, the ReID features better encompass the identity of a person in the unsupervised domain. We conducted experiments on the Market1501, DukeMTMC and MSMT17 datasets. Results show that the unsupervised domain adaptation problem in ReID is very challenging. Nevertheless, our method shows improvement in half of the domain transfers and achieve state-of-the-art performance for one of them.},
archivePrefix = {arXiv},
arxivId = {2007.15560},
author = {Khraimeche, Yacine and Bilodeau, Guillaume-Alexandre and Steele, David and Mahadik, Harshad},
eprint = {2007.15560},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2020 - Unsupervised Disentanglement GAN for.pdf:pdf},
title = {{Unsupervised Disentanglement GAN for Domain Adaptive Person Re-Identification}},
url = {http://arxiv.org/abs/2007.15560},
year = {2020}
}
@article{Fan2017,
abstract = {The superiority of deeply learned pedestrian representations has been reported in very recent literature of person re-identification (re-ID). In this paper, we consider the more pragmatic issue of learning a deep feature with no or only a few labels. We propose a progressive unsupervised learning (PUL) method to transfer pretrained deep representations to unseen domains. Our method is easy to implement and can be viewed as an effective baseline for unsupervised re-ID feature learning. Specifically, PUL iterates between 1) pedestrian clustering and 2) fine-tuning of the convolutional neural network (CNN) to improve the original model trained on the irrelevant labeled dataset. Since the clustering results can be very noisy, we add a selection operation between the clustering and fine-tuning. At the beginning when the model is weak, CNN is fine-tuned on a small amount of reliable examples which locate near to cluster centroids in the feature space. As the model becomes stronger in subsequent iterations, more images are being adaptively selected as CNN training samples. Progressively, pedestrian clustering and the CNN model are improved simultaneously until algorithm convergence. This process is naturally formulated as self-paced learning. We then point out promising directions that may lead to further improvement. Extensive experiments on three large-scale re-ID datasets demonstrate that PUL outputs discriminative features that improve the re-ID accuracy. Our code has been released at https://github.com/hehefan/ Unsupervised-Person-Re-identification-Clustering-and-Fine-tuning.},
archivePrefix = {arXiv},
arxivId = {1705.10444},
author = {Fan, Hehe and Zheng, Liang and Yang, Yi},
eprint = {1705.10444},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/2017 - Unsupervised Person Re-identification.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Clustering,Convolutional neural network,Large-scale person re-identification,Unsupervised learning},
pages = {1--9},
title = {{Unsupervised person re-identification: Clustering and fine-tuning}},
year = {2017}
}
@article{Radford2016,
abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
archivePrefix = {arXiv},
arxivId = {1511.06434},
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
eprint = {1511.06434},
file = {:F$\backslash$:/Personal/2020/UADY/Tesis/MejoresTrabajos/Arquitecturas GAN/2015 - DCGAN.pdf:pdf},
journal = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
pages = {1--16},
title = {{Unsupervised representation learning with deep convolutional generative adversarial networks}},
year = {2016}
}
